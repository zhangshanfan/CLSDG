{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c5a046d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:42:16.277457Z",
     "start_time": "2022-05-03T12:42:16.231535Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "#sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))\n",
    "# For replicating the experiments\n",
    "SEED = 42\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from model import LinTrans, LogReg\n",
    "from optimizer import loss_function\n",
    "from utils import *\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from clustering_metric import clustering_metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07edf20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:42:16.869349Z",
     "start_time": "2022-05-03T12:42:16.836179Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gnnlayers', type=int, default=1, help=\"Number of gnn layers\")\n",
    "parser.add_argument('--linlayers', type=int, default=1, help=\"Number of hidden layers\")\n",
    "parser.add_argument('--epochs', type=int, default=400, help='Number of epochs to train.')\n",
    "parser.add_argument('--dims', type=int, default=[500], help='Number of units in hidden layer 1.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--upth_st', type=float, default=0.0011, help='Upper Threshold start.')\n",
    "parser.add_argument('--lowth_st', type=float, default=0.1, help='Lower Threshold start.')\n",
    "parser.add_argument('--upth_ed', type=float, default=0.001, help='Upper Threshold end.')\n",
    "parser.add_argument('--lowth_ed', type=float, default=0.5, help='Lower Threshold end.')\n",
    "parser.add_argument('--upd', type=int, default=10, help='Update epoch.')\n",
    "parser.add_argument('--bs', type=int, default=10000, help='Batchsize.')\n",
    "parser.add_argument('--dataset', type=str, default='wiki', help='type of dataset.')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "args,_ = parser.parse_known_args()\n",
    "args.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70b9a62c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:42:17.540036Z",
     "start_time": "2022-05-03T12:42:17.520055Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_similarity(z, upper_threshold, lower_treshold, pos_num, neg_num):\n",
    "    f_adj = np.matmul(z, np.transpose(z))\n",
    "    cosine = f_adj\n",
    "    cosine = cosine.reshape([-1,])\n",
    "    pos_num = round(upper_threshold * len(cosine))\n",
    "    neg_num = round((1-lower_treshold) * len(cosine))\n",
    "    \n",
    "    pos_inds = np.argpartition(-cosine, pos_num)[:pos_num]\n",
    "    neg_inds = np.argpartition(cosine, neg_num)[:neg_num]\n",
    "    \n",
    "    return np.array(pos_inds), np.array(neg_inds)\n",
    "\n",
    "def update_threshold(upper_threshold, lower_treshold, up_eta, low_eta):\n",
    "    upth = upper_threshold + up_eta\n",
    "    lowth = lower_treshold + low_eta\n",
    "    return upth, lowth\n",
    "\n",
    "def load_network_data(adj_name, nodes_numbers):\n",
    "    raw_edges = pd.read_csv(adj_name,header=None,sep=' ')-1\n",
    "    drop_self_loop = raw_edges[raw_edges[0]!=raw_edges[1]]\n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    features = np.eye(nodes_numbers)\n",
    "    return adj, features\n",
    "\n",
    "def get_scores(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    \n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    preds_all[preds_all>0.7] = 1\n",
    "    preds_all[preds_all<=0.7] = 0\n",
    "    \n",
    "    acc_score = accuracy_score(labels_all, preds_all)\n",
    "    f1 = f1_score(labels_all, preds_all, average='macro')\n",
    "\n",
    "    return roc_score, ap_score, acc_score, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "335d3230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:43:11.807406Z",
     "start_time": "2022-05-03T12:43:11.776908Z"
    }
   },
   "outputs": [],
   "source": [
    "def gae_for(args):\n",
    "    print(\"Using {} dataset\".format(args.dataset))\n",
    "    if args.dataset == 'wiki':\n",
    "        n_clusters = 17\n",
    "        Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "        adj, features, lables = load_wiki()\n",
    "    else:\n",
    "        nodes_number = 1176    # 这里需要输入网络的节点数量\n",
    "        n_clusters = 20     # 指定类簇数量\n",
    "        Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "        adj, features = load_network_data(args.dataset, nodes_number)\n",
    "\n",
    "    n_nodes, feat_dim = features.shape\n",
    "    dims = [feat_dim] + args.dims\n",
    "    \n",
    "    layers = args.linlayers\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    \n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    adj_orig = adj\n",
    "\n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)    # val:0.05  test:0.1\n",
    "    adj = adj_train\n",
    "    n = adj.shape[0]\n",
    "\n",
    "    adj_norm_s = preprocess_graph(adj, args.gnnlayers, norm='sym', renorm=True)\n",
    "    sm_fea_s = sp.csr_matrix(features).toarray()\n",
    "    \n",
    "    print('Laplacian Smoothing...')\n",
    "    for a in adj_norm_s:\n",
    "        sm_fea_s = a.dot(sm_fea_s)\n",
    "    adj_1st = (adj + sp.eye(n)).toarray()\n",
    "\n",
    "    adj_label = torch.FloatTensor(adj_1st)\n",
    "    \n",
    "    model = LinTrans(layers, dims)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    sm_fea_s = torch.FloatTensor(sm_fea_s)\n",
    "    adj_label = adj_label.reshape([-1,])\n",
    "\n",
    "    inx = sm_fea_s\n",
    "    \n",
    "    pos_num = len(adj.indices)\n",
    "    neg_num = n_nodes*n_nodes-pos_num\n",
    "\n",
    "    up_eta = (args.upth_ed - args.upth_st) / (args.epochs/args.upd)\n",
    "    low_eta = (args.lowth_ed - args.lowth_st) / (args.epochs/args.upd)\n",
    "\n",
    "    pos_inds, neg_inds = update_similarity(normalize(sm_fea_s.numpy()), args.upth_st, args.lowth_st, pos_num, neg_num)\n",
    "    upth, lowth = update_threshold(args.upth_st, args.lowth_st, up_eta, low_eta)\n",
    "\n",
    "    bs = min(args.bs, len(pos_inds))\n",
    "    length = len(pos_inds)\n",
    "    \n",
    "    pos_inds_cuda = torch.LongTensor(pos_inds)\n",
    "    best_lp = 0.\n",
    "    print('Start Training...')\n",
    "    for epoch in range(args.epochs):\n",
    "        st, ed = 0, bs\n",
    "        batch_num = 0\n",
    "        model.train()\n",
    "        length = len(pos_inds)\n",
    "        \n",
    "        while ( ed <= length ):\n",
    "            sampled_neg = torch.LongTensor(np.random.choice(neg_inds, size=ed-st))\n",
    "            sampled_inds = torch.cat((pos_inds_cuda[st:ed], sampled_neg), 0)\n",
    "            t = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            xind = sampled_inds // n_nodes\n",
    "            yind = sampled_inds % n_nodes\n",
    "            x = torch.index_select(inx, 0, xind)\n",
    "            y = torch.index_select(inx, 0, yind)\n",
    "            zx = model(x)\n",
    "            zy = model(y)\n",
    "            batch_label = torch.cat((torch.ones(ed-st), torch.zeros(ed-st)))\n",
    "            batch_pred = model.dcs(zx, zy)\n",
    "            loss = loss_function(adj_preds=batch_pred, adj_labels=batch_label, n_nodes=ed-st)\n",
    "            \n",
    "            loss.backward()\n",
    "            cur_loss = loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            st = ed\n",
    "            batch_num += 1\n",
    "            if ed < length and ed + bs >= length:\n",
    "                ed += length - ed\n",
    "            else:\n",
    "                ed += bs\n",
    "\n",
    "        if (epoch + 1) % args.upd == 0:\n",
    "            model.eval()\n",
    "            mu = model(inx)\n",
    "            hidden_emb = mu.cpu().data.numpy()\n",
    "            upth, lowth = update_threshold(upth, lowth, up_eta, low_eta)\n",
    "            pos_inds, neg_inds = update_similarity(hidden_emb, upth, lowth, pos_num, neg_num)\n",
    "            bs = min(args.bs, len(pos_inds))\n",
    "            pos_inds_cuda = torch.LongTensor(pos_inds)\n",
    "            val_auc, val_ap, val_acc, val_f1 = get_scores(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "            if val_auc + val_ap >= best_lp:\n",
    "                best_lp = val_auc + val_ap\n",
    "                best_emb = hidden_emb\n",
    "            print(\"Epoch: {}, train_loss_gae={:.5f}, val_ap={:.5f}, val_acc={:.5f},time={:.5f}\".format(epoch + 1, cur_loss, val_ap, val_acc, time.time() - t))\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    auc_score, ap_score, acc_score, f1_score = get_scores(best_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test AP score: ',ap_score)\n",
    "    print('Test AUC score: ',auc_score)\n",
    "    print('Test ACC score: ',acc_score)\n",
    "    print('Test F1 score: ',f1_score)\n",
    "    return best_emb, adj_orig, test_edges, test_edges_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f4ac9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T12:53:02.241433Z",
     "start_time": "2022-05-03T12:43:12.418586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using wiki dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Epoch: 10, train_loss_gae=0.74762, val_ap=0.91636, val_acc=0.75820,time=2.14800\n",
      "Epoch: 20, train_loss_gae=0.68811, val_ap=0.91025, val_acc=0.80829,time=1.93728\n",
      "Epoch: 30, train_loss_gae=0.65748, val_ap=0.90566, val_acc=0.81002,time=1.80201\n",
      "Epoch: 40, train_loss_gae=0.63626, val_ap=0.90457, val_acc=0.80570,time=1.81635\n",
      "Epoch: 50, train_loss_gae=0.62337, val_ap=0.90283, val_acc=0.80397,time=1.62670\n",
      "Epoch: 60, train_loss_gae=0.61336, val_ap=0.90224, val_acc=0.79965,time=1.72267\n",
      "Epoch: 70, train_loss_gae=0.60414, val_ap=0.90153, val_acc=0.79793,time=1.82674\n",
      "Epoch: 80, train_loss_gae=0.59764, val_ap=0.90100, val_acc=0.79706,time=1.77348\n",
      "Epoch: 90, train_loss_gae=0.59304, val_ap=0.90078, val_acc=0.79275,time=1.76826\n",
      "Epoch: 100, train_loss_gae=0.58696, val_ap=0.90032, val_acc=0.78929,time=1.79749\n",
      "Epoch: 110, train_loss_gae=0.58186, val_ap=0.89987, val_acc=0.78238,time=1.21156\n",
      "Epoch: 120, train_loss_gae=0.57584, val_ap=0.89953, val_acc=0.78066,time=1.65892\n",
      "Epoch: 130, train_loss_gae=0.57355, val_ap=0.89959, val_acc=0.78066,time=1.98715\n",
      "Epoch: 140, train_loss_gae=0.56803, val_ap=0.89889, val_acc=0.78066,time=1.62887\n",
      "Epoch: 150, train_loss_gae=0.56383, val_ap=0.89891, val_acc=0.78066,time=1.95128\n",
      "Epoch: 160, train_loss_gae=0.55978, val_ap=0.89854, val_acc=0.77807,time=1.77638\n",
      "Epoch: 170, train_loss_gae=0.55684, val_ap=0.89808, val_acc=0.77720,time=1.62041\n",
      "Epoch: 180, train_loss_gae=0.55606, val_ap=0.89760, val_acc=0.77720,time=1.86676\n",
      "Epoch: 190, train_loss_gae=0.55244, val_ap=0.89768, val_acc=0.77461,time=1.69503\n",
      "Epoch: 200, train_loss_gae=0.54962, val_ap=0.89733, val_acc=0.77547,time=1.77132\n",
      "Epoch: 210, train_loss_gae=0.54788, val_ap=0.89708, val_acc=0.77547,time=1.72183\n",
      "Epoch: 220, train_loss_gae=0.54573, val_ap=0.89702, val_acc=0.77634,time=1.66028\n",
      "Epoch: 230, train_loss_gae=0.54375, val_ap=0.89694, val_acc=0.77547,time=1.83500\n",
      "Epoch: 240, train_loss_gae=0.54206, val_ap=0.89648, val_acc=0.77547,time=1.74554\n",
      "Epoch: 250, train_loss_gae=0.53992, val_ap=0.89631, val_acc=0.77634,time=1.61551\n",
      "Epoch: 260, train_loss_gae=0.53834, val_ap=0.89600, val_acc=0.77547,time=1.87102\n",
      "Epoch: 270, train_loss_gae=0.53757, val_ap=0.89589, val_acc=0.77461,time=1.58798\n",
      "Epoch: 280, train_loss_gae=0.53572, val_ap=0.89594, val_acc=0.77634,time=1.64547\n",
      "Epoch: 290, train_loss_gae=0.53394, val_ap=0.89587, val_acc=0.77634,time=1.92154\n",
      "Epoch: 300, train_loss_gae=0.53335, val_ap=0.89540, val_acc=0.77547,time=1.81433\n",
      "Epoch: 310, train_loss_gae=0.53201, val_ap=0.89498, val_acc=0.77461,time=1.67545\n",
      "Epoch: 320, train_loss_gae=0.53145, val_ap=0.89506, val_acc=0.77547,time=1.69364\n",
      "Epoch: 330, train_loss_gae=0.53038, val_ap=0.89500, val_acc=0.77375,time=1.61824\n",
      "Epoch: 340, train_loss_gae=0.52852, val_ap=0.89513, val_acc=0.77288,time=1.69674\n",
      "Epoch: 350, train_loss_gae=0.52766, val_ap=0.89479, val_acc=0.77461,time=1.67444\n",
      "Epoch: 360, train_loss_gae=0.52719, val_ap=0.89501, val_acc=0.77375,time=1.60063\n",
      "Epoch: 370, train_loss_gae=0.52642, val_ap=0.89469, val_acc=0.77375,time=1.68077\n",
      "Epoch: 380, train_loss_gae=0.52510, val_ap=0.89435, val_acc=0.77375,time=1.81927\n",
      "Epoch: 390, train_loss_gae=0.52476, val_ap=0.89436, val_acc=0.77461,time=1.47993\n",
      "Epoch: 400, train_loss_gae=0.52405, val_ap=0.89427, val_acc=0.77375,time=1.78149\n",
      "Optimization Finished!\n",
      "Test AP score:  0.921225877329346\n",
      "Test AUC score:  0.9094470926038557\n",
      "Test ACC score:  0.7730802415875755\n",
      "Test F1 score:  0.7677239666083724\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gae_for(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccb247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
