{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66dcbabb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:34:00.228805Z",
     "start_time": "2022-05-07T09:34:00.215841Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "#sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))\n",
    "# For replicating the experiments\n",
    "SEED = 42\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from model import LinTrans, LogReg\n",
    "from optimizer import loss_function\n",
    "from utils import *\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from clustering_metric import clustering_metrics\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "461e64b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:34:00.747361Z",
     "start_time": "2022-05-07T09:34:00.720431Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gnnlayers', type=int, default=1, help=\"Number of gnn layers\")\n",
    "parser.add_argument('--linlayers', type=int, default=1, help=\"Number of hidden layers\")\n",
    "parser.add_argument('--epochs', type=int, default=300, help='Number of epochs to train.')\n",
    "parser.add_argument('--dims', type=int, default=[500], help='Number of units in hidden layer 1.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--upth_st', type=float, default=0.0011, help='Upper Threshold start.')\n",
    "parser.add_argument('--lowth_st', type=float, default=0.1, help='Lower Threshold start.')\n",
    "parser.add_argument('--upth_ed', type=float, default=0.001, help='Upper Threshold end.')\n",
    "parser.add_argument('--lowth_ed', type=float, default=0.5, help='Lower Threshold end.')\n",
    "parser.add_argument('--upd', type=int, default=10, help='Update epoch.')\n",
    "parser.add_argument('--bs', type=int, default=10000, help='Batchsize.')\n",
    "parser.add_argument('--dataset', type=str, default='Cora', help='type of dataset.')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "args,_ = parser.parse_known_args()\n",
    "args.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ab85f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:34:01.439916Z",
     "start_time": "2022-05-07T09:34:01.423957Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_similarity(z, upper_threshold, lower_treshold, pos_num, neg_num):\n",
    "    f_adj = np.matmul(z, np.transpose(z))\n",
    "    cosine = f_adj\n",
    "    cosine = cosine.reshape([-1,])\n",
    "    pos_num = round(upper_threshold * len(cosine))\n",
    "    neg_num = round((1-lower_treshold) * len(cosine))\n",
    "    \n",
    "    pos_inds = np.argpartition(-cosine, pos_num)[:pos_num]\n",
    "    neg_inds = np.argpartition(cosine, neg_num)[:neg_num]\n",
    "    \n",
    "    return np.array(pos_inds), np.array(neg_inds)\n",
    "\n",
    "def update_threshold(upper_threshold, lower_treshold, up_eta, low_eta):\n",
    "    upth = upper_threshold + up_eta\n",
    "    lowth = lower_treshold + low_eta\n",
    "    return upth, lowth\n",
    "\n",
    "def load_network_data(adj_name):\n",
    "    if adj_name == 'Cora':\n",
    "        nodes_numbers = 2708\n",
    "        datasets = Planetoid('./datasets', adj_name)\n",
    "        edges = datasets[0].edge_index\n",
    "        raw_edges = pd.DataFrame([[edges[0,i].item(), edges[1,i].item()] for i in range(edges.shape[1])])\n",
    "    elif adj_name == 'wiki':\n",
    "        nodes_numbers = 2405\n",
    "        raw_edges = pd.read_csv('datasets/graph.txt', header=None, sep='\\t')\n",
    "    elif adj_name == 'power':\n",
    "        nodes_numbers = 1176\n",
    "        raw_edges = pd.read_csv('datasets/power-eris1176.mtx', header=None, sep=' ')-1\n",
    "    elif adj_name == 'dublin':\n",
    "        nodes_numbers = 410\n",
    "        raw_edges = pd.read_csv('datasets/ia-infect-dublin.mtx', header=None, sep=' ') - 1\n",
    "    else:\n",
    "        print(\"Dataset is not exist!\")\n",
    "    \n",
    "    drop_self_loop = raw_edges[raw_edges[0]!=raw_edges[1]]\n",
    "    \n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    \n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "    \n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    \n",
    "    features = np.eye(nodes_numbers)\n",
    "    \n",
    "    return adj, features\n",
    "\n",
    "def get_scores(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    \n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "145f35a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:34:02.185846Z",
     "start_time": "2022-05-07T09:34:02.159482Z"
    }
   },
   "outputs": [],
   "source": [
    "def gae_for(args):\n",
    "    print(\"Using {} dataset\".format(args.dataset))\n",
    "    if args.dataset == 'Cora':\n",
    "        n_clusters = 7\n",
    "        Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "        adj, features = load_network_data(args.dataset)\n",
    "    elif args.dataset == 'wiki':\n",
    "        n_clusters = 17\n",
    "        Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "        adj, features = load_network_data(args.dataset)\n",
    "    elif args.dataset == 'power':\n",
    "        n_clusters = 12\n",
    "        Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "        adj, features = load_network_data(args.dataset)\n",
    "    elif args.dataset == 'dublin':\n",
    "        n_clusters = 12\n",
    "        Cluster = SpectralClustering(n_clusters=n_clusters, affinity = 'precomputed', random_state=0)\n",
    "        adj, features = load_network_data(args.dataset)\n",
    "\n",
    "    n_nodes, feat_dim = features.shape\n",
    "    dims = [feat_dim] + args.dims\n",
    "    \n",
    "    layers = args.linlayers\n",
    "    # Store original adjacency matrix (without diagonal entries) for later\n",
    "    \n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    adj_orig = adj\n",
    "\n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)    # val:0.05  test:0.1\n",
    "    adj = adj_train\n",
    "    n = adj.shape[0]\n",
    "\n",
    "    adj_norm_s = preprocess_graph(adj, args.gnnlayers, norm='sym', renorm=True)\n",
    "    sm_fea_s = sp.csr_matrix(features).toarray()\n",
    "    \n",
    "    print('Laplacian Smoothing...')\n",
    "    for a in adj_norm_s:\n",
    "        sm_fea_s = a.dot(sm_fea_s)\n",
    "    adj_1st = (adj + sp.eye(n)).toarray()\n",
    "\n",
    "    adj_label = torch.FloatTensor(adj_1st)\n",
    "    \n",
    "    model = LinTrans(layers, dims)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    \n",
    "    sm_fea_s = torch.FloatTensor(sm_fea_s)\n",
    "    adj_label = adj_label.reshape([-1,])\n",
    "\n",
    "    inx = sm_fea_s\n",
    "    \n",
    "    pos_num = len(adj.indices)\n",
    "    neg_num = n_nodes*n_nodes-pos_num\n",
    "\n",
    "    up_eta = (args.upth_ed - args.upth_st) / (args.epochs/args.upd)\n",
    "    low_eta = (args.lowth_ed - args.lowth_st) / (args.epochs/args.upd)\n",
    "\n",
    "    pos_inds, neg_inds = update_similarity(normalize(sm_fea_s.numpy()), args.upth_st, args.lowth_st, pos_num, neg_num)\n",
    "    upth, lowth = update_threshold(args.upth_st, args.lowth_st, up_eta, low_eta)\n",
    "\n",
    "    bs = min(args.bs, len(pos_inds))\n",
    "    length = len(pos_inds)\n",
    "    \n",
    "    pos_inds_cuda = torch.LongTensor(pos_inds)\n",
    "    best_lp = 0.\n",
    "    print('Start Training...')\n",
    "    for epoch in range(args.epochs):\n",
    "        st, ed = 0, bs\n",
    "        batch_num = 0\n",
    "        model.train()\n",
    "        length = len(pos_inds)\n",
    "        \n",
    "        while ( ed <= length ):\n",
    "            sampled_neg = torch.LongTensor(np.random.choice(neg_inds, size=ed-st))\n",
    "            sampled_inds = torch.cat((pos_inds_cuda[st:ed], sampled_neg), 0)\n",
    "            t = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            xind = sampled_inds // n_nodes\n",
    "            yind = sampled_inds % n_nodes\n",
    "            x = torch.index_select(inx, 0, xind)\n",
    "            y = torch.index_select(inx, 0, yind)\n",
    "            zx = model(x)\n",
    "            zy = model(y)\n",
    "            batch_label = torch.cat((torch.ones(ed-st), torch.zeros(ed-st)))\n",
    "            batch_pred = model.dcs(zx, zy)\n",
    "            loss = loss_function(adj_preds=batch_pred, adj_labels=batch_label, n_nodes=ed-st)\n",
    "            \n",
    "            loss.backward()\n",
    "            cur_loss = loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            st = ed\n",
    "            batch_num += 1\n",
    "            if ed < length and ed + bs >= length:\n",
    "                ed += length - ed\n",
    "            else:\n",
    "                ed += bs\n",
    "\n",
    "        if (epoch + 1) % args.upd == 0:\n",
    "            model.eval()\n",
    "            mu = model(inx)\n",
    "            hidden_emb = mu.cpu().data.numpy()\n",
    "            upth, lowth = update_threshold(upth, lowth, up_eta, low_eta)\n",
    "            pos_inds, neg_inds = update_similarity(hidden_emb, upth, lowth, pos_num, neg_num)\n",
    "            bs = min(args.bs, len(pos_inds))\n",
    "            pos_inds_cuda = torch.LongTensor(pos_inds)\n",
    "            val_auc, val_ap = get_scores(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "            if val_auc + val_ap >= best_lp:\n",
    "                best_lp = val_auc + val_ap\n",
    "                best_emb = hidden_emb\n",
    "            print(\"Epoch: {}, train_loss_gae={:.5f}, val_ap={:.5f}, val_auc={:.5f},time={:.5f}\".format(epoch + 1, cur_loss, val_ap, val_auc, time.time() - t))\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    auc_score, ap_score = get_scores(best_emb, adj_orig, test_edges, test_edges_false)\n",
    "    print('Test AP score: ',ap_score)\n",
    "    print('Test AUC score: ',auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af5372b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T09:41:22.028410Z",
     "start_time": "2022-05-07T09:34:02.831610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cora dataset\n",
      "Laplacian Smoothing...\n",
      "Start Training...\n",
      "Epoch: 10, train_loss_gae=0.74355, val_ap=0.82022, val_auc=0.76014,time=1.24022\n",
      "Epoch: 20, train_loss_gae=0.69796, val_ap=0.86341, val_auc=0.81421,time=1.23045\n",
      "Epoch: 30, train_loss_gae=0.68196, val_ap=0.87310, val_auc=0.82491,time=1.16028\n",
      "Epoch: 40, train_loss_gae=0.67294, val_ap=0.86930, val_auc=0.82089,time=1.13272\n",
      "Epoch: 50, train_loss_gae=0.66756, val_ap=0.86486, val_auc=0.81544,time=1.13945\n",
      "Epoch: 60, train_loss_gae=0.66368, val_ap=0.85860, val_auc=0.80523,time=1.24442\n",
      "Epoch: 70, train_loss_gae=0.66055, val_ap=0.85334, val_auc=0.79846,time=2.23456\n",
      "Epoch: 80, train_loss_gae=0.65741, val_ap=0.84399, val_auc=0.78464,time=2.20679\n",
      "Epoch: 90, train_loss_gae=0.65411, val_ap=0.83832, val_auc=0.77662,time=2.18959\n",
      "Epoch: 100, train_loss_gae=0.65047, val_ap=0.83229, val_auc=0.76959,time=2.24524\n",
      "Epoch: 110, train_loss_gae=0.64675, val_ap=0.82615, val_auc=0.76290,time=2.20560\n",
      "Epoch: 120, train_loss_gae=0.64280, val_ap=0.81937, val_auc=0.75702,time=2.20579\n",
      "Epoch: 130, train_loss_gae=0.63870, val_ap=0.81810, val_auc=0.75839,time=2.19157\n",
      "Epoch: 140, train_loss_gae=0.63398, val_ap=0.81563, val_auc=0.75809,time=2.34422\n",
      "Epoch: 150, train_loss_gae=0.62923, val_ap=0.81737, val_auc=0.76163,time=2.35590\n",
      "Epoch: 160, train_loss_gae=0.62448, val_ap=0.81737, val_auc=0.76335,time=2.19772\n",
      "Epoch: 170, train_loss_gae=0.61983, val_ap=0.82085, val_auc=0.76913,time=2.12120\n",
      "Epoch: 180, train_loss_gae=0.61499, val_ap=0.82378, val_auc=0.77447,time=2.17406\n",
      "Epoch: 190, train_loss_gae=0.61089, val_ap=0.82795, val_auc=0.78075,time=2.09640\n",
      "Epoch: 200, train_loss_gae=0.60683, val_ap=0.83061, val_auc=0.78375,time=2.12050\n",
      "Epoch: 210, train_loss_gae=0.60294, val_ap=0.83276, val_auc=0.78713,time=2.22147\n",
      "Epoch: 220, train_loss_gae=0.59938, val_ap=0.83541, val_auc=0.79047,time=2.21157\n",
      "Epoch: 230, train_loss_gae=0.59583, val_ap=0.83796, val_auc=0.79300,time=2.09259\n",
      "Epoch: 240, train_loss_gae=0.59301, val_ap=0.84009, val_auc=0.79651,time=2.28597\n",
      "Epoch: 250, train_loss_gae=0.58999, val_ap=0.84178, val_auc=0.79877,time=2.21798\n",
      "Epoch: 260, train_loss_gae=0.58700, val_ap=0.84306, val_auc=0.80099,time=2.12668\n",
      "Epoch: 270, train_loss_gae=0.58439, val_ap=0.84410, val_auc=0.80208,time=2.28383\n",
      "Epoch: 280, train_loss_gae=0.58204, val_ap=0.84503, val_auc=0.80299,time=2.15466\n",
      "Epoch: 290, train_loss_gae=0.57952, val_ap=0.84581, val_auc=0.80412,time=2.13590\n",
      "Epoch: 300, train_loss_gae=0.57711, val_ap=0.84755, val_auc=0.80630,time=2.06358\n",
      "Optimization Finished!\n",
      "Test AP score:  0.8568291670264863\n",
      "Test AUC score:  0.8035927108800306\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gae_for(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9e709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
