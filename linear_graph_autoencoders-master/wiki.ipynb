{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b33ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:08:23.827029Z",
     "start_time": "2022-05-04T13:08:17.301744Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from linear_gae.evaluation import get_roc_score, clustering_latent_space\n",
    "from linear_gae.kcore import compute_kcore, expand_embedding\n",
    "from linear_gae.model import *\n",
    "from linear_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from linear_gae.preprocessing import *\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0db3ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:08:25.761644Z",
     "start_time": "2022-05-04T13:08:25.749584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x2e22df4ae10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')   # 添加的，不报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32de6fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:08:26.862268Z",
     "start_time": "2022-05-04T13:08:26.842828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Available Models:\\n\\n- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\\n          GCN encoder and inner product decoder\\n\\n- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\\n           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\\n           product decoder\\n\\n- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\\n             workshop paper, with linear encoder, and inner product decoder\\n\\n- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\\n              of NeurIPS workshop paper, with Gaussian priors, linear encoders\\n              for mu and sigma, and inner product decoder\\n \\n- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\\n               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\\n               product decoder\\n \\n- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\\n                in section 4 of NeurIPS workshop paper, with Gaussian priors,\\n                3-layer GCN encoders for mu and sigma, and inner product\\n                decoder\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select graph dataset\n",
    "flags.DEFINE_string('dataset', 'power', 'Name of the graph dataset')\n",
    "\n",
    "# Select machine learning task to perform on graph\n",
    "flags.DEFINE_string('task', 'link_prediction', 'Name of the learning task')\n",
    "\n",
    "# Model\n",
    "flags.DEFINE_string('model', 'gcn_ae', 'Name of the model')\n",
    "''' Available Models:\n",
    "\n",
    "- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\n",
    "          GCN encoder and inner product decoder\n",
    "\n",
    "- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\n",
    "           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\n",
    "           product decoder\n",
    "\n",
    "- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\n",
    "             workshop paper, with linear encoder, and inner product decoder\n",
    "\n",
    "- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\n",
    "              of NeurIPS workshop paper, with Gaussian priors, linear encoders\n",
    "              for mu and sigma, and inner product decoder\n",
    " \n",
    "- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\n",
    "               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\n",
    "               product decoder\n",
    " \n",
    "- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\n",
    "                in section 4 of NeurIPS workshop paper, with Gaussian priors,\n",
    "                3-layer GCN encoders for mu and sigma, and inner product\n",
    "                decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae0231f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:08:27.729817Z",
     "start_time": "2022-05-04T13:08:27.719845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs in training.')\n",
    "flags.DEFINE_boolean('features', False, 'Include node features or not in encoder')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (with Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Number of units in GCN hidden layer(s).')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of encoder output, i.e. \\\n",
    "                                       embedding dimension')\n",
    "\n",
    "# Experimental setup parameters\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 5., 'Proportion of edges in validation set \\\n",
    "                                   (for Link Prediction task)')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      (for Link Prediction task)')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to report validation \\\n",
    "                                           results at each epoch (for \\\n",
    "                                           Link Prediction task)')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print comments details.')\n",
    "\n",
    "# Parameters related to the \"degeneracy framework\" from IJCAI 2019 paper,\n",
    "# aiming at scaling-up Graph AE/VAE by training the model only on the k-core\n",
    "# (smaller) version of the graph, then expanding embedding to remaining nodes\n",
    "# via simpler and faster heuristics\n",
    "flags.DEFINE_boolean('kcore', False, 'Whether to run k-core decomposition \\\n",
    "                                      and use the framework. False = model \\\n",
    "                                      will be trained on the entire graph')\n",
    "flags.DEFINE_integer('k', 2, 'Which k-core to use. Higher k => smaller graphs\\\n",
    "                              and faster (but maybe less accurate) training')\n",
    "flags.DEFINE_integer('nb_iterations', 10, 'Number of fix point iterations in \\\n",
    "                                           algorithm 2 of IJCAI paper. See \\\n",
    "                                           kcore.py file for details')\n",
    "\n",
    "# Lists to collect average results\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    mean_roc = []\n",
    "    mean_ap = []\n",
    "    mean_acc = []\n",
    "    mean_f1 = []\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    mean_time_kcore = []\n",
    "    mean_time_train = []\n",
    "    mean_time_expand = []\n",
    "    mean_core_size = []\n",
    "mean_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3b2844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:09:23.528013Z",
     "start_time": "2022-05-04T13:09:23.514051Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename_adj, nodes_numbers):\n",
    "    raw_edges = pd.read_csv(filename_adj, header=None, sep='\\t')\n",
    "    \n",
    "    drop_self_loop = raw_edges[raw_edges[0]!=raw_edges[1]]\n",
    "    \n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    \n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "        \n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    \n",
    "    features = sp.lil_matrix(np.eye(nodes_numbers))\n",
    "    \n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a188727e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:09:26.132030Z",
     "start_time": "2022-05-04T13:09:24.213295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load graph dataset\n",
    "if FLAGS.verbose:\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "adj_name = 'datasets/graph.txt'\n",
    "nodes_number = 2405\n",
    "adj_init, features_init = load_data(adj_name, nodes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e07281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:15:10.660505Z",
     "start_time": "2022-05-04T13:09:29.493872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73518 time= 0.29218\n",
      "Epoch: 0002 train_loss= 0.73465 time= 0.15658\n",
      "Epoch: 0003 train_loss= 0.73279 time= 0.16228\n",
      "Epoch: 0004 train_loss= 0.72861 time= 0.15292\n",
      "Epoch: 0005 train_loss= 0.72106 time= 0.15552\n",
      "Epoch: 0006 train_loss= 0.70943 time= 0.15157\n",
      "Epoch: 0007 train_loss= 0.69389 time= 0.15097\n",
      "Epoch: 0008 train_loss= 0.67673 time= 0.15246\n",
      "Epoch: 0009 train_loss= 0.66352 time= 0.15335\n",
      "Epoch: 0010 train_loss= 0.66118 time= 0.15485\n",
      "Epoch: 0011 train_loss= 0.66541 time= 0.15322\n",
      "Epoch: 0012 train_loss= 0.66293 time= 0.15173\n",
      "Epoch: 0013 train_loss= 0.65420 time= 0.15254\n",
      "Epoch: 0014 train_loss= 0.64500 time= 0.14744\n",
      "Epoch: 0015 train_loss= 0.63892 time= 0.15474\n",
      "Epoch: 0016 train_loss= 0.63621 time= 0.16927\n",
      "Epoch: 0017 train_loss= 0.63521 time= 0.17230\n",
      "Epoch: 0018 train_loss= 0.63418 time= 0.16775\n",
      "Epoch: 0019 train_loss= 0.63221 time= 0.15749\n",
      "Epoch: 0020 train_loss= 0.62913 time= 0.16903\n",
      "Epoch: 0021 train_loss= 0.62525 time= 0.15420\n",
      "Epoch: 0022 train_loss= 0.62109 time= 0.16175\n",
      "Epoch: 0023 train_loss= 0.61718 time= 0.15400\n",
      "Epoch: 0024 train_loss= 0.61378 time= 0.15605\n",
      "Epoch: 0025 train_loss= 0.61074 time= 0.15783\n",
      "Epoch: 0026 train_loss= 0.60756 time= 0.16519\n",
      "Epoch: 0027 train_loss= 0.60369 time= 0.15812\n",
      "Epoch: 0028 train_loss= 0.59891 time= 0.15954\n",
      "Epoch: 0029 train_loss= 0.59341 time= 0.16807\n",
      "Epoch: 0030 train_loss= 0.58769 time= 0.15974\n",
      "Epoch: 0031 train_loss= 0.58217 time= 0.16327\n",
      "Epoch: 0032 train_loss= 0.57704 time= 0.15167\n",
      "Epoch: 0033 train_loss= 0.57225 time= 0.16148\n",
      "Epoch: 0034 train_loss= 0.56760 time= 0.15536\n",
      "Epoch: 0035 train_loss= 0.56296 time= 0.15916\n",
      "Epoch: 0036 train_loss= 0.55842 time= 0.16085\n",
      "Epoch: 0037 train_loss= 0.55414 time= 0.16753\n",
      "Epoch: 0038 train_loss= 0.55021 time= 0.16653\n",
      "Epoch: 0039 train_loss= 0.54656 time= 0.15866\n",
      "Epoch: 0040 train_loss= 0.54299 time= 0.15410\n",
      "Epoch: 0041 train_loss= 0.53923 time= 0.15351\n",
      "Epoch: 0042 train_loss= 0.53511 time= 0.15428\n",
      "Epoch: 0043 train_loss= 0.53067 time= 0.15109\n",
      "Epoch: 0044 train_loss= 0.52614 time= 0.16257\n",
      "Epoch: 0045 train_loss= 0.52183 time= 0.15437\n",
      "Epoch: 0046 train_loss= 0.51793 time= 0.15592\n",
      "Epoch: 0047 train_loss= 0.51454 time= 0.15454\n",
      "Epoch: 0048 train_loss= 0.51164 time= 0.14481\n",
      "Epoch: 0049 train_loss= 0.50914 time= 0.15390\n",
      "Epoch: 0050 train_loss= 0.50698 time= 0.15272\n",
      "Epoch: 0051 train_loss= 0.50509 time= 0.15444\n",
      "Epoch: 0052 train_loss= 0.50345 time= 0.14978\n",
      "Epoch: 0053 train_loss= 0.50199 time= 0.15101\n",
      "Epoch: 0054 train_loss= 0.50063 time= 0.15449\n",
      "Epoch: 0055 train_loss= 0.49930 time= 0.15379\n",
      "Epoch: 0056 train_loss= 0.49790 time= 0.15143\n",
      "Epoch: 0057 train_loss= 0.49638 time= 0.16286\n",
      "Epoch: 0058 train_loss= 0.49476 time= 0.15515\n",
      "Epoch: 0059 train_loss= 0.49304 time= 0.15208\n",
      "Epoch: 0060 train_loss= 0.49127 time= 0.15336\n",
      "Epoch: 0061 train_loss= 0.48947 time= 0.14927\n",
      "Epoch: 0062 train_loss= 0.48765 time= 0.15422\n",
      "Epoch: 0063 train_loss= 0.48585 time= 0.15459\n",
      "Epoch: 0064 train_loss= 0.48407 time= 0.15095\n",
      "Epoch: 0065 train_loss= 0.48234 time= 0.16368\n",
      "Epoch: 0066 train_loss= 0.48065 time= 0.15462\n",
      "Epoch: 0067 train_loss= 0.47899 time= 0.15377\n",
      "Epoch: 0068 train_loss= 0.47733 time= 0.15519\n",
      "Epoch: 0069 train_loss= 0.47567 time= 0.15727\n",
      "Epoch: 0070 train_loss= 0.47404 time= 0.15314\n",
      "Epoch: 0071 train_loss= 0.47248 time= 0.15405\n",
      "Epoch: 0072 train_loss= 0.47105 time= 0.15586\n",
      "Epoch: 0073 train_loss= 0.46981 time= 0.15040\n",
      "Epoch: 0074 train_loss= 0.46876 time= 0.14901\n",
      "Epoch: 0075 train_loss= 0.46792 time= 0.15505\n",
      "Epoch: 0076 train_loss= 0.46723 time= 0.15368\n",
      "Epoch: 0077 train_loss= 0.46666 time= 0.15775\n",
      "Epoch: 0078 train_loss= 0.46612 time= 0.15568\n",
      "Epoch: 0079 train_loss= 0.46555 time= 0.15436\n",
      "Epoch: 0080 train_loss= 0.46487 time= 0.15213\n",
      "Epoch: 0081 train_loss= 0.46407 time= 0.15220\n",
      "Epoch: 0082 train_loss= 0.46315 time= 0.15704\n",
      "Epoch: 0083 train_loss= 0.46219 time= 0.15701\n",
      "Epoch: 0084 train_loss= 0.46121 time= 0.15426\n",
      "Epoch: 0085 train_loss= 0.46027 time= 0.15292\n",
      "Epoch: 0086 train_loss= 0.45940 time= 0.15297\n",
      "Epoch: 0087 train_loss= 0.45859 time= 0.15196\n",
      "Epoch: 0088 train_loss= 0.45785 time= 0.15491\n",
      "Epoch: 0089 train_loss= 0.45714 time= 0.15402\n",
      "Epoch: 0090 train_loss= 0.45645 time= 0.15769\n",
      "Epoch: 0091 train_loss= 0.45574 time= 0.15303\n",
      "Epoch: 0092 train_loss= 0.45501 time= 0.15181\n",
      "Epoch: 0093 train_loss= 0.45424 time= 0.15142\n",
      "Epoch: 0094 train_loss= 0.45343 time= 0.15996\n",
      "Epoch: 0095 train_loss= 0.45262 time= 0.15045\n",
      "Epoch: 0096 train_loss= 0.45179 time= 0.15616\n",
      "Epoch: 0097 train_loss= 0.45096 time= 0.15468\n",
      "Epoch: 0098 train_loss= 0.45015 time= 0.15387\n",
      "Epoch: 0099 train_loss= 0.44935 time= 0.15139\n",
      "Epoch: 0100 train_loss= 0.44857 time= 0.14939\n",
      "Epoch: 0101 train_loss= 0.44781 time= 0.15436\n",
      "Epoch: 0102 train_loss= 0.44707 time= 0.15335\n",
      "Epoch: 0103 train_loss= 0.44635 time= 0.14896\n",
      "Epoch: 0104 train_loss= 0.44565 time= 0.15023\n",
      "Epoch: 0105 train_loss= 0.44497 time= 0.15357\n",
      "Epoch: 0106 train_loss= 0.44431 time= 0.15100\n",
      "Epoch: 0107 train_loss= 0.44368 time= 0.15750\n",
      "Epoch: 0108 train_loss= 0.44307 time= 0.15507\n",
      "Epoch: 0109 train_loss= 0.44250 time= 0.15082\n",
      "Epoch: 0110 train_loss= 0.44196 time= 0.15299\n",
      "Epoch: 0111 train_loss= 0.44144 time= 0.15358\n",
      "Epoch: 0112 train_loss= 0.44094 time= 0.14902\n",
      "Epoch: 0113 train_loss= 0.44046 time= 0.14973\n",
      "Epoch: 0114 train_loss= 0.44000 time= 0.15532\n",
      "Epoch: 0115 train_loss= 0.43954 time= 0.15243\n",
      "Epoch: 0116 train_loss= 0.43908 time= 0.15356\n",
      "Epoch: 0117 train_loss= 0.43862 time= 0.15173\n",
      "Epoch: 0118 train_loss= 0.43815 time= 0.15470\n",
      "Epoch: 0119 train_loss= 0.43767 time= 0.15333\n",
      "Epoch: 0120 train_loss= 0.43720 time= 0.15238\n",
      "Epoch: 0121 train_loss= 0.43672 time= 0.15962\n",
      "Epoch: 0122 train_loss= 0.43625 time= 0.15441\n",
      "Epoch: 0123 train_loss= 0.43578 time= 0.15445\n",
      "Epoch: 0124 train_loss= 0.43532 time= 0.15734\n",
      "Epoch: 0125 train_loss= 0.43487 time= 0.15138\n",
      "Epoch: 0126 train_loss= 0.43443 time= 0.14918\n",
      "Epoch: 0127 train_loss= 0.43400 time= 0.15220\n",
      "Epoch: 0128 train_loss= 0.43359 time= 0.16157\n",
      "Epoch: 0129 train_loss= 0.43319 time= 0.14989\n",
      "Epoch: 0130 train_loss= 0.43282 time= 0.14748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0131 train_loss= 0.43246 time= 0.15901\n",
      "Epoch: 0132 train_loss= 0.43213 time= 0.15293\n",
      "Epoch: 0133 train_loss= 0.43182 time= 0.15510\n",
      "Epoch: 0134 train_loss= 0.43154 time= 0.15343\n",
      "Epoch: 0135 train_loss= 0.43127 time= 0.14926\n",
      "Epoch: 0136 train_loss= 0.43102 time= 0.15907\n",
      "Epoch: 0137 train_loss= 0.43077 time= 0.15234\n",
      "Epoch: 0138 train_loss= 0.43052 time= 0.15589\n",
      "Epoch: 0139 train_loss= 0.43026 time= 0.14767\n",
      "Epoch: 0140 train_loss= 0.43000 time= 0.15513\n",
      "Epoch: 0141 train_loss= 0.42974 time= 0.15221\n",
      "Epoch: 0142 train_loss= 0.42946 time= 0.15858\n",
      "Epoch: 0143 train_loss= 0.42919 time= 0.15374\n",
      "Epoch: 0144 train_loss= 0.42891 time= 0.16114\n",
      "Epoch: 0145 train_loss= 0.42863 time= 0.15371\n",
      "Epoch: 0146 train_loss= 0.42835 time= 0.15401\n",
      "Epoch: 0147 train_loss= 0.42808 time= 0.14911\n",
      "Epoch: 0148 train_loss= 0.42782 time= 0.15142\n",
      "Epoch: 0149 train_loss= 0.42756 time= 0.14997\n",
      "Epoch: 0150 train_loss= 0.42731 time= 0.15680\n",
      "Epoch: 0151 train_loss= 0.42707 time= 0.15080\n",
      "Epoch: 0152 train_loss= 0.42683 time= 0.15097\n",
      "Epoch: 0153 train_loss= 0.42660 time= 0.15876\n",
      "Epoch: 0154 train_loss= 0.42638 time= 0.15272\n",
      "Epoch: 0155 train_loss= 0.42616 time= 0.15825\n",
      "Epoch: 0156 train_loss= 0.42595 time= 0.15480\n",
      "Epoch: 0157 train_loss= 0.42575 time= 0.15532\n",
      "Epoch: 0158 train_loss= 0.42555 time= 0.15500\n",
      "Epoch: 0159 train_loss= 0.42536 time= 0.15451\n",
      "Epoch: 0160 train_loss= 0.42518 time= 0.16070\n",
      "Epoch: 0161 train_loss= 0.42500 time= 0.15240\n",
      "Epoch: 0162 train_loss= 0.42482 time= 0.15515\n",
      "Epoch: 0163 train_loss= 0.42465 time= 0.16780\n",
      "Epoch: 0164 train_loss= 0.42448 time= 0.14707\n",
      "Epoch: 0165 train_loss= 0.42431 time= 0.15190\n",
      "Epoch: 0166 train_loss= 0.42413 time= 0.15658\n",
      "Epoch: 0167 train_loss= 0.42396 time= 0.15325\n",
      "Epoch: 0168 train_loss= 0.42379 time= 0.15169\n",
      "Epoch: 0169 train_loss= 0.42361 time= 0.15074\n",
      "Epoch: 0170 train_loss= 0.42343 time= 0.15832\n",
      "Epoch: 0171 train_loss= 0.42325 time= 0.15451\n",
      "Epoch: 0172 train_loss= 0.42307 time= 0.15679\n",
      "Epoch: 0173 train_loss= 0.42289 time= 0.15436\n",
      "Epoch: 0174 train_loss= 0.42271 time= 0.15261\n",
      "Epoch: 0175 train_loss= 0.42252 time= 0.14903\n",
      "Epoch: 0176 train_loss= 0.42233 time= 0.15382\n",
      "Epoch: 0177 train_loss= 0.42214 time= 0.15111\n",
      "Epoch: 0178 train_loss= 0.42195 time= 0.14726\n",
      "Epoch: 0179 train_loss= 0.42175 time= 0.15788\n",
      "Epoch: 0180 train_loss= 0.42155 time= 0.15294\n",
      "Epoch: 0181 train_loss= 0.42134 time= 0.15114\n",
      "Epoch: 0182 train_loss= 0.42113 time= 0.15099\n",
      "Epoch: 0183 train_loss= 0.42092 time= 0.15203\n",
      "Epoch: 0184 train_loss= 0.42070 time= 0.15439\n",
      "Epoch: 0185 train_loss= 0.42048 time= 0.15579\n",
      "Epoch: 0186 train_loss= 0.42026 time= 0.15310\n",
      "Epoch: 0187 train_loss= 0.42003 time= 0.15448\n",
      "Epoch: 0188 train_loss= 0.41979 time= 0.15049\n",
      "Epoch: 0189 train_loss= 0.41956 time= 0.15608\n",
      "Epoch: 0190 train_loss= 0.41932 time= 0.14687\n",
      "Epoch: 0191 train_loss= 0.41908 time= 0.14717\n",
      "Epoch: 0192 train_loss= 0.41884 time= 0.15616\n",
      "Epoch: 0193 train_loss= 0.41860 time= 0.15447\n",
      "Epoch: 0194 train_loss= 0.41836 time= 0.15812\n",
      "Epoch: 0195 train_loss= 0.41812 time= 0.15392\n",
      "Epoch: 0196 train_loss= 0.41788 time= 0.15285\n",
      "Epoch: 0197 train_loss= 0.41765 time= 0.15559\n",
      "Epoch: 0198 train_loss= 0.41743 time= 0.15628\n",
      "Epoch: 0199 train_loss= 0.41721 time= 0.15934\n",
      "Epoch: 0200 train_loss= 0.41700 time= 0.15437\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73520 time= 0.21522\n",
      "Epoch: 0002 train_loss= 0.73485 time= 0.14915\n",
      "Epoch: 0003 train_loss= 0.73365 time= 0.16456\n",
      "Epoch: 0004 train_loss= 0.73085 time= 0.15260\n",
      "Epoch: 0005 train_loss= 0.72558 time= 0.15107\n",
      "Epoch: 0006 train_loss= 0.71696 time= 0.15384\n",
      "Epoch: 0007 train_loss= 0.70446 time= 0.15537\n",
      "Epoch: 0008 train_loss= 0.68865 time= 0.14919\n",
      "Epoch: 0009 train_loss= 0.67229 time= 0.15431\n",
      "Epoch: 0010 train_loss= 0.66135 time= 0.15332\n",
      "Epoch: 0011 train_loss= 0.66129 time= 0.15420\n",
      "Epoch: 0012 train_loss= 0.66415 time= 0.15201\n",
      "Epoch: 0013 train_loss= 0.65974 time= 0.15292\n",
      "Epoch: 0014 train_loss= 0.65043 time= 0.15118\n",
      "Epoch: 0015 train_loss= 0.64137 time= 0.15342\n",
      "Epoch: 0016 train_loss= 0.63528 time= 0.15708\n",
      "Epoch: 0017 train_loss= 0.63206 time= 0.15813\n",
      "Epoch: 0018 train_loss= 0.63015 time= 0.15316\n",
      "Epoch: 0019 train_loss= 0.62811 time= 0.15284\n",
      "Epoch: 0020 train_loss= 0.62519 time= 0.15375\n",
      "Epoch: 0021 train_loss= 0.62127 time= 0.15195\n",
      "Epoch: 0022 train_loss= 0.61660 time= 0.15511\n",
      "Epoch: 0023 train_loss= 0.61153 time= 0.16008\n",
      "Epoch: 0024 train_loss= 0.60638 time= 0.15631\n",
      "Epoch: 0025 train_loss= 0.60132 time= 0.15217\n",
      "Epoch: 0026 train_loss= 0.59628 time= 0.15268\n",
      "Epoch: 0027 train_loss= 0.59105 time= 0.14778\n",
      "Epoch: 0028 train_loss= 0.58541 time= 0.15288\n",
      "Epoch: 0029 train_loss= 0.57926 time= 0.15297\n",
      "Epoch: 0030 train_loss= 0.57277 time= 0.15476\n",
      "Epoch: 0031 train_loss= 0.56626 time= 0.15480\n",
      "Epoch: 0032 train_loss= 0.56006 time= 0.15296\n",
      "Epoch: 0033 train_loss= 0.55444 time= 0.15082\n",
      "Epoch: 0034 train_loss= 0.54950 time= 0.15305\n",
      "Epoch: 0035 train_loss= 0.54512 time= 0.15521\n",
      "Epoch: 0036 train_loss= 0.54110 time= 0.15060\n",
      "Epoch: 0037 train_loss= 0.53725 time= 0.15825\n",
      "Epoch: 0038 train_loss= 0.53349 time= 0.15477\n",
      "Epoch: 0039 train_loss= 0.52990 time= 0.14997\n",
      "Epoch: 0040 train_loss= 0.52663 time= 0.14881\n",
      "Epoch: 0041 train_loss= 0.52385 time= 0.15212\n",
      "Epoch: 0042 train_loss= 0.52162 time= 0.15465\n",
      "Epoch: 0043 train_loss= 0.51980 time= 0.16317\n",
      "Epoch: 0044 train_loss= 0.51813 time= 0.16101\n",
      "Epoch: 0045 train_loss= 0.51635 time= 0.16823\n",
      "Epoch: 0046 train_loss= 0.51433 time= 0.16646\n",
      "Epoch: 0047 train_loss= 0.51207 time= 0.15515\n",
      "Epoch: 0048 train_loss= 0.50969 time= 0.16245\n",
      "Epoch: 0049 train_loss= 0.50738 time= 0.15797\n",
      "Epoch: 0050 train_loss= 0.50527 time= 0.16611\n",
      "Epoch: 0051 train_loss= 0.50347 time= 0.15949\n",
      "Epoch: 0052 train_loss= 0.50199 time= 0.16437\n",
      "Epoch: 0053 train_loss= 0.50082 time= 0.15797\n",
      "Epoch: 0054 train_loss= 0.49992 time= 0.16566\n",
      "Epoch: 0055 train_loss= 0.49918 time= 0.18151\n",
      "Epoch: 0056 train_loss= 0.49853 time= 0.16749\n",
      "Epoch: 0057 train_loss= 0.49789 time= 0.16146\n",
      "Epoch: 0058 train_loss= 0.49718 time= 0.15828\n",
      "Epoch: 0059 train_loss= 0.49637 time= 0.15703\n",
      "Epoch: 0060 train_loss= 0.49543 time= 0.17367\n",
      "Epoch: 0061 train_loss= 0.49435 time= 0.15817\n",
      "Epoch: 0062 train_loss= 0.49316 time= 0.16249\n",
      "Epoch: 0063 train_loss= 0.49193 time= 0.16640\n",
      "Epoch: 0064 train_loss= 0.49069 time= 0.16363\n",
      "Epoch: 0065 train_loss= 0.48949 time= 0.16179\n",
      "Epoch: 0066 train_loss= 0.48833 time= 0.16263\n",
      "Epoch: 0067 train_loss= 0.48720 time= 0.17332\n",
      "Epoch: 0068 train_loss= 0.48609 time= 0.15644\n",
      "Epoch: 0069 train_loss= 0.48497 time= 0.16457\n",
      "Epoch: 0070 train_loss= 0.48384 time= 0.15885\n",
      "Epoch: 0071 train_loss= 0.48268 time= 0.16955\n",
      "Epoch: 0072 train_loss= 0.48145 time= 0.16318\n",
      "Epoch: 0073 train_loss= 0.48015 time= 0.15924\n",
      "Epoch: 0074 train_loss= 0.47875 time= 0.16585\n",
      "Epoch: 0075 train_loss= 0.47728 time= 0.16278\n",
      "Epoch: 0076 train_loss= 0.47575 time= 0.18218\n",
      "Epoch: 0077 train_loss= 0.47418 time= 0.16600\n",
      "Epoch: 0078 train_loss= 0.47258 time= 0.17546\n",
      "Epoch: 0079 train_loss= 0.47095 time= 0.17798\n",
      "Epoch: 0080 train_loss= 0.46930 time= 0.16701\n",
      "Epoch: 0081 train_loss= 0.46766 time= 0.17591\n",
      "Epoch: 0082 train_loss= 0.46606 time= 0.17782\n",
      "Epoch: 0083 train_loss= 0.46454 time= 0.16764\n",
      "Epoch: 0084 train_loss= 0.46313 time= 0.18270\n",
      "Epoch: 0085 train_loss= 0.46184 time= 0.17099\n",
      "Epoch: 0086 train_loss= 0.46068 time= 0.16423\n",
      "Epoch: 0087 train_loss= 0.45967 time= 0.16923\n",
      "Epoch: 0088 train_loss= 0.45878 time= 0.15273\n",
      "Epoch: 0089 train_loss= 0.45799 time= 0.17080\n",
      "Epoch: 0090 train_loss= 0.45728 time= 0.17543\n",
      "Epoch: 0091 train_loss= 0.45661 time= 0.16865\n",
      "Epoch: 0092 train_loss= 0.45596 time= 0.17798\n",
      "Epoch: 0093 train_loss= 0.45532 time= 0.18312\n",
      "Epoch: 0094 train_loss= 0.45469 time= 0.17806\n",
      "Epoch: 0095 train_loss= 0.45404 time= 0.16892\n",
      "Epoch: 0096 train_loss= 0.45338 time= 0.17612\n",
      "Epoch: 0097 train_loss= 0.45272 time= 0.17547\n",
      "Epoch: 0098 train_loss= 0.45207 time= 0.17064\n",
      "Epoch: 0099 train_loss= 0.45146 time= 0.17445\n",
      "Epoch: 0100 train_loss= 0.45087 time= 0.16968\n",
      "Epoch: 0101 train_loss= 0.45032 time= 0.17177\n",
      "Epoch: 0102 train_loss= 0.44980 time= 0.17554\n",
      "Epoch: 0103 train_loss= 0.44930 time= 0.17531\n",
      "Epoch: 0104 train_loss= 0.44879 time= 0.16844\n",
      "Epoch: 0105 train_loss= 0.44828 time= 0.16595\n",
      "Epoch: 0106 train_loss= 0.44776 time= 0.18139\n",
      "Epoch: 0107 train_loss= 0.44723 time= 0.18513\n",
      "Epoch: 0108 train_loss= 0.44669 time= 0.17334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0109 train_loss= 0.44614 time= 0.18274\n",
      "Epoch: 0110 train_loss= 0.44560 time= 0.17649\n",
      "Epoch: 0111 train_loss= 0.44505 time= 0.17341\n",
      "Epoch: 0112 train_loss= 0.44449 time= 0.18361\n",
      "Epoch: 0113 train_loss= 0.44393 time= 0.16458\n",
      "Epoch: 0114 train_loss= 0.44334 time= 0.18263\n",
      "Epoch: 0115 train_loss= 0.44273 time= 0.17281\n",
      "Epoch: 0116 train_loss= 0.44210 time= 0.16585\n",
      "Epoch: 0117 train_loss= 0.44144 time= 0.16998\n",
      "Epoch: 0118 train_loss= 0.44077 time= 0.17099\n",
      "Epoch: 0119 train_loss= 0.44010 time= 0.16821\n",
      "Epoch: 0120 train_loss= 0.43942 time= 0.17334\n",
      "Epoch: 0121 train_loss= 0.43876 time= 0.16537\n",
      "Epoch: 0122 train_loss= 0.43812 time= 0.16921\n",
      "Epoch: 0123 train_loss= 0.43752 time= 0.16648\n",
      "Epoch: 0124 train_loss= 0.43695 time= 0.15547\n",
      "Epoch: 0125 train_loss= 0.43643 time= 0.17121\n",
      "Epoch: 0126 train_loss= 0.43596 time= 0.16070\n",
      "Epoch: 0127 train_loss= 0.43555 time= 0.17285\n",
      "Epoch: 0128 train_loss= 0.43519 time= 0.18628\n",
      "Epoch: 0129 train_loss= 0.43487 time= 0.17415\n",
      "Epoch: 0130 train_loss= 0.43460 time= 0.17267\n",
      "Epoch: 0131 train_loss= 0.43435 time= 0.17596\n",
      "Epoch: 0132 train_loss= 0.43411 time= 0.17170\n",
      "Epoch: 0133 train_loss= 0.43388 time= 0.16526\n",
      "Epoch: 0134 train_loss= 0.43366 time= 0.16995\n",
      "Epoch: 0135 train_loss= 0.43342 time= 0.16715\n",
      "Epoch: 0136 train_loss= 0.43318 time= 0.17265\n",
      "Epoch: 0137 train_loss= 0.43293 time= 0.16155\n",
      "Epoch: 0138 train_loss= 0.43268 time= 0.17460\n",
      "Epoch: 0139 train_loss= 0.43242 time= 0.16125\n",
      "Epoch: 0140 train_loss= 0.43216 time= 0.16532\n",
      "Epoch: 0141 train_loss= 0.43191 time= 0.16424\n",
      "Epoch: 0142 train_loss= 0.43167 time= 0.17096\n",
      "Epoch: 0143 train_loss= 0.43143 time= 0.15999\n",
      "Epoch: 0144 train_loss= 0.43120 time= 0.17790\n",
      "Epoch: 0145 train_loss= 0.43098 time= 0.16426\n",
      "Epoch: 0146 train_loss= 0.43076 time= 0.16721\n",
      "Epoch: 0147 train_loss= 0.43054 time= 0.15921\n",
      "Epoch: 0148 train_loss= 0.43032 time= 0.16464\n",
      "Epoch: 0149 train_loss= 0.43009 time= 0.17334\n",
      "Epoch: 0150 train_loss= 0.42986 time= 0.16362\n",
      "Epoch: 0151 train_loss= 0.42962 time= 0.17512\n",
      "Epoch: 0152 train_loss= 0.42937 time= 0.15798\n",
      "Epoch: 0153 train_loss= 0.42912 time= 0.16771\n",
      "Epoch: 0154 train_loss= 0.42885 time= 0.16941\n",
      "Epoch: 0155 train_loss= 0.42858 time= 0.17167\n",
      "Epoch: 0156 train_loss= 0.42830 time= 0.16287\n",
      "Epoch: 0157 train_loss= 0.42801 time= 0.16959\n",
      "Epoch: 0158 train_loss= 0.42772 time= 0.17322\n",
      "Epoch: 0159 train_loss= 0.42741 time= 0.16183\n",
      "Epoch: 0160 train_loss= 0.42710 time= 0.17186\n",
      "Epoch: 0161 train_loss= 0.42677 time= 0.16387\n",
      "Epoch: 0162 train_loss= 0.42643 time= 0.16630\n",
      "Epoch: 0163 train_loss= 0.42608 time= 0.17031\n",
      "Epoch: 0164 train_loss= 0.42572 time= 0.17117\n",
      "Epoch: 0165 train_loss= 0.42534 time= 0.15656\n",
      "Epoch: 0166 train_loss= 0.42496 time= 0.17105\n",
      "Epoch: 0167 train_loss= 0.42457 time= 0.15708\n",
      "Epoch: 0168 train_loss= 0.42417 time= 0.16292\n",
      "Epoch: 0169 train_loss= 0.42376 time= 0.16840\n",
      "Epoch: 0170 train_loss= 0.42335 time= 0.16124\n",
      "Epoch: 0171 train_loss= 0.42295 time= 0.15734\n",
      "Epoch: 0172 train_loss= 0.42255 time= 0.16209\n",
      "Epoch: 0173 train_loss= 0.42217 time= 0.17070\n",
      "Epoch: 0174 train_loss= 0.42181 time= 0.16477\n",
      "Epoch: 0175 train_loss= 0.42146 time= 0.17562\n",
      "Epoch: 0176 train_loss= 0.42115 time= 0.16745\n",
      "Epoch: 0177 train_loss= 0.42085 time= 0.17146\n",
      "Epoch: 0178 train_loss= 0.42059 time= 0.16754\n",
      "Epoch: 0179 train_loss= 0.42035 time= 0.17470\n",
      "Epoch: 0180 train_loss= 0.42013 time= 0.16059\n",
      "Epoch: 0181 train_loss= 0.41992 time= 0.16882\n",
      "Epoch: 0182 train_loss= 0.41973 time= 0.16242\n",
      "Epoch: 0183 train_loss= 0.41954 time= 0.16007\n",
      "Epoch: 0184 train_loss= 0.41935 time= 0.17032\n",
      "Epoch: 0185 train_loss= 0.41915 time= 0.17334\n",
      "Epoch: 0186 train_loss= 0.41895 time= 0.17769\n",
      "Epoch: 0187 train_loss= 0.41875 time= 0.17410\n",
      "Epoch: 0188 train_loss= 0.41854 time= 0.16755\n",
      "Epoch: 0189 train_loss= 0.41833 time= 0.16524\n",
      "Epoch: 0190 train_loss= 0.41811 time= 0.16800\n",
      "Epoch: 0191 train_loss= 0.41790 time= 0.16196\n",
      "Epoch: 0192 train_loss= 0.41769 time= 0.17493\n",
      "Epoch: 0193 train_loss= 0.41748 time= 0.16486\n",
      "Epoch: 0194 train_loss= 0.41728 time= 0.16374\n",
      "Epoch: 0195 train_loss= 0.41707 time= 0.16139\n",
      "Epoch: 0196 train_loss= 0.41686 time= 0.16673\n",
      "Epoch: 0197 train_loss= 0.41666 time= 0.16923\n",
      "Epoch: 0198 train_loss= 0.41645 time= 0.16175\n",
      "Epoch: 0199 train_loss= 0.41624 time= 0.16621\n",
      "Epoch: 0200 train_loss= 0.41604 time= 0.16943\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73521 time= 0.27368\n",
      "Epoch: 0002 train_loss= 0.73483 time= 0.15957\n",
      "Epoch: 0003 train_loss= 0.73354 time= 0.16577\n",
      "Epoch: 0004 train_loss= 0.73063 time= 0.16356\n",
      "Epoch: 0005 train_loss= 0.72530 time= 0.15918\n",
      "Epoch: 0006 train_loss= 0.71676 time= 0.16576\n",
      "Epoch: 0007 train_loss= 0.70458 time= 0.16069\n",
      "Epoch: 0008 train_loss= 0.68923 time= 0.16424\n",
      "Epoch: 0009 train_loss= 0.67306 time= 0.16755\n",
      "Epoch: 0010 train_loss= 0.66111 time= 0.17368\n",
      "Epoch: 0011 train_loss= 0.65891 time= 0.17101\n",
      "Epoch: 0012 train_loss= 0.66213 time= 0.16726\n",
      "Epoch: 0013 train_loss= 0.65955 time= 0.16067\n",
      "Epoch: 0014 train_loss= 0.65096 time= 0.17363\n",
      "Epoch: 0015 train_loss= 0.64125 time= 0.15611\n",
      "Epoch: 0016 train_loss= 0.63392 time= 0.16181\n",
      "Epoch: 0017 train_loss= 0.62962 time= 0.17023\n",
      "Epoch: 0018 train_loss= 0.62708 time= 0.15958\n",
      "Epoch: 0019 train_loss= 0.62467 time= 0.16784\n",
      "Epoch: 0020 train_loss= 0.62132 time= 0.16587\n",
      "Epoch: 0021 train_loss= 0.61670 time= 0.17295\n",
      "Epoch: 0022 train_loss= 0.61098 time= 0.16758\n",
      "Epoch: 0023 train_loss= 0.60459 time= 0.16953\n",
      "Epoch: 0024 train_loss= 0.59800 time= 0.16359\n",
      "Epoch: 0025 train_loss= 0.59159 time= 0.16176\n",
      "Epoch: 0026 train_loss= 0.58545 time= 0.15781\n",
      "Epoch: 0027 train_loss= 0.57941 time= 0.17354\n",
      "Epoch: 0028 train_loss= 0.57322 time= 0.15891\n",
      "Epoch: 0029 train_loss= 0.56676 time= 0.17042\n",
      "Epoch: 0030 train_loss= 0.56028 time= 0.16196\n",
      "Epoch: 0031 train_loss= 0.55430 time= 0.16638\n",
      "Epoch: 0032 train_loss= 0.54937 time= 0.16564\n",
      "Epoch: 0033 train_loss= 0.54572 time= 0.17109\n",
      "Epoch: 0034 train_loss= 0.54309 time= 0.16743\n",
      "Epoch: 0035 train_loss= 0.54097 time= 0.15705\n",
      "Epoch: 0036 train_loss= 0.53894 time= 0.16815\n",
      "Epoch: 0037 train_loss= 0.53693 time= 0.15729\n",
      "Epoch: 0038 train_loss= 0.53496 time= 0.17219\n",
      "Epoch: 0039 train_loss= 0.53304 time= 0.15433\n",
      "Epoch: 0040 train_loss= 0.53110 time= 0.17920\n",
      "Epoch: 0041 train_loss= 0.52913 time= 0.16814\n",
      "Epoch: 0042 train_loss= 0.52714 time= 0.16127\n",
      "Epoch: 0043 train_loss= 0.52520 time= 0.17983\n",
      "Epoch: 0044 train_loss= 0.52328 time= 0.16521\n",
      "Epoch: 0045 train_loss= 0.52132 time= 0.17303\n",
      "Epoch: 0046 train_loss= 0.51927 time= 0.16756\n",
      "Epoch: 0047 train_loss= 0.51711 time= 0.16859\n",
      "Epoch: 0048 train_loss= 0.51489 time= 0.16703\n",
      "Epoch: 0049 train_loss= 0.51266 time= 0.16922\n",
      "Epoch: 0050 train_loss= 0.51047 time= 0.16118\n",
      "Epoch: 0051 train_loss= 0.50828 time= 0.18177\n",
      "Epoch: 0052 train_loss= 0.50603 time= 0.17478\n",
      "Epoch: 0053 train_loss= 0.50365 time= 0.17086\n",
      "Epoch: 0054 train_loss= 0.50115 time= 0.16689\n",
      "Epoch: 0055 train_loss= 0.49856 time= 0.17287\n",
      "Epoch: 0056 train_loss= 0.49596 time= 0.16650\n",
      "Epoch: 0057 train_loss= 0.49345 time= 0.16730\n",
      "Epoch: 0058 train_loss= 0.49109 time= 0.17310\n",
      "Epoch: 0059 train_loss= 0.48888 time= 0.16400\n",
      "Epoch: 0060 train_loss= 0.48682 time= 0.16287\n",
      "Epoch: 0061 train_loss= 0.48489 time= 0.16487\n",
      "Epoch: 0062 train_loss= 0.48307 time= 0.16322\n",
      "Epoch: 0063 train_loss= 0.48133 time= 0.15873\n",
      "Epoch: 0064 train_loss= 0.47963 time= 0.16655\n",
      "Epoch: 0065 train_loss= 0.47791 time= 0.16572\n",
      "Epoch: 0066 train_loss= 0.47612 time= 0.17164\n",
      "Epoch: 0067 train_loss= 0.47428 time= 0.17720\n",
      "Epoch: 0068 train_loss= 0.47246 time= 0.17171\n",
      "Epoch: 0069 train_loss= 0.47076 time= 0.16557\n",
      "Epoch: 0070 train_loss= 0.46926 time= 0.16196\n",
      "Epoch: 0071 train_loss= 0.46797 time= 0.16825\n",
      "Epoch: 0072 train_loss= 0.46689 time= 0.15510\n",
      "Epoch: 0073 train_loss= 0.46596 time= 0.18173\n",
      "Epoch: 0074 train_loss= 0.46514 time= 0.16383\n",
      "Epoch: 0075 train_loss= 0.46436 time= 0.16095\n",
      "Epoch: 0076 train_loss= 0.46356 time= 0.16825\n",
      "Epoch: 0077 train_loss= 0.46266 time= 0.18218\n",
      "Epoch: 0078 train_loss= 0.46166 time= 0.17326\n",
      "Epoch: 0079 train_loss= 0.46055 time= 0.17252\n",
      "Epoch: 0080 train_loss= 0.45939 time= 0.17149\n",
      "Epoch: 0081 train_loss= 0.45820 time= 0.16915\n",
      "Epoch: 0082 train_loss= 0.45703 time= 0.16890\n",
      "Epoch: 0083 train_loss= 0.45590 time= 0.17564\n",
      "Epoch: 0084 train_loss= 0.45482 time= 0.15918\n",
      "Epoch: 0085 train_loss= 0.45381 time= 0.16397\n",
      "Epoch: 0086 train_loss= 0.45290 time= 0.16753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 train_loss= 0.45207 time= 0.16042\n",
      "Epoch: 0088 train_loss= 0.45132 time= 0.16591\n",
      "Epoch: 0089 train_loss= 0.45061 time= 0.16442\n",
      "Epoch: 0090 train_loss= 0.44992 time= 0.18067\n",
      "Epoch: 0091 train_loss= 0.44924 time= 0.16836\n",
      "Epoch: 0092 train_loss= 0.44858 time= 0.17669\n",
      "Epoch: 0093 train_loss= 0.44792 time= 0.16953\n",
      "Epoch: 0094 train_loss= 0.44727 time= 0.17254\n",
      "Epoch: 0095 train_loss= 0.44662 time= 0.16365\n",
      "Epoch: 0096 train_loss= 0.44598 time= 0.17489\n",
      "Epoch: 0097 train_loss= 0.44534 time= 0.17561\n",
      "Epoch: 0098 train_loss= 0.44469 time= 0.16906\n",
      "Epoch: 0099 train_loss= 0.44402 time= 0.15866\n",
      "Epoch: 0100 train_loss= 0.44334 time= 0.16830\n",
      "Epoch: 0101 train_loss= 0.44264 time= 0.17300\n",
      "Epoch: 0102 train_loss= 0.44194 time= 0.17002\n",
      "Epoch: 0103 train_loss= 0.44124 time= 0.18202\n",
      "Epoch: 0104 train_loss= 0.44055 time= 0.16927\n",
      "Epoch: 0105 train_loss= 0.43988 time= 0.18198\n",
      "Epoch: 0106 train_loss= 0.43924 time= 0.17235\n",
      "Epoch: 0107 train_loss= 0.43863 time= 0.16454\n",
      "Epoch: 0108 train_loss= 0.43806 time= 0.17804\n",
      "Epoch: 0109 train_loss= 0.43754 time= 0.16323\n",
      "Epoch: 0110 train_loss= 0.43707 time= 0.17734\n",
      "Epoch: 0111 train_loss= 0.43664 time= 0.18450\n",
      "Epoch: 0112 train_loss= 0.43626 time= 0.15957\n",
      "Epoch: 0113 train_loss= 0.43592 time= 0.18168\n",
      "Epoch: 0114 train_loss= 0.43560 time= 0.16448\n",
      "Epoch: 0115 train_loss= 0.43529 time= 0.16936\n",
      "Epoch: 0116 train_loss= 0.43498 time= 0.17804\n",
      "Epoch: 0117 train_loss= 0.43468 time= 0.18510\n",
      "Epoch: 0118 train_loss= 0.43437 time= 0.17403\n",
      "Epoch: 0119 train_loss= 0.43405 time= 0.17318\n",
      "Epoch: 0120 train_loss= 0.43373 time= 0.17437\n",
      "Epoch: 0121 train_loss= 0.43341 time= 0.16759\n",
      "Epoch: 0122 train_loss= 0.43309 time= 0.19041\n",
      "Epoch: 0123 train_loss= 0.43279 time= 0.17813\n",
      "Epoch: 0124 train_loss= 0.43251 time= 0.17125\n",
      "Epoch: 0125 train_loss= 0.43224 time= 0.16616\n",
      "Epoch: 0126 train_loss= 0.43198 time= 0.17815\n",
      "Epoch: 0127 train_loss= 0.43174 time= 0.17414\n",
      "Epoch: 0128 train_loss= 0.43150 time= 0.17119\n",
      "Epoch: 0129 train_loss= 0.43128 time= 0.17516\n",
      "Epoch: 0130 train_loss= 0.43105 time= 0.16977\n",
      "Epoch: 0131 train_loss= 0.43083 time= 0.17703\n",
      "Epoch: 0132 train_loss= 0.43061 time= 0.16078\n",
      "Epoch: 0133 train_loss= 0.43039 time= 0.17404\n",
      "Epoch: 0134 train_loss= 0.43016 time= 0.16523\n",
      "Epoch: 0135 train_loss= 0.42993 time= 0.18291\n",
      "Epoch: 0136 train_loss= 0.42969 time= 0.17091\n",
      "Epoch: 0137 train_loss= 0.42946 time= 0.17767\n",
      "Epoch: 0138 train_loss= 0.42922 time= 0.17930\n",
      "Epoch: 0139 train_loss= 0.42897 time= 0.17388\n",
      "Epoch: 0140 train_loss= 0.42873 time= 0.16655\n",
      "Epoch: 0141 train_loss= 0.42848 time= 0.16210\n",
      "Epoch: 0142 train_loss= 0.42824 time= 0.18092\n",
      "Epoch: 0143 train_loss= 0.42799 time= 0.18352\n",
      "Epoch: 0144 train_loss= 0.42774 time= 0.16425\n",
      "Epoch: 0145 train_loss= 0.42748 time= 0.15980\n",
      "Epoch: 0146 train_loss= 0.42723 time= 0.18005\n",
      "Epoch: 0147 train_loss= 0.42697 time= 0.18478\n",
      "Epoch: 0148 train_loss= 0.42671 time= 0.18754\n",
      "Epoch: 0149 train_loss= 0.42644 time= 0.18338\n",
      "Epoch: 0150 train_loss= 0.42618 time= 0.17516\n",
      "Epoch: 0151 train_loss= 0.42591 time= 0.16047\n",
      "Epoch: 0152 train_loss= 0.42565 time= 0.16483\n",
      "Epoch: 0153 train_loss= 0.42538 time= 0.17185\n",
      "Epoch: 0154 train_loss= 0.42512 time= 0.16671\n",
      "Epoch: 0155 train_loss= 0.42487 time= 0.16143\n",
      "Epoch: 0156 train_loss= 0.42461 time= 0.15396\n",
      "Epoch: 0157 train_loss= 0.42437 time= 0.17453\n",
      "Epoch: 0158 train_loss= 0.42413 time= 0.17099\n",
      "Epoch: 0159 train_loss= 0.42389 time= 0.17193\n",
      "Epoch: 0160 train_loss= 0.42366 time= 0.16200\n",
      "Epoch: 0161 train_loss= 0.42342 time= 0.17126\n",
      "Epoch: 0162 train_loss= 0.42319 time= 0.16365\n",
      "Epoch: 0163 train_loss= 0.42295 time= 0.17199\n",
      "Epoch: 0164 train_loss= 0.42271 time= 0.17020\n",
      "Epoch: 0165 train_loss= 0.42247 time= 0.16514\n",
      "Epoch: 0166 train_loss= 0.42222 time= 0.16780\n",
      "Epoch: 0167 train_loss= 0.42196 time= 0.15896\n",
      "Epoch: 0168 train_loss= 0.42171 time= 0.16306\n",
      "Epoch: 0169 train_loss= 0.42144 time= 0.16997\n",
      "Epoch: 0170 train_loss= 0.42118 time= 0.17187\n",
      "Epoch: 0171 train_loss= 0.42091 time= 0.16288\n",
      "Epoch: 0172 train_loss= 0.42064 time= 0.16694\n",
      "Epoch: 0173 train_loss= 0.42038 time= 0.16006\n",
      "Epoch: 0174 train_loss= 0.42012 time= 0.17450\n",
      "Epoch: 0175 train_loss= 0.41986 time= 0.16747\n",
      "Epoch: 0176 train_loss= 0.41961 time= 0.17762\n",
      "Epoch: 0177 train_loss= 0.41937 time= 0.16488\n",
      "Epoch: 0178 train_loss= 0.41913 time= 0.16544\n",
      "Epoch: 0179 train_loss= 0.41890 time= 0.16017\n",
      "Epoch: 0180 train_loss= 0.41867 time= 0.17111\n",
      "Epoch: 0181 train_loss= 0.41845 time= 0.17553\n",
      "Epoch: 0182 train_loss= 0.41823 time= 0.16096\n",
      "Epoch: 0183 train_loss= 0.41802 time= 0.17371\n",
      "Epoch: 0184 train_loss= 0.41781 time= 0.18269\n",
      "Epoch: 0185 train_loss= 0.41760 time= 0.16692\n",
      "Epoch: 0186 train_loss= 0.41739 time= 0.16213\n",
      "Epoch: 0187 train_loss= 0.41719 time= 0.18032\n",
      "Epoch: 0188 train_loss= 0.41698 time= 0.17440\n",
      "Epoch: 0189 train_loss= 0.41678 time= 0.17292\n",
      "Epoch: 0190 train_loss= 0.41658 time= 0.16289\n",
      "Epoch: 0191 train_loss= 0.41638 time= 0.17010\n",
      "Epoch: 0192 train_loss= 0.41617 time= 0.15972\n",
      "Epoch: 0193 train_loss= 0.41598 time= 0.17468\n",
      "Epoch: 0194 train_loss= 0.41578 time= 0.16390\n",
      "Epoch: 0195 train_loss= 0.41559 time= 0.17977\n",
      "Epoch: 0196 train_loss= 0.41540 time= 0.17076\n",
      "Epoch: 0197 train_loss= 0.41522 time= 0.16743\n",
      "Epoch: 0198 train_loss= 0.41504 time= 0.16661\n",
      "Epoch: 0199 train_loss= 0.41487 time= 0.17850\n",
      "Epoch: 0200 train_loss= 0.41471 time= 0.17436\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73517 time= 0.29081\n",
      "Epoch: 0002 train_loss= 0.73452 time= 0.16155\n",
      "Epoch: 0003 train_loss= 0.73234 time= 0.16459\n",
      "Epoch: 0004 train_loss= 0.72758 time= 0.17508\n",
      "Epoch: 0005 train_loss= 0.71923 time= 0.15589\n",
      "Epoch: 0006 train_loss= 0.70663 time= 0.15634\n",
      "Epoch: 0007 train_loss= 0.69035 time= 0.16523\n",
      "Epoch: 0008 train_loss= 0.67332 time= 0.16435\n",
      "Epoch: 0009 train_loss= 0.66181 time= 0.16561\n",
      "Epoch: 0010 train_loss= 0.66193 time= 0.15659\n",
      "Epoch: 0011 train_loss= 0.66571 time= 0.15579\n",
      "Epoch: 0012 train_loss= 0.66174 time= 0.17081\n",
      "Epoch: 0013 train_loss= 0.65275 time= 0.17089\n",
      "Epoch: 0014 train_loss= 0.64435 time= 0.15974\n",
      "Epoch: 0015 train_loss= 0.63940 time= 0.16556\n",
      "Epoch: 0016 train_loss= 0.63760 time= 0.15719\n",
      "Epoch: 0017 train_loss= 0.63720 time= 0.17011\n",
      "Epoch: 0018 train_loss= 0.63660 time= 0.16610\n",
      "Epoch: 0019 train_loss= 0.63508 time= 0.17145\n",
      "Epoch: 0020 train_loss= 0.63259 time= 0.15609\n",
      "Epoch: 0021 train_loss= 0.62951 time= 0.17145\n",
      "Epoch: 0022 train_loss= 0.62640 time= 0.15834\n",
      "Epoch: 0023 train_loss= 0.62376 time= 0.16415\n",
      "Epoch: 0024 train_loss= 0.62181 time= 0.16512\n",
      "Epoch: 0025 train_loss= 0.62026 time= 0.16322\n",
      "Epoch: 0026 train_loss= 0.61851 time= 0.16221\n",
      "Epoch: 0027 train_loss= 0.61603 time= 0.17361\n",
      "Epoch: 0028 train_loss= 0.61277 time= 0.16096\n",
      "Epoch: 0029 train_loss= 0.60907 time= 0.16439\n",
      "Epoch: 0030 train_loss= 0.60536 time= 0.17028\n",
      "Epoch: 0031 train_loss= 0.60191 time= 0.16781\n",
      "Epoch: 0032 train_loss= 0.59864 time= 0.16980\n",
      "Epoch: 0033 train_loss= 0.59524 time= 0.17104\n",
      "Epoch: 0034 train_loss= 0.59143 time= 0.16839\n",
      "Epoch: 0035 train_loss= 0.58715 time= 0.16193\n",
      "Epoch: 0036 train_loss= 0.58255 time= 0.17143\n",
      "Epoch: 0037 train_loss= 0.57794 time= 0.15392\n",
      "Epoch: 0038 train_loss= 0.57352 time= 0.16335\n",
      "Epoch: 0039 train_loss= 0.56932 time= 0.15449\n",
      "Epoch: 0040 train_loss= 0.56519 time= 0.16750\n",
      "Epoch: 0041 train_loss= 0.56099 time= 0.16004\n",
      "Epoch: 0042 train_loss= 0.55662 time= 0.16442\n",
      "Epoch: 0043 train_loss= 0.55209 time= 0.16713\n",
      "Epoch: 0044 train_loss= 0.54748 time= 0.16864\n",
      "Epoch: 0045 train_loss= 0.54278 time= 0.16156\n",
      "Epoch: 0046 train_loss= 0.53805 time= 0.16655\n",
      "Epoch: 0047 train_loss= 0.53345 time= 0.18052\n",
      "Epoch: 0048 train_loss= 0.52917 time= 0.16767\n",
      "Epoch: 0049 train_loss= 0.52540 time= 0.17168\n",
      "Epoch: 0050 train_loss= 0.52220 time= 0.16739\n",
      "Epoch: 0051 train_loss= 0.51947 time= 0.17538\n",
      "Epoch: 0052 train_loss= 0.51696 time= 0.16736\n",
      "Epoch: 0053 train_loss= 0.51443 time= 0.17925\n",
      "Epoch: 0054 train_loss= 0.51179 time= 0.16883\n",
      "Epoch: 0055 train_loss= 0.50904 time= 0.16695\n",
      "Epoch: 0056 train_loss= 0.50622 time= 0.15725\n",
      "Epoch: 0057 train_loss= 0.50340 time= 0.16710\n",
      "Epoch: 0058 train_loss= 0.50066 time= 0.16858\n",
      "Epoch: 0059 train_loss= 0.49809 time= 0.17153\n",
      "Epoch: 0060 train_loss= 0.49577 time= 0.17529\n",
      "Epoch: 0061 train_loss= 0.49372 time= 0.17337\n",
      "Epoch: 0062 train_loss= 0.49191 time= 0.17054\n",
      "Epoch: 0063 train_loss= 0.49026 time= 0.16386\n",
      "Epoch: 0064 train_loss= 0.48863 time= 0.16818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0065 train_loss= 0.48696 time= 0.16181\n",
      "Epoch: 0066 train_loss= 0.48519 time= 0.17222\n",
      "Epoch: 0067 train_loss= 0.48333 time= 0.17326\n",
      "Epoch: 0068 train_loss= 0.48144 time= 0.16053\n",
      "Epoch: 0069 train_loss= 0.47955 time= 0.16900\n",
      "Epoch: 0070 train_loss= 0.47771 time= 0.15477\n",
      "Epoch: 0071 train_loss= 0.47590 time= 0.17280\n",
      "Epoch: 0072 train_loss= 0.47415 time= 0.17054\n",
      "Epoch: 0073 train_loss= 0.47248 time= 0.17894\n",
      "Epoch: 0074 train_loss= 0.47090 time= 0.16282\n",
      "Epoch: 0075 train_loss= 0.46941 time= 0.17022\n",
      "Epoch: 0076 train_loss= 0.46801 time= 0.17313\n",
      "Epoch: 0077 train_loss= 0.46666 time= 0.17159\n",
      "Epoch: 0078 train_loss= 0.46536 time= 0.16676\n",
      "Epoch: 0079 train_loss= 0.46409 time= 0.17444\n",
      "Epoch: 0080 train_loss= 0.46289 time= 0.17231\n",
      "Epoch: 0081 train_loss= 0.46174 time= 0.17514\n",
      "Epoch: 0082 train_loss= 0.46064 time= 0.16358\n",
      "Epoch: 0083 train_loss= 0.45959 time= 0.17174\n",
      "Epoch: 0084 train_loss= 0.45857 time= 0.16297\n",
      "Epoch: 0085 train_loss= 0.45759 time= 0.17626\n",
      "Epoch: 0086 train_loss= 0.45665 time= 0.17587\n",
      "Epoch: 0087 train_loss= 0.45577 time= 0.16014\n",
      "Epoch: 0088 train_loss= 0.45493 time= 0.17908\n",
      "Epoch: 0089 train_loss= 0.45411 time= 0.17501\n",
      "Epoch: 0090 train_loss= 0.45330 time= 0.18108\n",
      "Epoch: 0091 train_loss= 0.45249 time= 0.17415\n",
      "Epoch: 0092 train_loss= 0.45167 time= 0.18007\n",
      "Epoch: 0093 train_loss= 0.45085 time= 0.17737\n",
      "Epoch: 0094 train_loss= 0.45003 time= 0.16388\n",
      "Epoch: 0095 train_loss= 0.44922 time= 0.16885\n",
      "Epoch: 0096 train_loss= 0.44843 time= 0.18120\n",
      "Epoch: 0097 train_loss= 0.44766 time= 0.16707\n",
      "Epoch: 0098 train_loss= 0.44693 time= 0.16915\n",
      "Epoch: 0099 train_loss= 0.44623 time= 0.17639\n",
      "Epoch: 0100 train_loss= 0.44557 time= 0.17191\n",
      "Epoch: 0101 train_loss= 0.44494 time= 0.16788\n",
      "Epoch: 0102 train_loss= 0.44435 time= 0.16970\n",
      "Epoch: 0103 train_loss= 0.44380 time= 0.17546\n",
      "Epoch: 0104 train_loss= 0.44328 time= 0.16529\n",
      "Epoch: 0105 train_loss= 0.44278 time= 0.16473\n",
      "Epoch: 0106 train_loss= 0.44232 time= 0.17347\n",
      "Epoch: 0107 train_loss= 0.44188 time= 0.16501\n",
      "Epoch: 0108 train_loss= 0.44145 time= 0.16746\n",
      "Epoch: 0109 train_loss= 0.44104 time= 0.17042\n",
      "Epoch: 0110 train_loss= 0.44062 time= 0.16473\n",
      "Epoch: 0111 train_loss= 0.44020 time= 0.16591\n",
      "Epoch: 0112 train_loss= 0.43978 time= 0.16360\n",
      "Epoch: 0113 train_loss= 0.43934 time= 0.17583\n",
      "Epoch: 0114 train_loss= 0.43890 time= 0.16830\n",
      "Epoch: 0115 train_loss= 0.43845 time= 0.17302\n",
      "Epoch: 0116 train_loss= 0.43800 time= 0.16302\n",
      "Epoch: 0117 train_loss= 0.43755 time= 0.16107\n",
      "Epoch: 0118 train_loss= 0.43712 time= 0.17422\n",
      "Epoch: 0119 train_loss= 0.43669 time= 0.17553\n",
      "Epoch: 0120 train_loss= 0.43627 time= 0.16846\n",
      "Epoch: 0121 train_loss= 0.43586 time= 0.16092\n",
      "Epoch: 0122 train_loss= 0.43547 time= 0.17132\n",
      "Epoch: 0123 train_loss= 0.43509 time= 0.16057\n",
      "Epoch: 0124 train_loss= 0.43472 time= 0.16261\n",
      "Epoch: 0125 train_loss= 0.43437 time= 0.16720\n",
      "Epoch: 0126 train_loss= 0.43403 time= 0.17286\n",
      "Epoch: 0127 train_loss= 0.43369 time= 0.16580\n",
      "Epoch: 0128 train_loss= 0.43337 time= 0.16581\n",
      "Epoch: 0129 train_loss= 0.43306 time= 0.17839\n",
      "Epoch: 0130 train_loss= 0.43276 time= 0.16536\n",
      "Epoch: 0131 train_loss= 0.43246 time= 0.18306\n",
      "Epoch: 0132 train_loss= 0.43217 time= 0.16612\n",
      "Epoch: 0133 train_loss= 0.43188 time= 0.17503\n",
      "Epoch: 0134 train_loss= 0.43159 time= 0.16037\n",
      "Epoch: 0135 train_loss= 0.43131 time= 0.17964\n",
      "Epoch: 0136 train_loss= 0.43102 time= 0.17030\n",
      "Epoch: 0137 train_loss= 0.43073 time= 0.17758\n",
      "Epoch: 0138 train_loss= 0.43044 time= 0.15773\n",
      "Epoch: 0139 train_loss= 0.43015 time= 0.16273\n",
      "Epoch: 0140 train_loss= 0.42985 time= 0.16312\n",
      "Epoch: 0141 train_loss= 0.42954 time= 0.15131\n",
      "Epoch: 0142 train_loss= 0.42924 time= 0.17549\n",
      "Epoch: 0143 train_loss= 0.42892 time= 0.17036\n",
      "Epoch: 0144 train_loss= 0.42861 time= 0.17034\n",
      "Epoch: 0145 train_loss= 0.42829 time= 0.16773\n",
      "Epoch: 0146 train_loss= 0.42796 time= 0.16422\n",
      "Epoch: 0147 train_loss= 0.42763 time= 0.17845\n",
      "Epoch: 0148 train_loss= 0.42729 time= 0.16508\n",
      "Epoch: 0149 train_loss= 0.42695 time= 0.17817\n",
      "Epoch: 0150 train_loss= 0.42660 time= 0.16247\n",
      "Epoch: 0151 train_loss= 0.42624 time= 0.17271\n",
      "Epoch: 0152 train_loss= 0.42588 time= 0.17208\n",
      "Epoch: 0153 train_loss= 0.42551 time= 0.17250\n",
      "Epoch: 0154 train_loss= 0.42515 time= 0.17541\n",
      "Epoch: 0155 train_loss= 0.42478 time= 0.17354\n",
      "Epoch: 0156 train_loss= 0.42441 time= 0.17652\n",
      "Epoch: 0157 train_loss= 0.42404 time= 0.16284\n",
      "Epoch: 0158 train_loss= 0.42368 time= 0.17343\n",
      "Epoch: 0159 train_loss= 0.42333 time= 0.16395\n",
      "Epoch: 0160 train_loss= 0.42299 time= 0.17815\n",
      "Epoch: 0161 train_loss= 0.42266 time= 0.15552\n",
      "Epoch: 0162 train_loss= 0.42234 time= 0.16796\n",
      "Epoch: 0163 train_loss= 0.42204 time= 0.17043\n",
      "Epoch: 0164 train_loss= 0.42175 time= 0.16916\n",
      "Epoch: 0165 train_loss= 0.42148 time= 0.16548\n",
      "Epoch: 0166 train_loss= 0.42122 time= 0.16841\n",
      "Epoch: 0167 train_loss= 0.42097 time= 0.17453\n",
      "Epoch: 0168 train_loss= 0.42073 time= 0.17138\n",
      "Epoch: 0169 train_loss= 0.42049 time= 0.17572\n",
      "Epoch: 0170 train_loss= 0.42025 time= 0.16199\n",
      "Epoch: 0171 train_loss= 0.42002 time= 0.16617\n",
      "Epoch: 0172 train_loss= 0.41978 time= 0.16209\n",
      "Epoch: 0173 train_loss= 0.41954 time= 0.17879\n",
      "Epoch: 0174 train_loss= 0.41930 time= 0.16833\n",
      "Epoch: 0175 train_loss= 0.41906 time= 0.17035\n",
      "Epoch: 0176 train_loss= 0.41881 time= 0.16312\n",
      "Epoch: 0177 train_loss= 0.41857 time= 0.15929\n",
      "Epoch: 0178 train_loss= 0.41832 time= 0.16556\n",
      "Epoch: 0179 train_loss= 0.41807 time= 0.16455\n",
      "Epoch: 0180 train_loss= 0.41783 time= 0.16364\n",
      "Epoch: 0181 train_loss= 0.41758 time= 0.15688\n",
      "Epoch: 0182 train_loss= 0.41734 time= 0.17148\n",
      "Epoch: 0183 train_loss= 0.41709 time= 0.17532\n",
      "Epoch: 0184 train_loss= 0.41685 time= 0.16166\n",
      "Epoch: 0185 train_loss= 0.41661 time= 0.16926\n",
      "Epoch: 0186 train_loss= 0.41637 time= 0.16843\n",
      "Epoch: 0187 train_loss= 0.41613 time= 0.17943\n",
      "Epoch: 0188 train_loss= 0.41589 time= 0.16992\n",
      "Epoch: 0189 train_loss= 0.41566 time= 0.16482\n",
      "Epoch: 0190 train_loss= 0.41543 time= 0.17499\n",
      "Epoch: 0191 train_loss= 0.41520 time= 0.17054\n",
      "Epoch: 0192 train_loss= 0.41498 time= 0.15658\n",
      "Epoch: 0193 train_loss= 0.41476 time= 0.16721\n",
      "Epoch: 0194 train_loss= 0.41455 time= 0.17845\n",
      "Epoch: 0195 train_loss= 0.41435 time= 0.17018\n",
      "Epoch: 0196 train_loss= 0.41415 time= 0.16992\n",
      "Epoch: 0197 train_loss= 0.41396 time= 0.18128\n",
      "Epoch: 0198 train_loss= 0.41377 time= 0.17799\n",
      "Epoch: 0199 train_loss= 0.41359 time= 0.17337\n",
      "Epoch: 0200 train_loss= 0.41341 time= 0.16556\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73521 time= 0.28450\n",
      "Epoch: 0002 train_loss= 0.73483 time= 0.16385\n",
      "Epoch: 0003 train_loss= 0.73351 time= 0.16100\n",
      "Epoch: 0004 train_loss= 0.73050 time= 0.16430\n",
      "Epoch: 0005 train_loss= 0.72491 time= 0.16152\n",
      "Epoch: 0006 train_loss= 0.71590 time= 0.16973\n",
      "Epoch: 0007 train_loss= 0.70309 time= 0.16737\n",
      "Epoch: 0008 train_loss= 0.68731 time= 0.15224\n",
      "Epoch: 0009 train_loss= 0.67169 time= 0.16951\n",
      "Epoch: 0010 train_loss= 0.66227 time= 0.15373\n",
      "Epoch: 0011 train_loss= 0.66344 time= 0.17671\n",
      "Epoch: 0012 train_loss= 0.66583 time= 0.16085\n",
      "Epoch: 0013 train_loss= 0.66062 time= 0.15980\n",
      "Epoch: 0014 train_loss= 0.65115 time= 0.16206\n",
      "Epoch: 0015 train_loss= 0.64260 time= 0.17088\n",
      "Epoch: 0016 train_loss= 0.63743 time= 0.16921\n",
      "Epoch: 0017 train_loss= 0.63518 time= 0.15654\n",
      "Epoch: 0018 train_loss= 0.63408 time= 0.16811\n",
      "Epoch: 0019 train_loss= 0.63266 time= 0.14824\n",
      "Epoch: 0020 train_loss= 0.63023 time= 0.15476\n",
      "Epoch: 0021 train_loss= 0.62676 time= 0.16245\n",
      "Epoch: 0022 train_loss= 0.62262 time= 0.16656\n",
      "Epoch: 0023 train_loss= 0.61833 time= 0.16832\n",
      "Epoch: 0024 train_loss= 0.61433 time= 0.15932\n",
      "Epoch: 0025 train_loss= 0.61080 time= 0.17063\n",
      "Epoch: 0026 train_loss= 0.60750 time= 0.15485\n",
      "Epoch: 0027 train_loss= 0.60391 time= 0.16598\n",
      "Epoch: 0028 train_loss= 0.59959 time= 0.16112\n",
      "Epoch: 0029 train_loss= 0.59443 time= 0.15788\n",
      "Epoch: 0030 train_loss= 0.58871 time= 0.18360\n",
      "Epoch: 0031 train_loss= 0.58289 time= 0.16957\n",
      "Epoch: 0032 train_loss= 0.57726 time= 0.17083\n",
      "Epoch: 0033 train_loss= 0.57189 time= 0.16739\n",
      "Epoch: 0034 train_loss= 0.56662 time= 0.17354\n",
      "Epoch: 0035 train_loss= 0.56137 time= 0.16426\n",
      "Epoch: 0036 train_loss= 0.55620 time= 0.18136\n",
      "Epoch: 0037 train_loss= 0.55137 time= 0.17197\n",
      "Epoch: 0038 train_loss= 0.54721 time= 0.17758\n",
      "Epoch: 0039 train_loss= 0.54396 time= 0.17798\n",
      "Epoch: 0040 train_loss= 0.54172 time= 0.16861\n",
      "Epoch: 0041 train_loss= 0.54031 time= 0.17354\n",
      "Epoch: 0042 train_loss= 0.53943 time= 0.16415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0043 train_loss= 0.53864 time= 0.16788\n",
      "Epoch: 0044 train_loss= 0.53760 time= 0.15796\n",
      "Epoch: 0045 train_loss= 0.53609 time= 0.17712\n",
      "Epoch: 0046 train_loss= 0.53403 time= 0.17971\n",
      "Epoch: 0047 train_loss= 0.53147 time= 0.16653\n",
      "Epoch: 0048 train_loss= 0.52859 time= 0.16537\n",
      "Epoch: 0049 train_loss= 0.52564 time= 0.16251\n",
      "Epoch: 0050 train_loss= 0.52288 time= 0.18219\n",
      "Epoch: 0051 train_loss= 0.52049 time= 0.16427\n",
      "Epoch: 0052 train_loss= 0.51854 time= 0.16777\n",
      "Epoch: 0053 train_loss= 0.51694 time= 0.16235\n",
      "Epoch: 0054 train_loss= 0.51550 time= 0.16718\n",
      "Epoch: 0055 train_loss= 0.51402 time= 0.16354\n",
      "Epoch: 0056 train_loss= 0.51236 time= 0.16722\n",
      "Epoch: 0057 train_loss= 0.51046 time= 0.17653\n",
      "Epoch: 0058 train_loss= 0.50837 time= 0.17254\n",
      "Epoch: 0059 train_loss= 0.50613 time= 0.16688\n",
      "Epoch: 0060 train_loss= 0.50383 time= 0.16096\n",
      "Epoch: 0061 train_loss= 0.50153 time= 0.16484\n",
      "Epoch: 0062 train_loss= 0.49929 time= 0.17121\n",
      "Epoch: 0063 train_loss= 0.49717 time= 0.16556\n",
      "Epoch: 0064 train_loss= 0.49518 time= 0.17525\n",
      "Epoch: 0065 train_loss= 0.49334 time= 0.16513\n",
      "Epoch: 0066 train_loss= 0.49163 time= 0.16708\n",
      "Epoch: 0067 train_loss= 0.49004 time= 0.16010\n",
      "Epoch: 0068 train_loss= 0.48858 time= 0.15973\n",
      "Epoch: 0069 train_loss= 0.48725 time= 0.17122\n",
      "Epoch: 0070 train_loss= 0.48601 time= 0.16923\n",
      "Epoch: 0071 train_loss= 0.48482 time= 0.16952\n",
      "Epoch: 0072 train_loss= 0.48361 time= 0.15989\n",
      "Epoch: 0073 train_loss= 0.48233 time= 0.16878\n",
      "Epoch: 0074 train_loss= 0.48096 time= 0.17495\n",
      "Epoch: 0075 train_loss= 0.47950 time= 0.17245\n",
      "Epoch: 0076 train_loss= 0.47797 time= 0.17061\n",
      "Epoch: 0077 train_loss= 0.47640 time= 0.17054\n",
      "Epoch: 0078 train_loss= 0.47483 time= 0.17970\n",
      "Epoch: 0079 train_loss= 0.47330 time= 0.17579\n",
      "Epoch: 0080 train_loss= 0.47182 time= 0.16855\n",
      "Epoch: 0081 train_loss= 0.47041 time= 0.18162\n",
      "Epoch: 0082 train_loss= 0.46907 time= 0.16285\n",
      "Epoch: 0083 train_loss= 0.46782 time= 0.17108\n",
      "Epoch: 0084 train_loss= 0.46665 time= 0.16863\n",
      "Epoch: 0085 train_loss= 0.46556 time= 0.17075\n",
      "Epoch: 0086 train_loss= 0.46454 time= 0.17537\n",
      "Epoch: 0087 train_loss= 0.46358 time= 0.17129\n",
      "Epoch: 0088 train_loss= 0.46265 time= 0.17287\n",
      "Epoch: 0089 train_loss= 0.46173 time= 0.16498\n",
      "Epoch: 0090 train_loss= 0.46081 time= 0.15994\n",
      "Epoch: 0091 train_loss= 0.45992 time= 0.17493\n",
      "Epoch: 0092 train_loss= 0.45906 time= 0.16678\n",
      "Epoch: 0093 train_loss= 0.45822 time= 0.17852\n",
      "Epoch: 0094 train_loss= 0.45741 time= 0.18052\n",
      "Epoch: 0095 train_loss= 0.45664 time= 0.16790\n",
      "Epoch: 0096 train_loss= 0.45591 time= 0.16824\n",
      "Epoch: 0097 train_loss= 0.45523 time= 0.16359\n",
      "Epoch: 0098 train_loss= 0.45457 time= 0.17743\n",
      "Epoch: 0099 train_loss= 0.45393 time= 0.17128\n",
      "Epoch: 0100 train_loss= 0.45329 time= 0.16619\n",
      "Epoch: 0101 train_loss= 0.45264 time= 0.16582\n",
      "Epoch: 0102 train_loss= 0.45197 time= 0.15581\n",
      "Epoch: 0103 train_loss= 0.45129 time= 0.16150\n",
      "Epoch: 0104 train_loss= 0.45059 time= 0.16611\n",
      "Epoch: 0105 train_loss= 0.44988 time= 0.16660\n",
      "Epoch: 0106 train_loss= 0.44918 time= 0.16299\n",
      "Epoch: 0107 train_loss= 0.44848 time= 0.17071\n",
      "Epoch: 0108 train_loss= 0.44778 time= 0.17339\n",
      "Epoch: 0109 train_loss= 0.44709 time= 0.16475\n",
      "Epoch: 0110 train_loss= 0.44641 time= 0.17847\n",
      "Epoch: 0111 train_loss= 0.44573 time= 0.16473\n",
      "Epoch: 0112 train_loss= 0.44506 time= 0.17674\n",
      "Epoch: 0113 train_loss= 0.44439 time= 0.17764\n",
      "Epoch: 0114 train_loss= 0.44372 time= 0.16513\n",
      "Epoch: 0115 train_loss= 0.44306 time= 0.16370\n",
      "Epoch: 0116 train_loss= 0.44241 time= 0.17570\n",
      "Epoch: 0117 train_loss= 0.44179 time= 0.17852\n",
      "Epoch: 0118 train_loss= 0.44118 time= 0.17353\n",
      "Epoch: 0119 train_loss= 0.44061 time= 0.16993\n",
      "Epoch: 0120 train_loss= 0.44007 time= 0.16810\n",
      "Epoch: 0121 train_loss= 0.43957 time= 0.17018\n",
      "Epoch: 0122 train_loss= 0.43911 time= 0.18521\n",
      "Epoch: 0123 train_loss= 0.43867 time= 0.17432\n",
      "Epoch: 0124 train_loss= 0.43825 time= 0.18051\n",
      "Epoch: 0125 train_loss= 0.43784 time= 0.15593\n",
      "Epoch: 0126 train_loss= 0.43743 time= 0.16288\n",
      "Epoch: 0127 train_loss= 0.43701 time= 0.16372\n",
      "Epoch: 0128 train_loss= 0.43657 time= 0.15976\n",
      "Epoch: 0129 train_loss= 0.43611 time= 0.17075\n",
      "Epoch: 0130 train_loss= 0.43564 time= 0.15916\n",
      "Epoch: 0131 train_loss= 0.43516 time= 0.18055\n",
      "Epoch: 0132 train_loss= 0.43467 time= 0.16828\n",
      "Epoch: 0133 train_loss= 0.43417 time= 0.16803\n",
      "Epoch: 0134 train_loss= 0.43368 time= 0.17002\n",
      "Epoch: 0135 train_loss= 0.43320 time= 0.16015\n",
      "Epoch: 0136 train_loss= 0.43272 time= 0.18148\n",
      "Epoch: 0137 train_loss= 0.43226 time= 0.15828\n",
      "Epoch: 0138 train_loss= 0.43181 time= 0.16661\n",
      "Epoch: 0139 train_loss= 0.43137 time= 0.15872\n",
      "Epoch: 0140 train_loss= 0.43093 time= 0.16157\n",
      "Epoch: 0141 train_loss= 0.43051 time= 0.16954\n",
      "Epoch: 0142 train_loss= 0.43009 time= 0.16513\n",
      "Epoch: 0143 train_loss= 0.42968 time= 0.17545\n",
      "Epoch: 0144 train_loss= 0.42928 time= 0.16415\n",
      "Epoch: 0145 train_loss= 0.42888 time= 0.16005\n",
      "Epoch: 0146 train_loss= 0.42849 time= 0.16963\n",
      "Epoch: 0147 train_loss= 0.42811 time= 0.15694\n",
      "Epoch: 0148 train_loss= 0.42774 time= 0.18243\n",
      "Epoch: 0149 train_loss= 0.42738 time= 0.16615\n",
      "Epoch: 0150 train_loss= 0.42702 time= 0.17282\n",
      "Epoch: 0151 train_loss= 0.42668 time= 0.17097\n",
      "Epoch: 0152 train_loss= 0.42635 time= 0.16938\n",
      "Epoch: 0153 train_loss= 0.42603 time= 0.17553\n",
      "Epoch: 0154 train_loss= 0.42571 time= 0.16844\n",
      "Epoch: 0155 train_loss= 0.42539 time= 0.17249\n",
      "Epoch: 0156 train_loss= 0.42507 time= 0.17006\n",
      "Epoch: 0157 train_loss= 0.42476 time= 0.17300\n",
      "Epoch: 0158 train_loss= 0.42444 time= 0.17042\n",
      "Epoch: 0159 train_loss= 0.42412 time= 0.15504\n",
      "Epoch: 0160 train_loss= 0.42380 time= 0.17288\n",
      "Epoch: 0161 train_loss= 0.42348 time= 0.17121\n",
      "Epoch: 0162 train_loss= 0.42316 time= 0.16170\n",
      "Epoch: 0163 train_loss= 0.42284 time= 0.16256\n",
      "Epoch: 0164 train_loss= 0.42253 time= 0.16748\n",
      "Epoch: 0165 train_loss= 0.42222 time= 0.18221\n",
      "Epoch: 0166 train_loss= 0.42191 time= 0.17287\n",
      "Epoch: 0167 train_loss= 0.42162 time= 0.17018\n",
      "Epoch: 0168 train_loss= 0.42133 time= 0.15703\n",
      "Epoch: 0169 train_loss= 0.42106 time= 0.15846\n",
      "Epoch: 0170 train_loss= 0.42080 time= 0.17286\n",
      "Epoch: 0171 train_loss= 0.42054 time= 0.16981\n",
      "Epoch: 0172 train_loss= 0.42030 time= 0.17855\n",
      "Epoch: 0173 train_loss= 0.42006 time= 0.16499\n",
      "Epoch: 0174 train_loss= 0.41984 time= 0.16418\n",
      "Epoch: 0175 train_loss= 0.41961 time= 0.17144\n",
      "Epoch: 0176 train_loss= 0.41939 time= 0.17305\n",
      "Epoch: 0177 train_loss= 0.41916 time= 0.16739\n",
      "Epoch: 0178 train_loss= 0.41893 time= 0.16340\n",
      "Epoch: 0179 train_loss= 0.41870 time= 0.17175\n",
      "Epoch: 0180 train_loss= 0.41846 time= 0.17059\n",
      "Epoch: 0181 train_loss= 0.41821 time= 0.16465\n",
      "Epoch: 0182 train_loss= 0.41796 time= 0.16560\n",
      "Epoch: 0183 train_loss= 0.41770 time= 0.16813\n",
      "Epoch: 0184 train_loss= 0.41743 time= 0.17419\n",
      "Epoch: 0185 train_loss= 0.41716 time= 0.15483\n",
      "Epoch: 0186 train_loss= 0.41689 time= 0.16873\n",
      "Epoch: 0187 train_loss= 0.41661 time= 0.15681\n",
      "Epoch: 0188 train_loss= 0.41633 time= 0.16078\n",
      "Epoch: 0189 train_loss= 0.41605 time= 0.17154\n",
      "Epoch: 0190 train_loss= 0.41576 time= 0.15946\n",
      "Epoch: 0191 train_loss= 0.41548 time= 0.16416\n",
      "Epoch: 0192 train_loss= 0.41519 time= 0.17234\n",
      "Epoch: 0193 train_loss= 0.41491 time= 0.17028\n",
      "Epoch: 0194 train_loss= 0.41463 time= 0.16712\n",
      "Epoch: 0195 train_loss= 0.41435 time= 0.16764\n",
      "Epoch: 0196 train_loss= 0.41407 time= 0.17460\n",
      "Epoch: 0197 train_loss= 0.41380 time= 0.16443\n",
      "Epoch: 0198 train_loss= 0.41353 time= 0.15825\n",
      "Epoch: 0199 train_loss= 0.41327 time= 0.16755\n",
      "Epoch: 0200 train_loss= 0.41301 time= 0.16665\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73517 time= 0.32027\n",
      "Epoch: 0002 train_loss= 0.73449 time= 0.18293\n",
      "Epoch: 0003 train_loss= 0.73222 time= 0.20138\n",
      "Epoch: 0004 train_loss= 0.72733 time= 0.23396\n",
      "Epoch: 0005 train_loss= 0.71888 time= 0.24266\n",
      "Epoch: 0006 train_loss= 0.70638 time= 0.25515\n",
      "Epoch: 0007 train_loss= 0.69049 time= 0.21625\n",
      "Epoch: 0008 train_loss= 0.67413 time= 0.17672\n",
      "Epoch: 0009 train_loss= 0.66319 time= 0.17018\n",
      "Epoch: 0010 train_loss= 0.66287 time= 0.18503\n",
      "Epoch: 0011 train_loss= 0.66542 time= 0.21313\n",
      "Epoch: 0012 train_loss= 0.66094 time= 0.25528\n",
      "Epoch: 0013 train_loss= 0.65189 time= 0.26907\n",
      "Epoch: 0014 train_loss= 0.64332 time= 0.25887\n",
      "Epoch: 0015 train_loss= 0.63782 time= 0.23567\n",
      "Epoch: 0016 train_loss= 0.63521 time= 0.17536\n",
      "Epoch: 0017 train_loss= 0.63394 time= 0.17787\n",
      "Epoch: 0018 train_loss= 0.63255 time= 0.17753\n",
      "Epoch: 0019 train_loss= 0.63033 time= 0.16795\n",
      "Epoch: 0020 train_loss= 0.62721 time= 0.18052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0021 train_loss= 0.62346 time= 0.16505\n",
      "Epoch: 0022 train_loss= 0.61952 time= 0.16982\n",
      "Epoch: 0023 train_loss= 0.61580 time= 0.16580\n",
      "Epoch: 0024 train_loss= 0.61243 time= 0.17369\n",
      "Epoch: 0025 train_loss= 0.60926 time= 0.18017\n",
      "Epoch: 0026 train_loss= 0.60587 time= 0.16620\n",
      "Epoch: 0027 train_loss= 0.60187 time= 0.17399\n",
      "Epoch: 0028 train_loss= 0.59706 time= 0.17483\n",
      "Epoch: 0029 train_loss= 0.59158 time= 0.17005\n",
      "Epoch: 0030 train_loss= 0.58580 time= 0.17847\n",
      "Epoch: 0031 train_loss= 0.58004 time= 0.17176\n",
      "Epoch: 0032 train_loss= 0.57453 time= 0.17623\n",
      "Epoch: 0033 train_loss= 0.56933 time= 0.17199\n",
      "Epoch: 0034 train_loss= 0.56448 time= 0.16650\n",
      "Epoch: 0035 train_loss= 0.56002 time= 0.16637\n",
      "Epoch: 0036 train_loss= 0.55610 time= 0.16967\n",
      "Epoch: 0037 train_loss= 0.55286 time= 0.16282\n",
      "Epoch: 0038 train_loss= 0.55037 time= 0.16846\n",
      "Epoch: 0039 train_loss= 0.54851 time= 0.16672\n",
      "Epoch: 0040 train_loss= 0.54704 time= 0.16616\n",
      "Epoch: 0041 train_loss= 0.54568 time= 0.16038\n",
      "Epoch: 0042 train_loss= 0.54423 time= 0.16163\n",
      "Epoch: 0043 train_loss= 0.54259 time= 0.15976\n",
      "Epoch: 0044 train_loss= 0.54073 time= 0.15844\n",
      "Epoch: 0045 train_loss= 0.53864 time= 0.16538\n",
      "Epoch: 0046 train_loss= 0.53635 time= 0.16419\n",
      "Epoch: 0047 train_loss= 0.53394 time= 0.16851\n",
      "Epoch: 0048 train_loss= 0.53151 time= 0.15819\n",
      "Epoch: 0049 train_loss= 0.52919 time= 0.15714\n",
      "Epoch: 0050 train_loss= 0.52708 time= 0.17041\n",
      "Epoch: 0051 train_loss= 0.52521 time= 0.16256\n",
      "Epoch: 0052 train_loss= 0.52352 time= 0.16285\n",
      "Epoch: 0053 train_loss= 0.52190 time= 0.16603\n",
      "Epoch: 0054 train_loss= 0.52026 time= 0.16282\n",
      "Epoch: 0055 train_loss= 0.51854 time= 0.17593\n",
      "Epoch: 0056 train_loss= 0.51670 time= 0.16255\n",
      "Epoch: 0057 train_loss= 0.51477 time= 0.16060\n",
      "Epoch: 0058 train_loss= 0.51274 time= 0.16077\n",
      "Epoch: 0059 train_loss= 0.51062 time= 0.16099\n",
      "Epoch: 0060 train_loss= 0.50839 time= 0.17120\n",
      "Epoch: 0061 train_loss= 0.50605 time= 0.16670\n",
      "Epoch: 0062 train_loss= 0.50360 time= 0.17253\n",
      "Epoch: 0063 train_loss= 0.50104 time= 0.16150\n",
      "Epoch: 0064 train_loss= 0.49843 time= 0.16241\n",
      "Epoch: 0065 train_loss= 0.49582 time= 0.16456\n",
      "Epoch: 0066 train_loss= 0.49330 time= 0.16551\n",
      "Epoch: 0067 train_loss= 0.49093 time= 0.17154\n",
      "Epoch: 0068 train_loss= 0.48881 time= 0.16913\n",
      "Epoch: 0069 train_loss= 0.48697 time= 0.17163\n",
      "Epoch: 0070 train_loss= 0.48539 time= 0.15139\n",
      "Epoch: 0071 train_loss= 0.48400 time= 0.16596\n",
      "Epoch: 0072 train_loss= 0.48265 time= 0.16325\n",
      "Epoch: 0073 train_loss= 0.48121 time= 0.16312\n",
      "Epoch: 0074 train_loss= 0.47959 time= 0.16901\n",
      "Epoch: 0075 train_loss= 0.47777 time= 0.15933\n",
      "Epoch: 0076 train_loss= 0.47582 time= 0.16716\n",
      "Epoch: 0077 train_loss= 0.47383 time= 0.16934\n",
      "Epoch: 0078 train_loss= 0.47193 time= 0.15784\n",
      "Epoch: 0079 train_loss= 0.47022 time= 0.16948\n",
      "Epoch: 0080 train_loss= 0.46876 time= 0.16404\n",
      "Epoch: 0081 train_loss= 0.46754 time= 0.16618\n",
      "Epoch: 0082 train_loss= 0.46650 time= 0.16397\n",
      "Epoch: 0083 train_loss= 0.46553 time= 0.16054\n",
      "Epoch: 0084 train_loss= 0.46455 time= 0.16262\n",
      "Epoch: 0085 train_loss= 0.46352 time= 0.16538\n",
      "Epoch: 0086 train_loss= 0.46242 time= 0.16855\n",
      "Epoch: 0087 train_loss= 0.46126 time= 0.16579\n",
      "Epoch: 0088 train_loss= 0.46007 time= 0.16234\n",
      "Epoch: 0089 train_loss= 0.45889 time= 0.17254\n",
      "Epoch: 0090 train_loss= 0.45777 time= 0.16738\n",
      "Epoch: 0091 train_loss= 0.45676 time= 0.15498\n",
      "Epoch: 0092 train_loss= 0.45590 time= 0.17905\n",
      "Epoch: 0093 train_loss= 0.45519 time= 0.16084\n",
      "Epoch: 0094 train_loss= 0.45460 time= 0.17027\n",
      "Epoch: 0095 train_loss= 0.45409 time= 0.16181\n",
      "Epoch: 0096 train_loss= 0.45364 time= 0.17931\n",
      "Epoch: 0097 train_loss= 0.45321 time= 0.17260\n",
      "Epoch: 0098 train_loss= 0.45275 time= 0.15871\n",
      "Epoch: 0099 train_loss= 0.45226 time= 0.16646\n",
      "Epoch: 0100 train_loss= 0.45170 time= 0.16693\n",
      "Epoch: 0101 train_loss= 0.45111 time= 0.17401\n",
      "Epoch: 0102 train_loss= 0.45048 time= 0.15619\n",
      "Epoch: 0103 train_loss= 0.44985 time= 0.17300\n",
      "Epoch: 0104 train_loss= 0.44922 time= 0.18037\n",
      "Epoch: 0105 train_loss= 0.44860 time= 0.16132\n",
      "Epoch: 0106 train_loss= 0.44800 time= 0.17850\n",
      "Epoch: 0107 train_loss= 0.44742 time= 0.16346\n",
      "Epoch: 0108 train_loss= 0.44686 time= 0.16511\n",
      "Epoch: 0109 train_loss= 0.44632 time= 0.16834\n",
      "Epoch: 0110 train_loss= 0.44578 time= 0.16704\n",
      "Epoch: 0111 train_loss= 0.44525 time= 0.17587\n",
      "Epoch: 0112 train_loss= 0.44471 time= 0.17566\n",
      "Epoch: 0113 train_loss= 0.44418 time= 0.16062\n",
      "Epoch: 0114 train_loss= 0.44364 time= 0.18394\n",
      "Epoch: 0115 train_loss= 0.44310 time= 0.16773\n",
      "Epoch: 0116 train_loss= 0.44256 time= 0.16506\n",
      "Epoch: 0117 train_loss= 0.44202 time= 0.17032\n",
      "Epoch: 0118 train_loss= 0.44149 time= 0.15313\n",
      "Epoch: 0119 train_loss= 0.44098 time= 0.16827\n",
      "Epoch: 0120 train_loss= 0.44048 time= 0.16256\n",
      "Epoch: 0121 train_loss= 0.44000 time= 0.16057\n",
      "Epoch: 0122 train_loss= 0.43953 time= 0.17366\n",
      "Epoch: 0123 train_loss= 0.43908 time= 0.17040\n",
      "Epoch: 0124 train_loss= 0.43864 time= 0.16868\n",
      "Epoch: 0125 train_loss= 0.43820 time= 0.16525\n",
      "Epoch: 0126 train_loss= 0.43776 time= 0.16824\n",
      "Epoch: 0127 train_loss= 0.43732 time= 0.17659\n",
      "Epoch: 0128 train_loss= 0.43686 time= 0.17537\n",
      "Epoch: 0129 train_loss= 0.43640 time= 0.18163\n",
      "Epoch: 0130 train_loss= 0.43592 time= 0.15950\n",
      "Epoch: 0131 train_loss= 0.43545 time= 0.16414\n",
      "Epoch: 0132 train_loss= 0.43498 time= 0.16737\n",
      "Epoch: 0133 train_loss= 0.43452 time= 0.17190\n",
      "Epoch: 0134 train_loss= 0.43407 time= 0.16630\n",
      "Epoch: 0135 train_loss= 0.43365 time= 0.17006\n",
      "Epoch: 0136 train_loss= 0.43325 time= 0.17603\n",
      "Epoch: 0137 train_loss= 0.43287 time= 0.15852\n",
      "Epoch: 0138 train_loss= 0.43252 time= 0.16726\n",
      "Epoch: 0139 train_loss= 0.43219 time= 0.17662\n",
      "Epoch: 0140 train_loss= 0.43188 time= 0.17478\n",
      "Epoch: 0141 train_loss= 0.43159 time= 0.17017\n",
      "Epoch: 0142 train_loss= 0.43129 time= 0.16314\n",
      "Epoch: 0143 train_loss= 0.43100 time= 0.16556\n",
      "Epoch: 0144 train_loss= 0.43070 time= 0.18052\n",
      "Epoch: 0145 train_loss= 0.43040 time= 0.17225\n",
      "Epoch: 0146 train_loss= 0.43008 time= 0.17343\n",
      "Epoch: 0147 train_loss= 0.42974 time= 0.16262\n",
      "Epoch: 0148 train_loss= 0.42940 time= 0.17376\n",
      "Epoch: 0149 train_loss= 0.42906 time= 0.17333\n",
      "Epoch: 0150 train_loss= 0.42871 time= 0.17651\n",
      "Epoch: 0151 train_loss= 0.42837 time= 0.17827\n",
      "Epoch: 0152 train_loss= 0.42803 time= 0.15817\n",
      "Epoch: 0153 train_loss= 0.42769 time= 0.16299\n",
      "Epoch: 0154 train_loss= 0.42736 time= 0.16714\n",
      "Epoch: 0155 train_loss= 0.42704 time= 0.17059\n",
      "Epoch: 0156 train_loss= 0.42672 time= 0.17296\n",
      "Epoch: 0157 train_loss= 0.42640 time= 0.16157\n",
      "Epoch: 0158 train_loss= 0.42608 time= 0.16399\n",
      "Epoch: 0159 train_loss= 0.42577 time= 0.16419\n",
      "Epoch: 0160 train_loss= 0.42546 time= 0.15780\n",
      "Epoch: 0161 train_loss= 0.42515 time= 0.17727\n",
      "Epoch: 0162 train_loss= 0.42484 time= 0.17115\n",
      "Epoch: 0163 train_loss= 0.42454 time= 0.15995\n",
      "Epoch: 0164 train_loss= 0.42424 time= 0.17296\n",
      "Epoch: 0165 train_loss= 0.42394 time= 0.16088\n",
      "Epoch: 0166 train_loss= 0.42364 time= 0.16622\n",
      "Epoch: 0167 train_loss= 0.42334 time= 0.16809\n",
      "Epoch: 0168 train_loss= 0.42306 time= 0.16436\n",
      "Epoch: 0169 train_loss= 0.42277 time= 0.17028\n",
      "Epoch: 0170 train_loss= 0.42250 time= 0.16453\n",
      "Epoch: 0171 train_loss= 0.42224 time= 0.16335\n",
      "Epoch: 0172 train_loss= 0.42198 time= 0.16553\n",
      "Epoch: 0173 train_loss= 0.42174 time= 0.16725\n",
      "Epoch: 0174 train_loss= 0.42151 time= 0.16820\n",
      "Epoch: 0175 train_loss= 0.42129 time= 0.16607\n",
      "Epoch: 0176 train_loss= 0.42107 time= 0.16948\n",
      "Epoch: 0177 train_loss= 0.42086 time= 0.16054\n",
      "Epoch: 0178 train_loss= 0.42066 time= 0.15871\n",
      "Epoch: 0179 train_loss= 0.42045 time= 0.16761\n",
      "Epoch: 0180 train_loss= 0.42025 time= 0.17055\n",
      "Epoch: 0181 train_loss= 0.42004 time= 0.16655\n",
      "Epoch: 0182 train_loss= 0.41984 time= 0.16656\n",
      "Epoch: 0183 train_loss= 0.41963 time= 0.17152\n",
      "Epoch: 0184 train_loss= 0.41942 time= 0.16119\n",
      "Epoch: 0185 train_loss= 0.41920 time= 0.16181\n",
      "Epoch: 0186 train_loss= 0.41899 time= 0.17070\n",
      "Epoch: 0187 train_loss= 0.41878 time= 0.16385\n",
      "Epoch: 0188 train_loss= 0.41856 time= 0.17186\n",
      "Epoch: 0189 train_loss= 0.41835 time= 0.16629\n",
      "Epoch: 0190 train_loss= 0.41815 time= 0.16537\n",
      "Epoch: 0191 train_loss= 0.41794 time= 0.16942\n",
      "Epoch: 0192 train_loss= 0.41774 time= 0.16955\n",
      "Epoch: 0193 train_loss= 0.41754 time= 0.17553\n",
      "Epoch: 0194 train_loss= 0.41735 time= 0.15876\n",
      "Epoch: 0195 train_loss= 0.41717 time= 0.15758\n",
      "Epoch: 0196 train_loss= 0.41699 time= 0.16123\n",
      "Epoch: 0197 train_loss= 0.41682 time= 0.16246\n",
      "Epoch: 0198 train_loss= 0.41665 time= 0.16730\n",
      "Epoch: 0199 train_loss= 0.41648 time= 0.16774\n",
      "Epoch: 0200 train_loss= 0.41632 time= 0.15456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73518 time= 0.30293\n",
      "Epoch: 0002 train_loss= 0.73465 time= 0.15780\n",
      "Epoch: 0003 train_loss= 0.73279 time= 0.16354\n",
      "Epoch: 0004 train_loss= 0.72860 time= 0.16491\n",
      "Epoch: 0005 train_loss= 0.72103 time= 0.15679\n",
      "Epoch: 0006 train_loss= 0.70931 time= 0.16310\n",
      "Epoch: 0007 train_loss= 0.69363 time= 0.17003\n",
      "Epoch: 0008 train_loss= 0.67632 time= 0.16229\n",
      "Epoch: 0009 train_loss= 0.66305 time= 0.16867\n",
      "Epoch: 0010 train_loss= 0.66084 time= 0.16125\n",
      "Epoch: 0011 train_loss= 0.66532 time= 0.16606\n",
      "Epoch: 0012 train_loss= 0.66288 time= 0.15659\n",
      "Epoch: 0013 train_loss= 0.65405 time= 0.16157\n",
      "Epoch: 0014 train_loss= 0.64480 time= 0.15857\n",
      "Epoch: 0015 train_loss= 0.63877 time= 0.14960\n",
      "Epoch: 0016 train_loss= 0.63620 time= 0.16134\n",
      "Epoch: 0017 train_loss= 0.63534 time= 0.16637\n",
      "Epoch: 0018 train_loss= 0.63441 time= 0.16375\n",
      "Epoch: 0019 train_loss= 0.63249 time= 0.16865\n",
      "Epoch: 0020 train_loss= 0.62945 time= 0.16473\n",
      "Epoch: 0021 train_loss= 0.62564 time= 0.16767\n",
      "Epoch: 0022 train_loss= 0.62161 time= 0.16610\n",
      "Epoch: 0023 train_loss= 0.61793 time= 0.16265\n",
      "Epoch: 0024 train_loss= 0.61487 time= 0.16044\n",
      "Epoch: 0025 train_loss= 0.61226 time= 0.17768\n",
      "Epoch: 0026 train_loss= 0.60954 time= 0.16767\n",
      "Epoch: 0027 train_loss= 0.60614 time= 0.16691\n",
      "Epoch: 0028 train_loss= 0.60191 time= 0.17228\n",
      "Epoch: 0029 train_loss= 0.59712 time= 0.16872\n",
      "Epoch: 0030 train_loss= 0.59231 time= 0.15855\n",
      "Epoch: 0031 train_loss= 0.58789 time= 0.17967\n",
      "Epoch: 0032 train_loss= 0.58398 time= 0.17052\n",
      "Epoch: 0033 train_loss= 0.58047 time= 0.16413\n",
      "Epoch: 0034 train_loss= 0.57711 time= 0.16454\n",
      "Epoch: 0035 train_loss= 0.57380 time= 0.16157\n",
      "Epoch: 0036 train_loss= 0.57063 time= 0.16659\n",
      "Epoch: 0037 train_loss= 0.56775 time= 0.17418\n",
      "Epoch: 0038 train_loss= 0.56521 time= 0.15783\n",
      "Epoch: 0039 train_loss= 0.56291 time= 0.17026\n",
      "Epoch: 0040 train_loss= 0.56055 time= 0.17064\n",
      "Epoch: 0041 train_loss= 0.55782 time= 0.17224\n",
      "Epoch: 0042 train_loss= 0.55457 time= 0.17768\n",
      "Epoch: 0043 train_loss= 0.55089 time= 0.16341\n",
      "Epoch: 0044 train_loss= 0.54703 time= 0.16662\n",
      "Epoch: 0045 train_loss= 0.54321 time= 0.15958\n",
      "Epoch: 0046 train_loss= 0.53950 time= 0.16316\n",
      "Epoch: 0047 train_loss= 0.53588 time= 0.16754\n",
      "Epoch: 0048 train_loss= 0.53231 time= 0.15957\n",
      "Epoch: 0049 train_loss= 0.52881 time= 0.17961\n",
      "Epoch: 0050 train_loss= 0.52545 time= 0.16556\n",
      "Epoch: 0051 train_loss= 0.52227 time= 0.16780\n",
      "Epoch: 0052 train_loss= 0.51925 time= 0.16383\n",
      "Epoch: 0053 train_loss= 0.51633 time= 0.16804\n",
      "Epoch: 0054 train_loss= 0.51343 time= 0.17410\n",
      "Epoch: 0055 train_loss= 0.51055 time= 0.15577\n",
      "Epoch: 0056 train_loss= 0.50774 time= 0.16806\n",
      "Epoch: 0057 train_loss= 0.50508 time= 0.16467\n",
      "Epoch: 0058 train_loss= 0.50265 time= 0.16344\n",
      "Epoch: 0059 train_loss= 0.50049 time= 0.17455\n",
      "Epoch: 0060 train_loss= 0.49859 time= 0.17290\n",
      "Epoch: 0061 train_loss= 0.49692 time= 0.16072\n",
      "Epoch: 0062 train_loss= 0.49542 time= 0.17081\n",
      "Epoch: 0063 train_loss= 0.49401 time= 0.15651\n",
      "Epoch: 0064 train_loss= 0.49259 time= 0.16152\n",
      "Epoch: 0065 train_loss= 0.49107 time= 0.15723\n",
      "Epoch: 0066 train_loss= 0.48938 time= 0.15968\n",
      "Epoch: 0067 train_loss= 0.48749 time= 0.16128\n",
      "Epoch: 0068 train_loss= 0.48540 time= 0.15651\n",
      "Epoch: 0069 train_loss= 0.48321 time= 0.16946\n",
      "Epoch: 0070 train_loss= 0.48099 time= 0.16205\n",
      "Epoch: 0071 train_loss= 0.47886 time= 0.17533\n",
      "Epoch: 0072 train_loss= 0.47686 time= 0.17782\n",
      "Epoch: 0073 train_loss= 0.47504 time= 0.17921\n",
      "Epoch: 0074 train_loss= 0.47339 time= 0.18542\n",
      "Epoch: 0075 train_loss= 0.47193 time= 0.15998\n",
      "Epoch: 0076 train_loss= 0.47062 time= 0.16314\n",
      "Epoch: 0077 train_loss= 0.46946 time= 0.16326\n",
      "Epoch: 0078 train_loss= 0.46843 time= 0.15837\n",
      "Epoch: 0079 train_loss= 0.46749 time= 0.18231\n",
      "Epoch: 0080 train_loss= 0.46660 time= 0.15775\n",
      "Epoch: 0081 train_loss= 0.46574 time= 0.16425\n",
      "Epoch: 0082 train_loss= 0.46487 time= 0.17846\n",
      "Epoch: 0083 train_loss= 0.46396 time= 0.17636\n",
      "Epoch: 0084 train_loss= 0.46302 time= 0.18491\n",
      "Epoch: 0085 train_loss= 0.46204 time= 0.17354\n",
      "Epoch: 0086 train_loss= 0.46103 time= 0.17340\n",
      "Epoch: 0087 train_loss= 0.46000 time= 0.17339\n",
      "Epoch: 0088 train_loss= 0.45897 time= 0.16546\n",
      "Epoch: 0089 train_loss= 0.45798 time= 0.17027\n",
      "Epoch: 0090 train_loss= 0.45703 time= 0.16278\n",
      "Epoch: 0091 train_loss= 0.45613 time= 0.15926\n",
      "Epoch: 0092 train_loss= 0.45525 time= 0.17181\n",
      "Epoch: 0093 train_loss= 0.45440 time= 0.17427\n",
      "Epoch: 0094 train_loss= 0.45357 time= 0.16456\n",
      "Epoch: 0095 train_loss= 0.45275 time= 0.16057\n",
      "Epoch: 0096 train_loss= 0.45194 time= 0.17304\n",
      "Epoch: 0097 train_loss= 0.45114 time= 0.16699\n",
      "Epoch: 0098 train_loss= 0.45035 time= 0.15514\n",
      "Epoch: 0099 train_loss= 0.44958 time= 0.18008\n",
      "Epoch: 0100 train_loss= 0.44883 time= 0.16463\n",
      "Epoch: 0101 train_loss= 0.44812 time= 0.17748\n",
      "Epoch: 0102 train_loss= 0.44746 time= 0.17605\n",
      "Epoch: 0103 train_loss= 0.44683 time= 0.16683\n",
      "Epoch: 0104 train_loss= 0.44626 time= 0.16183\n",
      "Epoch: 0105 train_loss= 0.44574 time= 0.16492\n",
      "Epoch: 0106 train_loss= 0.44526 time= 0.17609\n",
      "Epoch: 0107 train_loss= 0.44481 time= 0.17139\n",
      "Epoch: 0108 train_loss= 0.44438 time= 0.16835\n",
      "Epoch: 0109 train_loss= 0.44396 time= 0.16097\n",
      "Epoch: 0110 train_loss= 0.44353 time= 0.16175\n",
      "Epoch: 0111 train_loss= 0.44309 time= 0.17964\n",
      "Epoch: 0112 train_loss= 0.44263 time= 0.17289\n",
      "Epoch: 0113 train_loss= 0.44215 time= 0.16847\n",
      "Epoch: 0114 train_loss= 0.44168 time= 0.17783\n",
      "Epoch: 0115 train_loss= 0.44120 time= 0.16319\n",
      "Epoch: 0116 train_loss= 0.44072 time= 0.17387\n",
      "Epoch: 0117 train_loss= 0.44024 time= 0.15914\n",
      "Epoch: 0118 train_loss= 0.43978 time= 0.16713\n",
      "Epoch: 0119 train_loss= 0.43931 time= 0.16814\n",
      "Epoch: 0120 train_loss= 0.43885 time= 0.16356\n",
      "Epoch: 0121 train_loss= 0.43839 time= 0.16792\n",
      "Epoch: 0122 train_loss= 0.43793 time= 0.16170\n",
      "Epoch: 0123 train_loss= 0.43747 time= 0.16954\n",
      "Epoch: 0124 train_loss= 0.43701 time= 0.16969\n",
      "Epoch: 0125 train_loss= 0.43655 time= 0.15820\n",
      "Epoch: 0126 train_loss= 0.43609 time= 0.17336\n",
      "Epoch: 0127 train_loss= 0.43564 time= 0.16472\n",
      "Epoch: 0128 train_loss= 0.43518 time= 0.15565\n",
      "Epoch: 0129 train_loss= 0.43473 time= 0.16723\n",
      "Epoch: 0130 train_loss= 0.43428 time= 0.16694\n",
      "Epoch: 0131 train_loss= 0.43384 time= 0.16182\n",
      "Epoch: 0132 train_loss= 0.43341 time= 0.16199\n",
      "Epoch: 0133 train_loss= 0.43299 time= 0.15958\n",
      "Epoch: 0134 train_loss= 0.43258 time= 0.16855\n",
      "Epoch: 0135 train_loss= 0.43219 time= 0.16769\n",
      "Epoch: 0136 train_loss= 0.43180 time= 0.16782\n",
      "Epoch: 0137 train_loss= 0.43142 time= 0.16042\n",
      "Epoch: 0138 train_loss= 0.43106 time= 0.16389\n",
      "Epoch: 0139 train_loss= 0.43070 time= 0.16680\n",
      "Epoch: 0140 train_loss= 0.43034 time= 0.15516\n",
      "Epoch: 0141 train_loss= 0.43000 time= 0.16482\n",
      "Epoch: 0142 train_loss= 0.42965 time= 0.16437\n",
      "Epoch: 0143 train_loss= 0.42931 time= 0.16205\n",
      "Epoch: 0144 train_loss= 0.42898 time= 0.17253\n",
      "Epoch: 0145 train_loss= 0.42866 time= 0.16556\n",
      "Epoch: 0146 train_loss= 0.42834 time= 0.17054\n",
      "Epoch: 0147 train_loss= 0.42804 time= 0.16556\n",
      "Epoch: 0148 train_loss= 0.42775 time= 0.15603\n",
      "Epoch: 0149 train_loss= 0.42748 time= 0.17722\n",
      "Epoch: 0150 train_loss= 0.42722 time= 0.16811\n",
      "Epoch: 0151 train_loss= 0.42698 time= 0.16466\n",
      "Epoch: 0152 train_loss= 0.42675 time= 0.15819\n",
      "Epoch: 0153 train_loss= 0.42653 time= 0.17372\n",
      "Epoch: 0154 train_loss= 0.42632 time= 0.16845\n",
      "Epoch: 0155 train_loss= 0.42611 time= 0.15500\n",
      "Epoch: 0156 train_loss= 0.42589 time= 0.17227\n",
      "Epoch: 0157 train_loss= 0.42568 time= 0.16257\n",
      "Epoch: 0158 train_loss= 0.42545 time= 0.17026\n",
      "Epoch: 0159 train_loss= 0.42523 time= 0.16606\n",
      "Epoch: 0160 train_loss= 0.42500 time= 0.15873\n",
      "Epoch: 0161 train_loss= 0.42476 time= 0.17215\n",
      "Epoch: 0162 train_loss= 0.42453 time= 0.16165\n",
      "Epoch: 0163 train_loss= 0.42429 time= 0.16247\n",
      "Epoch: 0164 train_loss= 0.42406 time= 0.15436\n",
      "Epoch: 0165 train_loss= 0.42383 time= 0.15506\n",
      "Epoch: 0166 train_loss= 0.42360 time= 0.16755\n",
      "Epoch: 0167 train_loss= 0.42337 time= 0.16256\n",
      "Epoch: 0168 train_loss= 0.42315 time= 0.16356\n",
      "Epoch: 0169 train_loss= 0.42292 time= 0.16838\n",
      "Epoch: 0170 train_loss= 0.42270 time= 0.15977\n",
      "Epoch: 0171 train_loss= 0.42248 time= 0.17978\n",
      "Epoch: 0172 train_loss= 0.42225 time= 0.17620\n",
      "Epoch: 0173 train_loss= 0.42203 time= 0.16844\n",
      "Epoch: 0174 train_loss= 0.42180 time= 0.17670\n",
      "Epoch: 0175 train_loss= 0.42158 time= 0.16613\n",
      "Epoch: 0176 train_loss= 0.42135 time= 0.17619\n",
      "Epoch: 0177 train_loss= 0.42113 time= 0.17641\n",
      "Epoch: 0178 train_loss= 0.42090 time= 0.16523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0179 train_loss= 0.42068 time= 0.17803\n",
      "Epoch: 0180 train_loss= 0.42046 time= 0.16006\n",
      "Epoch: 0181 train_loss= 0.42024 time= 0.17267\n",
      "Epoch: 0182 train_loss= 0.42003 time= 0.16634\n",
      "Epoch: 0183 train_loss= 0.41981 time= 0.17520\n",
      "Epoch: 0184 train_loss= 0.41960 time= 0.17487\n",
      "Epoch: 0185 train_loss= 0.41940 time= 0.16869\n",
      "Epoch: 0186 train_loss= 0.41919 time= 0.17645\n",
      "Epoch: 0187 train_loss= 0.41899 time= 0.17156\n",
      "Epoch: 0188 train_loss= 0.41879 time= 0.15804\n",
      "Epoch: 0189 train_loss= 0.41858 time= 0.17649\n",
      "Epoch: 0190 train_loss= 0.41837 time= 0.16741\n",
      "Epoch: 0191 train_loss= 0.41817 time= 0.16739\n",
      "Epoch: 0192 train_loss= 0.41796 time= 0.16471\n",
      "Epoch: 0193 train_loss= 0.41774 time= 0.15842\n",
      "Epoch: 0194 train_loss= 0.41752 time= 0.16796\n",
      "Epoch: 0195 train_loss= 0.41730 time= 0.16317\n",
      "Epoch: 0196 train_loss= 0.41708 time= 0.16729\n",
      "Epoch: 0197 train_loss= 0.41685 time= 0.16793\n",
      "Epoch: 0198 train_loss= 0.41662 time= 0.16232\n",
      "Epoch: 0199 train_loss= 0.41639 time= 0.17402\n",
      "Epoch: 0200 train_loss= 0.41615 time= 0.15832\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73520 time= 0.32844\n",
      "Epoch: 0002 train_loss= 0.73479 time= 0.16333\n",
      "Epoch: 0003 train_loss= 0.73334 time= 0.15764\n",
      "Epoch: 0004 train_loss= 0.73004 time= 0.15446\n",
      "Epoch: 0005 train_loss= 0.72393 time= 0.16253\n",
      "Epoch: 0006 train_loss= 0.71412 time= 0.16657\n",
      "Epoch: 0007 train_loss= 0.70025 time= 0.16054\n",
      "Epoch: 0008 train_loss= 0.68340 time= 0.16444\n",
      "Epoch: 0009 train_loss= 0.66730 time= 0.15957\n",
      "Epoch: 0010 train_loss= 0.65886 time= 0.16356\n",
      "Epoch: 0011 train_loss= 0.66161 time= 0.17253\n",
      "Epoch: 0012 train_loss= 0.66342 time= 0.16256\n",
      "Epoch: 0013 train_loss= 0.65726 time= 0.16855\n",
      "Epoch: 0014 train_loss= 0.64762 time= 0.16627\n",
      "Epoch: 0015 train_loss= 0.63961 time= 0.16445\n",
      "Epoch: 0016 train_loss= 0.63516 time= 0.16984\n",
      "Epoch: 0017 train_loss= 0.63341 time= 0.15578\n",
      "Epoch: 0018 train_loss= 0.63253 time= 0.14837\n",
      "Epoch: 0019 train_loss= 0.63115 time= 0.16855\n",
      "Epoch: 0020 train_loss= 0.62870 time= 0.15124\n",
      "Epoch: 0021 train_loss= 0.62527 time= 0.15858\n",
      "Epoch: 0022 train_loss= 0.62127 time= 0.17233\n",
      "Epoch: 0023 train_loss= 0.61722 time= 0.16600\n",
      "Epoch: 0024 train_loss= 0.61356 time= 0.16496\n",
      "Epoch: 0025 train_loss= 0.61039 time= 0.17172\n",
      "Epoch: 0026 train_loss= 0.60743 time= 0.17172\n",
      "Epoch: 0027 train_loss= 0.60412 time= 0.16787\n",
      "Epoch: 0028 train_loss= 0.60002 time= 0.16674\n",
      "Epoch: 0029 train_loss= 0.59503 time= 0.17447\n",
      "Epoch: 0030 train_loss= 0.58941 time= 0.15143\n",
      "Epoch: 0031 train_loss= 0.58354 time= 0.16771\n",
      "Epoch: 0032 train_loss= 0.57772 time= 0.17370\n",
      "Epoch: 0033 train_loss= 0.57198 time= 0.16695\n",
      "Epoch: 0034 train_loss= 0.56614 time= 0.16770\n",
      "Epoch: 0035 train_loss= 0.56001 time= 0.16012\n",
      "Epoch: 0036 train_loss= 0.55364 time= 0.16706\n",
      "Epoch: 0037 train_loss= 0.54738 time= 0.16458\n",
      "Epoch: 0038 train_loss= 0.54167 time= 0.15946\n",
      "Epoch: 0039 train_loss= 0.53698 time= 0.16393\n",
      "Epoch: 0040 train_loss= 0.53361 time= 0.17350\n",
      "Epoch: 0041 train_loss= 0.53165 time= 0.16400\n",
      "Epoch: 0042 train_loss= 0.53082 time= 0.16688\n",
      "Epoch: 0043 train_loss= 0.53055 time= 0.15707\n",
      "Epoch: 0044 train_loss= 0.53014 time= 0.17053\n",
      "Epoch: 0045 train_loss= 0.52898 time= 0.16356\n",
      "Epoch: 0046 train_loss= 0.52681 time= 0.16168\n",
      "Epoch: 0047 train_loss= 0.52373 time= 0.16704\n",
      "Epoch: 0048 train_loss= 0.52005 time= 0.16501\n",
      "Epoch: 0049 train_loss= 0.51618 time= 0.16755\n",
      "Epoch: 0050 train_loss= 0.51243 time= 0.16705\n",
      "Epoch: 0051 train_loss= 0.50904 time= 0.16704\n",
      "Epoch: 0052 train_loss= 0.50610 time= 0.16409\n",
      "Epoch: 0053 train_loss= 0.50357 time= 0.16307\n",
      "Epoch: 0054 train_loss= 0.50137 time= 0.16542\n",
      "Epoch: 0055 train_loss= 0.49938 time= 0.15944\n",
      "Epoch: 0056 train_loss= 0.49753 time= 0.16635\n",
      "Epoch: 0057 train_loss= 0.49577 time= 0.16855\n",
      "Epoch: 0058 train_loss= 0.49412 time= 0.16755\n",
      "Epoch: 0059 train_loss= 0.49257 time= 0.16719\n",
      "Epoch: 0060 train_loss= 0.49113 time= 0.15678\n",
      "Epoch: 0061 train_loss= 0.48980 time= 0.15770\n",
      "Epoch: 0062 train_loss= 0.48856 time= 0.18052\n",
      "Epoch: 0063 train_loss= 0.48739 time= 0.16901\n",
      "Epoch: 0064 train_loss= 0.48620 time= 0.15985\n",
      "Epoch: 0065 train_loss= 0.48496 time= 0.16600\n",
      "Epoch: 0066 train_loss= 0.48366 time= 0.16092\n",
      "Epoch: 0067 train_loss= 0.48232 time= 0.15763\n",
      "Epoch: 0068 train_loss= 0.48099 time= 0.17262\n",
      "Epoch: 0069 train_loss= 0.47969 time= 0.16755\n",
      "Epoch: 0070 train_loss= 0.47842 time= 0.17728\n",
      "Epoch: 0071 train_loss= 0.47717 time= 0.16510\n",
      "Epoch: 0072 train_loss= 0.47592 time= 0.16621\n",
      "Epoch: 0073 train_loss= 0.47468 time= 0.16885\n",
      "Epoch: 0074 train_loss= 0.47345 time= 0.16257\n",
      "Epoch: 0075 train_loss= 0.47222 time= 0.17377\n",
      "Epoch: 0076 train_loss= 0.47100 time= 0.17482\n",
      "Epoch: 0077 train_loss= 0.46979 time= 0.16661\n",
      "Epoch: 0078 train_loss= 0.46862 time= 0.16588\n",
      "Epoch: 0079 train_loss= 0.46750 time= 0.15988\n",
      "Epoch: 0080 train_loss= 0.46645 time= 0.17186\n",
      "Epoch: 0081 train_loss= 0.46547 time= 0.16156\n",
      "Epoch: 0082 train_loss= 0.46457 time= 0.16714\n",
      "Epoch: 0083 train_loss= 0.46374 time= 0.16530\n",
      "Epoch: 0084 train_loss= 0.46296 time= 0.16276\n",
      "Epoch: 0085 train_loss= 0.46222 time= 0.16394\n",
      "Epoch: 0086 train_loss= 0.46147 time= 0.17922\n",
      "Epoch: 0087 train_loss= 0.46071 time= 0.17043\n",
      "Epoch: 0088 train_loss= 0.45990 time= 0.16849\n",
      "Epoch: 0089 train_loss= 0.45904 time= 0.16878\n",
      "Epoch: 0090 train_loss= 0.45813 time= 0.16408\n",
      "Epoch: 0091 train_loss= 0.45718 time= 0.15392\n",
      "Epoch: 0092 train_loss= 0.45621 time= 0.16273\n",
      "Epoch: 0093 train_loss= 0.45523 time= 0.17653\n",
      "Epoch: 0094 train_loss= 0.45426 time= 0.16568\n",
      "Epoch: 0095 train_loss= 0.45330 time= 0.16145\n",
      "Epoch: 0096 train_loss= 0.45237 time= 0.17553\n",
      "Epoch: 0097 train_loss= 0.45146 time= 0.16286\n",
      "Epoch: 0098 train_loss= 0.45057 time= 0.16182\n",
      "Epoch: 0099 train_loss= 0.44971 time= 0.16771\n",
      "Epoch: 0100 train_loss= 0.44889 time= 0.16584\n",
      "Epoch: 0101 train_loss= 0.44809 time= 0.17929\n",
      "Epoch: 0102 train_loss= 0.44733 time= 0.16391\n",
      "Epoch: 0103 train_loss= 0.44659 time= 0.16298\n",
      "Epoch: 0104 train_loss= 0.44588 time= 0.16066\n",
      "Epoch: 0105 train_loss= 0.44519 time= 0.16755\n",
      "Epoch: 0106 train_loss= 0.44452 time= 0.17487\n",
      "Epoch: 0107 train_loss= 0.44386 time= 0.15767\n",
      "Epoch: 0108 train_loss= 0.44320 time= 0.16744\n",
      "Epoch: 0109 train_loss= 0.44254 time= 0.17457\n",
      "Epoch: 0110 train_loss= 0.44187 time= 0.16540\n",
      "Epoch: 0111 train_loss= 0.44120 time= 0.17694\n",
      "Epoch: 0112 train_loss= 0.44052 time= 0.17227\n",
      "Epoch: 0113 train_loss= 0.43985 time= 0.16678\n",
      "Epoch: 0114 train_loss= 0.43919 time= 0.16317\n",
      "Epoch: 0115 train_loss= 0.43854 time= 0.14755\n",
      "Epoch: 0116 train_loss= 0.43792 time= 0.18141\n",
      "Epoch: 0117 train_loss= 0.43732 time= 0.17165\n",
      "Epoch: 0118 train_loss= 0.43676 time= 0.17482\n",
      "Epoch: 0119 train_loss= 0.43624 time= 0.17150\n",
      "Epoch: 0120 train_loss= 0.43575 time= 0.16787\n",
      "Epoch: 0121 train_loss= 0.43531 time= 0.18320\n",
      "Epoch: 0122 train_loss= 0.43490 time= 0.16983\n",
      "Epoch: 0123 train_loss= 0.43453 time= 0.17589\n",
      "Epoch: 0124 train_loss= 0.43419 time= 0.17261\n",
      "Epoch: 0125 train_loss= 0.43387 time= 0.16586\n",
      "Epoch: 0126 train_loss= 0.43355 time= 0.18202\n",
      "Epoch: 0127 train_loss= 0.43324 time= 0.16930\n",
      "Epoch: 0128 train_loss= 0.43292 time= 0.16057\n",
      "Epoch: 0129 train_loss= 0.43258 time= 0.16396\n",
      "Epoch: 0130 train_loss= 0.43224 time= 0.17101\n",
      "Epoch: 0131 train_loss= 0.43188 time= 0.18177\n",
      "Epoch: 0132 train_loss= 0.43152 time= 0.16756\n",
      "Epoch: 0133 train_loss= 0.43115 time= 0.16138\n",
      "Epoch: 0134 train_loss= 0.43079 time= 0.16802\n",
      "Epoch: 0135 train_loss= 0.43043 time= 0.16214\n",
      "Epoch: 0136 train_loss= 0.43007 time= 0.17583\n",
      "Epoch: 0137 train_loss= 0.42972 time= 0.15566\n",
      "Epoch: 0138 train_loss= 0.42936 time= 0.15775\n",
      "Epoch: 0139 train_loss= 0.42901 time= 0.16450\n",
      "Epoch: 0140 train_loss= 0.42865 time= 0.16577\n",
      "Epoch: 0141 train_loss= 0.42828 time= 0.16855\n",
      "Epoch: 0142 train_loss= 0.42790 time= 0.16840\n",
      "Epoch: 0143 train_loss= 0.42752 time= 0.15858\n",
      "Epoch: 0144 train_loss= 0.42713 time= 0.16889\n",
      "Epoch: 0145 train_loss= 0.42674 time= 0.15365\n",
      "Epoch: 0146 train_loss= 0.42634 time= 0.17691\n",
      "Epoch: 0147 train_loss= 0.42595 time= 0.16392\n",
      "Epoch: 0148 train_loss= 0.42556 time= 0.15949\n",
      "Epoch: 0149 train_loss= 0.42519 time= 0.18224\n",
      "Epoch: 0150 train_loss= 0.42482 time= 0.16045\n",
      "Epoch: 0151 train_loss= 0.42447 time= 0.17673\n",
      "Epoch: 0152 train_loss= 0.42414 time= 0.17583\n",
      "Epoch: 0153 train_loss= 0.42382 time= 0.16955\n",
      "Epoch: 0154 train_loss= 0.42352 time= 0.17136\n",
      "Epoch: 0155 train_loss= 0.42323 time= 0.16129\n",
      "Epoch: 0156 train_loss= 0.42295 time= 0.16661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0157 train_loss= 0.42268 time= 0.16275\n",
      "Epoch: 0158 train_loss= 0.42240 time= 0.16781\n",
      "Epoch: 0159 train_loss= 0.42213 time= 0.16277\n",
      "Epoch: 0160 train_loss= 0.42186 time= 0.16622\n",
      "Epoch: 0161 train_loss= 0.42158 time= 0.16988\n",
      "Epoch: 0162 train_loss= 0.42130 time= 0.16412\n",
      "Epoch: 0163 train_loss= 0.42102 time= 0.16311\n",
      "Epoch: 0164 train_loss= 0.42074 time= 0.17026\n",
      "Epoch: 0165 train_loss= 0.42047 time= 0.16892\n",
      "Epoch: 0166 train_loss= 0.42021 time= 0.16400\n",
      "Epoch: 0167 train_loss= 0.41995 time= 0.17261\n",
      "Epoch: 0168 train_loss= 0.41970 time= 0.16559\n",
      "Epoch: 0169 train_loss= 0.41947 time= 0.16270\n",
      "Epoch: 0170 train_loss= 0.41924 time= 0.17226\n",
      "Epoch: 0171 train_loss= 0.41903 time= 0.16067\n",
      "Epoch: 0172 train_loss= 0.41882 time= 0.16833\n",
      "Epoch: 0173 train_loss= 0.41862 time= 0.15310\n",
      "Epoch: 0174 train_loss= 0.41843 time= 0.15071\n",
      "Epoch: 0175 train_loss= 0.41825 time= 0.16735\n",
      "Epoch: 0176 train_loss= 0.41807 time= 0.16143\n",
      "Epoch: 0177 train_loss= 0.41790 time= 0.16057\n",
      "Epoch: 0178 train_loss= 0.41773 time= 0.16256\n",
      "Epoch: 0179 train_loss= 0.41757 time= 0.15715\n",
      "Epoch: 0180 train_loss= 0.41741 time= 0.17074\n",
      "Epoch: 0181 train_loss= 0.41726 time= 0.16292\n",
      "Epoch: 0182 train_loss= 0.41712 time= 0.17390\n",
      "Epoch: 0183 train_loss= 0.41697 time= 0.17765\n",
      "Epoch: 0184 train_loss= 0.41683 time= 0.16266\n",
      "Epoch: 0185 train_loss= 0.41670 time= 0.16489\n",
      "Epoch: 0186 train_loss= 0.41657 time= 0.16908\n",
      "Epoch: 0187 train_loss= 0.41644 time= 0.15751\n",
      "Epoch: 0188 train_loss= 0.41631 time= 0.16867\n",
      "Epoch: 0189 train_loss= 0.41619 time= 0.17621\n",
      "Epoch: 0190 train_loss= 0.41607 time= 0.17152\n",
      "Epoch: 0191 train_loss= 0.41595 time= 0.16934\n",
      "Epoch: 0192 train_loss= 0.41583 time= 0.16706\n",
      "Epoch: 0193 train_loss= 0.41571 time= 0.16288\n",
      "Epoch: 0194 train_loss= 0.41560 time= 0.15689\n",
      "Epoch: 0195 train_loss= 0.41549 time= 0.17387\n",
      "Epoch: 0196 train_loss= 0.41538 time= 0.17195\n",
      "Epoch: 0197 train_loss= 0.41526 time= 0.16068\n",
      "Epoch: 0198 train_loss= 0.41515 time= 0.16428\n",
      "Epoch: 0199 train_loss= 0.41504 time= 0.16030\n",
      "Epoch: 0200 train_loss= 0.41493 time= 0.16234\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73519 time= 0.31367\n",
      "Epoch: 0002 train_loss= 0.73471 time= 0.16691\n",
      "Epoch: 0003 train_loss= 0.73310 time= 0.15853\n",
      "Epoch: 0004 train_loss= 0.72948 time= 0.15264\n",
      "Epoch: 0005 train_loss= 0.72293 time= 0.15879\n",
      "Epoch: 0006 train_loss= 0.71264 time= 0.15513\n",
      "Epoch: 0007 train_loss= 0.69846 time= 0.17208\n",
      "Epoch: 0008 train_loss= 0.68175 time= 0.16053\n",
      "Epoch: 0009 train_loss= 0.66657 time= 0.16757\n",
      "Epoch: 0010 train_loss= 0.65961 time= 0.17057\n",
      "Epoch: 0011 train_loss= 0.66287 time= 0.16126\n",
      "Epoch: 0012 train_loss= 0.66394 time= 0.16837\n",
      "Epoch: 0013 train_loss= 0.65727 time= 0.16218\n",
      "Epoch: 0014 train_loss= 0.64753 time= 0.15769\n",
      "Epoch: 0015 train_loss= 0.63955 time= 0.16579\n",
      "Epoch: 0016 train_loss= 0.63507 time= 0.16630\n",
      "Epoch: 0017 train_loss= 0.63312 time= 0.16603\n",
      "Epoch: 0018 train_loss= 0.63189 time= 0.16205\n",
      "Epoch: 0019 train_loss= 0.63002 time= 0.16257\n",
      "Epoch: 0020 train_loss= 0.62698 time= 0.16556\n",
      "Epoch: 0021 train_loss= 0.62286 time= 0.16356\n",
      "Epoch: 0022 train_loss= 0.61809 time= 0.17296\n",
      "Epoch: 0023 train_loss= 0.61321 time= 0.16728\n",
      "Epoch: 0024 train_loss= 0.60865 time= 0.16750\n",
      "Epoch: 0025 train_loss= 0.60455 time= 0.16713\n",
      "Epoch: 0026 train_loss= 0.60064 time= 0.16092\n",
      "Epoch: 0027 train_loss= 0.59641 time= 0.16823\n",
      "Epoch: 0028 train_loss= 0.59144 time= 0.15791\n",
      "Epoch: 0029 train_loss= 0.58574 time= 0.14415\n",
      "Epoch: 0030 train_loss= 0.57973 time= 0.16543\n",
      "Epoch: 0031 train_loss= 0.57401 time= 0.17018\n",
      "Epoch: 0032 train_loss= 0.56907 time= 0.16533\n",
      "Epoch: 0033 train_loss= 0.56501 time= 0.15903\n",
      "Epoch: 0034 train_loss= 0.56151 time= 0.16733\n",
      "Epoch: 0035 train_loss= 0.55815 time= 0.16896\n",
      "Epoch: 0036 train_loss= 0.55474 time= 0.15372\n",
      "Epoch: 0037 train_loss= 0.55134 time= 0.15742\n",
      "Epoch: 0038 train_loss= 0.54817 time= 0.16586\n",
      "Epoch: 0039 train_loss= 0.54533 time= 0.15709\n",
      "Epoch: 0040 train_loss= 0.54270 time= 0.17252\n",
      "Epoch: 0041 train_loss= 0.54006 time= 0.15494\n",
      "Epoch: 0042 train_loss= 0.53728 time= 0.16050\n",
      "Epoch: 0043 train_loss= 0.53446 time= 0.16355\n",
      "Epoch: 0044 train_loss= 0.53172 time= 0.16159\n",
      "Epoch: 0045 train_loss= 0.52907 time= 0.17045\n",
      "Epoch: 0046 train_loss= 0.52636 time= 0.17500\n",
      "Epoch: 0047 train_loss= 0.52343 time= 0.16796\n",
      "Epoch: 0048 train_loss= 0.52021 time= 0.16868\n",
      "Epoch: 0049 train_loss= 0.51681 time= 0.16123\n",
      "Epoch: 0050 train_loss= 0.51342 time= 0.16878\n",
      "Epoch: 0051 train_loss= 0.51022 time= 0.17651\n",
      "Epoch: 0052 train_loss= 0.50734 time= 0.16792\n",
      "Epoch: 0053 train_loss= 0.50485 time= 0.16731\n",
      "Epoch: 0054 train_loss= 0.50279 time= 0.15521\n",
      "Epoch: 0055 train_loss= 0.50115 time= 0.17454\n",
      "Epoch: 0056 train_loss= 0.49987 time= 0.17380\n",
      "Epoch: 0057 train_loss= 0.49885 time= 0.17079\n",
      "Epoch: 0058 train_loss= 0.49792 time= 0.16287\n",
      "Epoch: 0059 train_loss= 0.49693 time= 0.18055\n",
      "Epoch: 0060 train_loss= 0.49576 time= 0.15708\n",
      "Epoch: 0061 train_loss= 0.49438 time= 0.16505\n",
      "Epoch: 0062 train_loss= 0.49282 time= 0.16215\n",
      "Epoch: 0063 train_loss= 0.49116 time= 0.16525\n",
      "Epoch: 0064 train_loss= 0.48949 time= 0.16961\n",
      "Epoch: 0065 train_loss= 0.48785 time= 0.15509\n",
      "Epoch: 0066 train_loss= 0.48626 time= 0.16425\n",
      "Epoch: 0067 train_loss= 0.48472 time= 0.17552\n",
      "Epoch: 0068 train_loss= 0.48321 time= 0.16293\n",
      "Epoch: 0069 train_loss= 0.48169 time= 0.18384\n",
      "Epoch: 0070 train_loss= 0.48017 time= 0.16573\n",
      "Epoch: 0071 train_loss= 0.47864 time= 0.17329\n",
      "Epoch: 0072 train_loss= 0.47710 time= 0.15887\n",
      "Epoch: 0073 train_loss= 0.47555 time= 0.17411\n",
      "Epoch: 0074 train_loss= 0.47401 time= 0.16484\n",
      "Epoch: 0075 train_loss= 0.47248 time= 0.15698\n",
      "Epoch: 0076 train_loss= 0.47097 time= 0.16366\n",
      "Epoch: 0077 train_loss= 0.46950 time= 0.15991\n",
      "Epoch: 0078 train_loss= 0.46809 time= 0.16781\n",
      "Epoch: 0079 train_loss= 0.46675 time= 0.16664\n",
      "Epoch: 0080 train_loss= 0.46552 time= 0.16981\n",
      "Epoch: 0081 train_loss= 0.46440 time= 0.16921\n",
      "Epoch: 0082 train_loss= 0.46340 time= 0.16580\n",
      "Epoch: 0083 train_loss= 0.46247 time= 0.16432\n",
      "Epoch: 0084 train_loss= 0.46160 time= 0.18063\n",
      "Epoch: 0085 train_loss= 0.46073 time= 0.17566\n",
      "Epoch: 0086 train_loss= 0.45985 time= 0.16119\n",
      "Epoch: 0087 train_loss= 0.45898 time= 0.17478\n",
      "Epoch: 0088 train_loss= 0.45814 time= 0.15942\n",
      "Epoch: 0089 train_loss= 0.45733 time= 0.16312\n",
      "Epoch: 0090 train_loss= 0.45657 time= 0.16533\n",
      "Epoch: 0091 train_loss= 0.45586 time= 0.17365\n",
      "Epoch: 0092 train_loss= 0.45519 time= 0.17442\n",
      "Epoch: 0093 train_loss= 0.45455 time= 0.17759\n",
      "Epoch: 0094 train_loss= 0.45395 time= 0.16967\n",
      "Epoch: 0095 train_loss= 0.45338 time= 0.16485\n",
      "Epoch: 0096 train_loss= 0.45280 time= 0.16675\n",
      "Epoch: 0097 train_loss= 0.45222 time= 0.16986\n",
      "Epoch: 0098 train_loss= 0.45161 time= 0.16450\n",
      "Epoch: 0099 train_loss= 0.45098 time= 0.17948\n",
      "Epoch: 0100 train_loss= 0.45033 time= 0.16942\n",
      "Epoch: 0101 train_loss= 0.44969 time= 0.16618\n",
      "Epoch: 0102 train_loss= 0.44905 time= 0.17076\n",
      "Epoch: 0103 train_loss= 0.44840 time= 0.16605\n",
      "Epoch: 0104 train_loss= 0.44776 time= 0.17761\n",
      "Epoch: 0105 train_loss= 0.44713 time= 0.16746\n",
      "Epoch: 0106 train_loss= 0.44650 time= 0.16157\n",
      "Epoch: 0107 train_loss= 0.44589 time= 0.17445\n",
      "Epoch: 0108 train_loss= 0.44528 time= 0.16384\n",
      "Epoch: 0109 train_loss= 0.44466 time= 0.17224\n",
      "Epoch: 0110 train_loss= 0.44404 time= 0.16432\n",
      "Epoch: 0111 train_loss= 0.44343 time= 0.16809\n",
      "Epoch: 0112 train_loss= 0.44282 time= 0.16154\n",
      "Epoch: 0113 train_loss= 0.44221 time= 0.15253\n",
      "Epoch: 0114 train_loss= 0.44162 time= 0.17662\n",
      "Epoch: 0115 train_loss= 0.44103 time= 0.16755\n",
      "Epoch: 0116 train_loss= 0.44046 time= 0.16655\n",
      "Epoch: 0117 train_loss= 0.43992 time= 0.17084\n",
      "Epoch: 0118 train_loss= 0.43939 time= 0.16578\n",
      "Epoch: 0119 train_loss= 0.43889 time= 0.17484\n",
      "Epoch: 0120 train_loss= 0.43841 time= 0.16695\n",
      "Epoch: 0121 train_loss= 0.43793 time= 0.15573\n",
      "Epoch: 0122 train_loss= 0.43746 time= 0.17086\n",
      "Epoch: 0123 train_loss= 0.43699 time= 0.15795\n",
      "Epoch: 0124 train_loss= 0.43651 time= 0.16929\n",
      "Epoch: 0125 train_loss= 0.43602 time= 0.16245\n",
      "Epoch: 0126 train_loss= 0.43550 time= 0.16124\n",
      "Epoch: 0127 train_loss= 0.43498 time= 0.16776\n",
      "Epoch: 0128 train_loss= 0.43444 time= 0.16166\n",
      "Epoch: 0129 train_loss= 0.43390 time= 0.16547\n",
      "Epoch: 0130 train_loss= 0.43335 time= 0.15559\n",
      "Epoch: 0131 train_loss= 0.43282 time= 0.16057\n",
      "Epoch: 0132 train_loss= 0.43230 time= 0.16913\n",
      "Epoch: 0133 train_loss= 0.43179 time= 0.15983\n",
      "Epoch: 0134 train_loss= 0.43130 time= 0.16345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0135 train_loss= 0.43083 time= 0.15876\n",
      "Epoch: 0136 train_loss= 0.43037 time= 0.15646\n",
      "Epoch: 0137 train_loss= 0.42994 time= 0.16431\n",
      "Epoch: 0138 train_loss= 0.42952 time= 0.16415\n",
      "Epoch: 0139 train_loss= 0.42913 time= 0.16556\n",
      "Epoch: 0140 train_loss= 0.42876 time= 0.17671\n",
      "Epoch: 0141 train_loss= 0.42841 time= 0.17424\n",
      "Epoch: 0142 train_loss= 0.42808 time= 0.17745\n",
      "Epoch: 0143 train_loss= 0.42776 time= 0.17212\n",
      "Epoch: 0144 train_loss= 0.42747 time= 0.17374\n",
      "Epoch: 0145 train_loss= 0.42720 time= 0.17713\n",
      "Epoch: 0146 train_loss= 0.42693 time= 0.17356\n",
      "Epoch: 0147 train_loss= 0.42668 time= 0.16561\n",
      "Epoch: 0148 train_loss= 0.42644 time= 0.16710\n",
      "Epoch: 0149 train_loss= 0.42620 time= 0.16463\n",
      "Epoch: 0150 train_loss= 0.42596 time= 0.17085\n",
      "Epoch: 0151 train_loss= 0.42573 time= 0.17149\n",
      "Epoch: 0152 train_loss= 0.42550 time= 0.16955\n",
      "Epoch: 0153 train_loss= 0.42526 time= 0.17453\n",
      "Epoch: 0154 train_loss= 0.42503 time= 0.16595\n",
      "Epoch: 0155 train_loss= 0.42480 time= 0.18187\n",
      "Epoch: 0156 train_loss= 0.42456 time= 0.16186\n",
      "Epoch: 0157 train_loss= 0.42433 time= 0.17186\n",
      "Epoch: 0158 train_loss= 0.42410 time= 0.16592\n",
      "Epoch: 0159 train_loss= 0.42387 time= 0.17075\n",
      "Epoch: 0160 train_loss= 0.42364 time= 0.15942\n",
      "Epoch: 0161 train_loss= 0.42342 time= 0.16225\n",
      "Epoch: 0162 train_loss= 0.42319 time= 0.15558\n",
      "Epoch: 0163 train_loss= 0.42297 time= 0.16855\n",
      "Epoch: 0164 train_loss= 0.42274 time= 0.16041\n",
      "Epoch: 0165 train_loss= 0.42252 time= 0.16556\n",
      "Epoch: 0166 train_loss= 0.42230 time= 0.16855\n",
      "Epoch: 0167 train_loss= 0.42208 time= 0.15647\n",
      "Epoch: 0168 train_loss= 0.42186 time= 0.17578\n",
      "Epoch: 0169 train_loss= 0.42164 time= 0.16819\n",
      "Epoch: 0170 train_loss= 0.42142 time= 0.15553\n",
      "Epoch: 0171 train_loss= 0.42121 time= 0.17822\n",
      "Epoch: 0172 train_loss= 0.42100 time= 0.15835\n",
      "Epoch: 0173 train_loss= 0.42079 time= 0.16311\n",
      "Epoch: 0174 train_loss= 0.42059 time= 0.16710\n",
      "Epoch: 0175 train_loss= 0.42039 time= 0.16556\n",
      "Epoch: 0176 train_loss= 0.42020 time= 0.17416\n",
      "Epoch: 0177 train_loss= 0.42001 time= 0.16006\n",
      "Epoch: 0178 train_loss= 0.41983 time= 0.16877\n",
      "Epoch: 0179 train_loss= 0.41965 time= 0.16110\n",
      "Epoch: 0180 train_loss= 0.41947 time= 0.16074\n",
      "Epoch: 0181 train_loss= 0.41929 time= 0.17231\n",
      "Epoch: 0182 train_loss= 0.41911 time= 0.16310\n",
      "Epoch: 0183 train_loss= 0.41893 time= 0.16651\n",
      "Epoch: 0184 train_loss= 0.41875 time= 0.16449\n",
      "Epoch: 0185 train_loss= 0.41856 time= 0.16293\n",
      "Epoch: 0186 train_loss= 0.41838 time= 0.16922\n",
      "Epoch: 0187 train_loss= 0.41820 time= 0.15958\n",
      "Epoch: 0188 train_loss= 0.41802 time= 0.16556\n",
      "Epoch: 0189 train_loss= 0.41784 time= 0.16556\n",
      "Epoch: 0190 train_loss= 0.41766 time= 0.16755\n",
      "Epoch: 0191 train_loss= 0.41749 time= 0.16238\n",
      "Epoch: 0192 train_loss= 0.41732 time= 0.16116\n",
      "Epoch: 0193 train_loss= 0.41716 time= 0.16262\n",
      "Epoch: 0194 train_loss= 0.41701 time= 0.17165\n",
      "Epoch: 0195 train_loss= 0.41686 time= 0.15782\n",
      "Epoch: 0196 train_loss= 0.41672 time= 0.16248\n",
      "Epoch: 0197 train_loss= 0.41659 time= 0.16783\n",
      "Epoch: 0198 train_loss= 0.41646 time= 0.15983\n",
      "Epoch: 0199 train_loss= 0.41634 time= 0.17254\n",
      "Epoch: 0200 train_loss= 0.41623 time= 0.17054\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73519 time= 0.32376\n",
      "Epoch: 0002 train_loss= 0.73461 time= 0.15981\n",
      "Epoch: 0003 train_loss= 0.73268 time= 0.15347\n",
      "Epoch: 0004 train_loss= 0.72843 time= 0.16866\n",
      "Epoch: 0005 train_loss= 0.72096 time= 0.16548\n",
      "Epoch: 0006 train_loss= 0.70966 time= 0.15387\n",
      "Epoch: 0007 train_loss= 0.69472 time= 0.15446\n",
      "Epoch: 0008 train_loss= 0.67813 time= 0.15765\n",
      "Epoch: 0009 train_loss= 0.66461 time= 0.15558\n",
      "Epoch: 0010 train_loss= 0.66037 time= 0.17029\n",
      "Epoch: 0011 train_loss= 0.66376 time= 0.17229\n",
      "Epoch: 0012 train_loss= 0.66258 time= 0.17278\n",
      "Epoch: 0013 train_loss= 0.65485 time= 0.16496\n",
      "Epoch: 0014 train_loss= 0.64542 time= 0.16032\n",
      "Epoch: 0015 train_loss= 0.63819 time= 0.16347\n",
      "Epoch: 0016 train_loss= 0.63415 time= 0.15943\n",
      "Epoch: 0017 train_loss= 0.63223 time= 0.15880\n",
      "Epoch: 0018 train_loss= 0.63080 time= 0.16455\n",
      "Epoch: 0019 train_loss= 0.62874 time= 0.16929\n",
      "Epoch: 0020 train_loss= 0.62562 time= 0.15552\n",
      "Epoch: 0021 train_loss= 0.62155 time= 0.16772\n",
      "Epoch: 0022 train_loss= 0.61691 time= 0.16356\n",
      "Epoch: 0023 train_loss= 0.61214 time= 0.16609\n",
      "Epoch: 0024 train_loss= 0.60759 time= 0.16639\n",
      "Epoch: 0025 train_loss= 0.60330 time= 0.16446\n",
      "Epoch: 0026 train_loss= 0.59900 time= 0.16954\n",
      "Epoch: 0027 train_loss= 0.59426 time= 0.15394\n",
      "Epoch: 0028 train_loss= 0.58871 time= 0.15681\n",
      "Epoch: 0029 train_loss= 0.58236 time= 0.16623\n",
      "Epoch: 0030 train_loss= 0.57545 time= 0.16952\n",
      "Epoch: 0031 train_loss= 0.56840 time= 0.18712\n",
      "Epoch: 0032 train_loss= 0.56159 time= 0.18672\n",
      "Epoch: 0033 train_loss= 0.55513 time= 0.16764\n",
      "Epoch: 0034 train_loss= 0.54898 time= 0.16860\n",
      "Epoch: 0035 train_loss= 0.54302 time= 0.17027\n",
      "Epoch: 0036 train_loss= 0.53730 time= 0.16823\n",
      "Epoch: 0037 train_loss= 0.53200 time= 0.17550\n",
      "Epoch: 0038 train_loss= 0.52730 time= 0.15667\n",
      "Epoch: 0039 train_loss= 0.52326 time= 0.16197\n",
      "Epoch: 0040 train_loss= 0.51976 time= 0.17596\n",
      "Epoch: 0041 train_loss= 0.51656 time= 0.17282\n",
      "Epoch: 0042 train_loss= 0.51347 time= 0.16877\n",
      "Epoch: 0043 train_loss= 0.51036 time= 0.17267\n",
      "Epoch: 0044 train_loss= 0.50723 time= 0.16855\n",
      "Epoch: 0045 train_loss= 0.50419 time= 0.16570\n",
      "Epoch: 0046 train_loss= 0.50145 time= 0.15459\n",
      "Epoch: 0047 train_loss= 0.49912 time= 0.16828\n",
      "Epoch: 0048 train_loss= 0.49722 time= 0.17860\n",
      "Epoch: 0049 train_loss= 0.49563 time= 0.16540\n",
      "Epoch: 0050 train_loss= 0.49416 time= 0.17086\n",
      "Epoch: 0051 train_loss= 0.49265 time= 0.15461\n",
      "Epoch: 0052 train_loss= 0.49103 time= 0.16770\n",
      "Epoch: 0053 train_loss= 0.48932 time= 0.16157\n",
      "Epoch: 0054 train_loss= 0.48759 time= 0.16299\n",
      "Epoch: 0055 train_loss= 0.48590 time= 0.17405\n",
      "Epoch: 0056 train_loss= 0.48430 time= 0.17219\n",
      "Epoch: 0057 train_loss= 0.48280 time= 0.16696\n",
      "Epoch: 0058 train_loss= 0.48137 time= 0.16652\n",
      "Epoch: 0059 train_loss= 0.48001 time= 0.16281\n",
      "Epoch: 0060 train_loss= 0.47870 time= 0.17043\n",
      "Epoch: 0061 train_loss= 0.47741 time= 0.15472\n",
      "Epoch: 0062 train_loss= 0.47613 time= 0.16090\n",
      "Epoch: 0063 train_loss= 0.47485 time= 0.17075\n",
      "Epoch: 0064 train_loss= 0.47357 time= 0.16607\n",
      "Epoch: 0065 train_loss= 0.47231 time= 0.15958\n",
      "Epoch: 0066 train_loss= 0.47110 time= 0.17253\n",
      "Epoch: 0067 train_loss= 0.46998 time= 0.17355\n",
      "Epoch: 0068 train_loss= 0.46898 time= 0.17533\n",
      "Epoch: 0069 train_loss= 0.46809 time= 0.16082\n",
      "Epoch: 0070 train_loss= 0.46733 time= 0.16655\n",
      "Epoch: 0071 train_loss= 0.46666 time= 0.17346\n",
      "Epoch: 0072 train_loss= 0.46607 time= 0.15789\n",
      "Epoch: 0073 train_loss= 0.46550 time= 0.17600\n",
      "Epoch: 0074 train_loss= 0.46493 time= 0.16838\n",
      "Epoch: 0075 train_loss= 0.46430 time= 0.15494\n",
      "Epoch: 0076 train_loss= 0.46359 time= 0.16976\n",
      "Epoch: 0077 train_loss= 0.46282 time= 0.16860\n",
      "Epoch: 0078 train_loss= 0.46199 time= 0.16045\n",
      "Epoch: 0079 train_loss= 0.46114 time= 0.16542\n",
      "Epoch: 0080 train_loss= 0.46029 time= 0.16641\n",
      "Epoch: 0081 train_loss= 0.45944 time= 0.17178\n",
      "Epoch: 0082 train_loss= 0.45860 time= 0.16665\n",
      "Epoch: 0083 train_loss= 0.45779 time= 0.15546\n",
      "Epoch: 0084 train_loss= 0.45699 time= 0.17645\n",
      "Epoch: 0085 train_loss= 0.45622 time= 0.15594\n",
      "Epoch: 0086 train_loss= 0.45547 time= 0.15970\n",
      "Epoch: 0087 train_loss= 0.45473 time= 0.16776\n",
      "Epoch: 0088 train_loss= 0.45401 time= 0.15480\n",
      "Epoch: 0089 train_loss= 0.45331 time= 0.16855\n",
      "Epoch: 0090 train_loss= 0.45264 time= 0.16049\n",
      "Epoch: 0091 train_loss= 0.45201 time= 0.16458\n",
      "Epoch: 0092 train_loss= 0.45142 time= 0.17111\n",
      "Epoch: 0093 train_loss= 0.45086 time= 0.16152\n",
      "Epoch: 0094 train_loss= 0.45032 time= 0.17318\n",
      "Epoch: 0095 train_loss= 0.44980 time= 0.17034\n",
      "Epoch: 0096 train_loss= 0.44928 time= 0.16256\n",
      "Epoch: 0097 train_loss= 0.44875 time= 0.18755\n",
      "Epoch: 0098 train_loss= 0.44822 time= 0.16644\n",
      "Epoch: 0099 train_loss= 0.44766 time= 0.17314\n",
      "Epoch: 0100 train_loss= 0.44709 time= 0.16159\n",
      "Epoch: 0101 train_loss= 0.44651 time= 0.16385\n",
      "Epoch: 0102 train_loss= 0.44592 time= 0.17479\n",
      "Epoch: 0103 train_loss= 0.44533 time= 0.16981\n",
      "Epoch: 0104 train_loss= 0.44473 time= 0.17505\n",
      "Epoch: 0105 train_loss= 0.44413 time= 0.17321\n",
      "Epoch: 0106 train_loss= 0.44354 time= 0.16740\n",
      "Epoch: 0107 train_loss= 0.44294 time= 0.18556\n",
      "Epoch: 0108 train_loss= 0.44235 time= 0.15961\n",
      "Epoch: 0109 train_loss= 0.44176 time= 0.15961\n",
      "Epoch: 0110 train_loss= 0.44118 time= 0.16491\n",
      "Epoch: 0111 train_loss= 0.44060 time= 0.16175\n",
      "Epoch: 0112 train_loss= 0.44002 time= 0.16940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0113 train_loss= 0.43946 time= 0.17453\n",
      "Epoch: 0114 train_loss= 0.43891 time= 0.16636\n",
      "Epoch: 0115 train_loss= 0.43837 time= 0.16770\n",
      "Epoch: 0116 train_loss= 0.43784 time= 0.16716\n",
      "Epoch: 0117 train_loss= 0.43733 time= 0.16336\n",
      "Epoch: 0118 train_loss= 0.43684 time= 0.16473\n",
      "Epoch: 0119 train_loss= 0.43636 time= 0.16308\n",
      "Epoch: 0120 train_loss= 0.43590 time= 0.16656\n",
      "Epoch: 0121 train_loss= 0.43545 time= 0.17224\n",
      "Epoch: 0122 train_loss= 0.43502 time= 0.15889\n",
      "Epoch: 0123 train_loss= 0.43460 time= 0.16878\n",
      "Epoch: 0124 train_loss= 0.43419 time= 0.16357\n",
      "Epoch: 0125 train_loss= 0.43379 time= 0.16959\n",
      "Epoch: 0126 train_loss= 0.43340 time= 0.16761\n",
      "Epoch: 0127 train_loss= 0.43301 time= 0.15402\n",
      "Epoch: 0128 train_loss= 0.43263 time= 0.17288\n",
      "Epoch: 0129 train_loss= 0.43225 time= 0.15956\n",
      "Epoch: 0130 train_loss= 0.43188 time= 0.17249\n",
      "Epoch: 0131 train_loss= 0.43150 time= 0.16834\n",
      "Epoch: 0132 train_loss= 0.43112 time= 0.17425\n",
      "Epoch: 0133 train_loss= 0.43073 time= 0.16609\n",
      "Epoch: 0134 train_loss= 0.43034 time= 0.16223\n",
      "Epoch: 0135 train_loss= 0.42995 time= 0.15942\n",
      "Epoch: 0136 train_loss= 0.42955 time= 0.16411\n",
      "Epoch: 0137 train_loss= 0.42915 time= 0.16890\n",
      "Epoch: 0138 train_loss= 0.42875 time= 0.15722\n",
      "Epoch: 0139 train_loss= 0.42836 time= 0.17254\n",
      "Epoch: 0140 train_loss= 0.42796 time= 0.16351\n",
      "Epoch: 0141 train_loss= 0.42756 time= 0.17300\n",
      "Epoch: 0142 train_loss= 0.42717 time= 0.17570\n",
      "Epoch: 0143 train_loss= 0.42679 time= 0.16192\n",
      "Epoch: 0144 train_loss= 0.42640 time= 0.17658\n",
      "Epoch: 0145 train_loss= 0.42602 time= 0.15383\n",
      "Epoch: 0146 train_loss= 0.42565 time= 0.15766\n",
      "Epoch: 0147 train_loss= 0.42528 time= 0.16923\n",
      "Epoch: 0148 train_loss= 0.42491 time= 0.16529\n",
      "Epoch: 0149 train_loss= 0.42456 time= 0.17147\n",
      "Epoch: 0150 train_loss= 0.42421 time= 0.17088\n",
      "Epoch: 0151 train_loss= 0.42387 time= 0.16355\n",
      "Epoch: 0152 train_loss= 0.42353 time= 0.17558\n",
      "Epoch: 0153 train_loss= 0.42319 time= 0.16488\n",
      "Epoch: 0154 train_loss= 0.42285 time= 0.16637\n",
      "Epoch: 0155 train_loss= 0.42252 time= 0.16212\n",
      "Epoch: 0156 train_loss= 0.42218 time= 0.15621\n",
      "Epoch: 0157 train_loss= 0.42185 time= 0.16274\n",
      "Epoch: 0158 train_loss= 0.42152 time= 0.15480\n",
      "Epoch: 0159 train_loss= 0.42120 time= 0.15530\n",
      "Epoch: 0160 train_loss= 0.42090 time= 0.16592\n",
      "Epoch: 0161 train_loss= 0.42061 time= 0.16336\n",
      "Epoch: 0162 train_loss= 0.42034 time= 0.17573\n",
      "Epoch: 0163 train_loss= 0.42009 time= 0.17354\n",
      "Epoch: 0164 train_loss= 0.41985 time= 0.17154\n",
      "Epoch: 0165 train_loss= 0.41962 time= 0.16768\n",
      "Epoch: 0166 train_loss= 0.41940 time= 0.17080\n",
      "Epoch: 0167 train_loss= 0.41919 time= 0.16844\n",
      "Epoch: 0168 train_loss= 0.41897 time= 0.17803\n",
      "Epoch: 0169 train_loss= 0.41875 time= 0.16443\n",
      "Epoch: 0170 train_loss= 0.41853 time= 0.18021\n",
      "Epoch: 0171 train_loss= 0.41831 time= 0.16891\n",
      "Epoch: 0172 train_loss= 0.41809 time= 0.16346\n",
      "Epoch: 0173 train_loss= 0.41787 time= 0.17656\n",
      "Epoch: 0174 train_loss= 0.41765 time= 0.16998\n",
      "Epoch: 0175 train_loss= 0.41745 time= 0.18289\n",
      "Epoch: 0176 train_loss= 0.41725 time= 0.16830\n",
      "Epoch: 0177 train_loss= 0.41706 time= 0.16095\n",
      "Epoch: 0178 train_loss= 0.41688 time= 0.17322\n",
      "Epoch: 0179 train_loss= 0.41671 time= 0.17151\n",
      "Epoch: 0180 train_loss= 0.41654 time= 0.17958\n",
      "Epoch: 0181 train_loss= 0.41638 time= 0.15896\n",
      "Epoch: 0182 train_loss= 0.41623 time= 0.15340\n",
      "Epoch: 0183 train_loss= 0.41608 time= 0.16593\n",
      "Epoch: 0184 train_loss= 0.41593 time= 0.16367\n",
      "Epoch: 0185 train_loss= 0.41579 time= 0.16556\n",
      "Epoch: 0186 train_loss= 0.41564 time= 0.15694\n",
      "Epoch: 0187 train_loss= 0.41549 time= 0.15999\n",
      "Epoch: 0188 train_loss= 0.41535 time= 0.18324\n",
      "Epoch: 0189 train_loss= 0.41520 time= 0.17098\n",
      "Epoch: 0190 train_loss= 0.41505 time= 0.16654\n",
      "Epoch: 0191 train_loss= 0.41489 time= 0.16883\n",
      "Epoch: 0192 train_loss= 0.41474 time= 0.16163\n",
      "Epoch: 0193 train_loss= 0.41459 time= 0.16572\n",
      "Epoch: 0194 train_loss= 0.41443 time= 0.15987\n",
      "Epoch: 0195 train_loss= 0.41427 time= 0.16424\n",
      "Epoch: 0196 train_loss= 0.41411 time= 0.17313\n",
      "Epoch: 0197 train_loss= 0.41395 time= 0.16267\n",
      "Epoch: 0198 train_loss= 0.41379 time= 0.16855\n",
      "Epoch: 0199 train_loss= 0.41362 time= 0.17033\n",
      "Epoch: 0200 train_loss= 0.41345 time= 0.16156\n",
      "Testing model...\n"
     ]
    }
   ],
   "source": [
    "# The entire training+test process is repeated FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "\n",
    "    if FLAGS.task == 'link_prediction' :\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking test edges...\")\n",
    "        # Edge Masking for Link Prediction: compute Train/Validation/Test set\n",
    "        adj, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "        mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "\n",
    "    # Start computation of running times\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Degeneracy Framework / K-Core Decomposition\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Starting k-core decomposition of the graph\")\n",
    "        # Save adjacency matrix of un-decomposed graph\n",
    "        # (needed to embed nodes that are not in k-core, after GAE training)\n",
    "        adj_orig = adj\n",
    "        # Get the (smaller) adjacency matrix of the k-core subgraph,\n",
    "        # and the corresponding nodes\n",
    "        adj, nodes_kcore = compute_kcore(adj, FLAGS.k)\n",
    "        # Get the (smaller) feature matrix of the nb_core graph\n",
    "        if FLAGS.features:\n",
    "            features = features_init[nodes_kcore,:]\n",
    "        # Flag to compute k-core decomposition's running time\n",
    "        t_core = time.time()\n",
    "    elif FLAGS.features:\n",
    "        features = features_init\n",
    "\n",
    "    # Preprocessing and initialization\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing and Initializing...\")\n",
    "    # Compute number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "    # If features are not used, replace feature matrix by identity matrix\n",
    "    if not FLAGS.features:\n",
    "        features = sp.identity(adj.shape[0])\n",
    "    # Preprocessing on node features\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ())\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = None\n",
    "    if FLAGS.model == 'gcn_ae':\n",
    "        # Standard Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # Standard Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                            features_nonzero)\n",
    "    elif FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes,\n",
    "                               features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_ae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = DeepGCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Variational Autoencoder\n",
    "        model = DeepGCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                                features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "    # Optimizer\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0]\n",
    "                                                - adj.sum()) * 2)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # Optimizer for Non-Variational Autoencoders\n",
    "        if FLAGS.model in ('gcn_ae', 'linear_ae', 'deep_gcn_ae'):\n",
    "            opt = OptimizerAE(preds = model.reconstructions,\n",
    "                              labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                            validate_indices = False), [-1]),\n",
    "                              pos_weight = pos_weight,\n",
    "                              norm = norm)\n",
    "        # Optimizer for Variational Autoencoders\n",
    "        elif FLAGS.model in ('gcn_vae', 'linear_vae', 'deep_gcn_vae'):\n",
    "            opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                               labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                             validate_indices = False), [-1]),\n",
    "                               model = model,\n",
    "                               num_nodes = num_nodes,\n",
    "                               pos_weight = pos_weight,\n",
    "                               norm = norm)\n",
    "\n",
    "    # Normalization and preprocessing on adjacency matrix\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = sparse_to_tuple(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Initialize TF session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Training...\")\n",
    "\n",
    "    for epoch in range(FLAGS.epochs):\n",
    "        # Flag to compute running time for each epoch\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_label, features,\n",
    "                                        placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.accuracy],\n",
    "                        feed_dict = feed_dict)\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display epoch information\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "            # Validation, for Link Prediction\n",
    "            if not FLAGS.kcore and FLAGS.validation and FLAGS.task == 'link_prediction':\n",
    "                feed_dict.update({placeholders['dropout']: 0})\n",
    "                emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "                feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "                val_roc, val_ap = get_roc_score(val_edges, val_edges_false, emb)\n",
    "                print(\"val_roc=\", \"{:.5f}\".format(val_roc), \"val_ap=\", \"{:.5f}\".format(val_ap))\n",
    "\n",
    "    # Flag to compute Graph AE/VAE training time\n",
    "    t_model = time.time()\n",
    "\n",
    "    # Compute embedding\n",
    "\n",
    "    # Get embedding from model\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "\n",
    "    # If k-core is used, only part of the nodes from the original\n",
    "    # graph are embedded. The remaining ones are projected in the\n",
    "    # latent space via the expand_embedding heuristic\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Propagation to remaining nodes...\")\n",
    "        # Project remaining nodes in latent space\n",
    "        emb = expand_embedding(adj_orig, emb, nodes_kcore, FLAGS.nb_iterations)\n",
    "        # Compute mean running times for K-Core, GAE Train and Propagation steps\n",
    "        mean_time_expand.append(time.time() - t_model)\n",
    "        mean_time_train.append(t_model - t_core)\n",
    "        mean_time_kcore.append(t_core - t_start)\n",
    "        # Compute mean size of K-Core graph\n",
    "        # Note: size is fixed if task is node clustering, but will vary if\n",
    "        # task is link prediction due to edge masking\n",
    "        mean_core_size.append(len(nodes_kcore))\n",
    "\n",
    "    # Compute mean total running time\n",
    "    mean_time.append(time.time() - t_start)\n",
    "\n",
    "    # Test model\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing model...\")\n",
    "    # Link Prediction: classification edges/non-edges\n",
    "    if FLAGS.task == 'link_prediction':\n",
    "        # Get ROC and AP scores\n",
    "        ap_score, roc_score, acc_score, f1_score = get_roc_score(test_edges, test_edges_false, emb)\n",
    "        # Report scores\n",
    "        mean_ap.append(ap_score)\n",
    "        mean_roc.append(roc_score)\n",
    "        mean_acc.append(acc_score)\n",
    "        mean_f1.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29cfe35c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T13:15:10.786202Z",
     "start_time": "2022-05-04T13:15:10.773237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results for gcn_ae model on power on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AP scores\n",
      " [0.9333834164583548, 0.9248066887832662, 0.9398935085906264, 0.9319432577246792, 0.9296344382520424, 0.9331233971140167, 0.9284450054033184, 0.9328227795145767, 0.9356651873439025, 0.9410411466179655]\n",
      "Mean AP score:  0.933075882580275 \n",
      "Std of AP scores:  0.004687662725212013 \n",
      " \n",
      "\n",
      "AUC scores\n",
      " [0.9130394161757667, 0.8990959449288718, 0.9170270405075336, 0.9058934802174674, 0.9068043097460621, 0.9109635288521165, 0.9083360071347691, 0.910599494819029, 0.9101245383505014, 0.9183015318462778]\n",
      "Mean AUC score:  0.9100185292578397 \n",
      "Std of AUC scores:  0.005268546939201213 \n",
      " \n",
      "\n",
      "ACC scores\n",
      " [0.7424503882657464, 0.7333908541846419, 0.7489214840379638, 0.7402933563416738, 0.734253666954271, 0.722174288179465, 0.7303710094909405, 0.7450388265746333, 0.7364106988783434, 0.7277825711820535]\n",
      "Mean ACC score:  0.7361087144089731 \n",
      "Std of ACC scores:  0.007803687187814445 \n",
      " \n",
      "\n",
      "F1 scores\n",
      " [0.7833030852994556, 0.7736263736263735, 0.7874360847333821, 0.7780235988200591, 0.7753464624361779, 0.7696709585121603, 0.7742867461177321, 0.7826406767193822, 0.7775755369493993, 0.7726126126126126]\n",
      "Mean F1 score:  0.7774522135826735 \n",
      "Std of F1 scores:  0.005236729074709267 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Report Final Results ######\n",
    "\n",
    "# Report final results\n",
    "print(\"\\nTest results for\", FLAGS.model,\n",
    "      \"model on\", FLAGS.dataset, \"on\", FLAGS.task, \"\\n\",\n",
    "      \"___________________________________________________\\n\")\n",
    "\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    print(\"AP scores\\n\", mean_ap)\n",
    "    print(\"Mean AP score: \", np.mean(mean_ap),\n",
    "          \"\\nStd of AP scores: \", np.std(mean_ap), \"\\n \\n\")\n",
    "    \n",
    "    print(\"AUC scores\\n\", mean_roc)\n",
    "    print(\"Mean AUC score: \", np.mean(mean_roc),\n",
    "          \"\\nStd of AUC scores: \", np.std(mean_roc), \"\\n \\n\")\n",
    "    \n",
    "    print(\"ACC scores\\n\", mean_acc)\n",
    "    print(\"Mean ACC score: \", np.mean(mean_acc),\n",
    "          \"\\nStd of ACC scores: \", np.std(mean_acc), \"\\n \\n\")\n",
    "    \n",
    "    print(\"F1 scores\\n\", mean_f1)\n",
    "    print(\"Mean F1 score: \", np.mean(mean_f1),\n",
    "          \"\\nStd of F1 scores: \", np.std(mean_f1), \"\\n \\n\")\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    print(\"Details on degeneracy framework, with k =\", FLAGS.k, \": \\n \\n\")\n",
    "\n",
    "    print(\"Running times for k-core decomposition\\n\", mean_time_kcore)\n",
    "    print(\"Mean: \", np.mean(mean_time_kcore),\n",
    "          \"\\nStd: \", np.std(mean_time_kcore), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for autoencoder training\\n\", mean_time_train)\n",
    "    print(\"Mean: \", np.mean(mean_time_train),\n",
    "          \"\\nStd: \", np.std(mean_time_train), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for propagation\\n\", mean_time_expand)\n",
    "    print(\"Mean: \", np.mean(mean_time_expand),\n",
    "          \"\\nStd: \", np.std(mean_time_expand), \"\\n \\n\")\n",
    "\n",
    "    print(\"Sizes of k-core subgraph\\n\", mean_core_size)\n",
    "    print(\"Mean: \", np.mean(mean_core_size),\n",
    "          \"\\nStd: \", np.std(mean_core_size), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc25d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
