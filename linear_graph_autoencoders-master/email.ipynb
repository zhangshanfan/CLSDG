{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c935237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:08:23.752715Z",
     "start_time": "2022-05-08T12:08:17.019118Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from linear_gae.evaluation import get_roc_score, clustering_latent_space\n",
    "from linear_gae.kcore import compute_kcore, expand_embedding\n",
    "from linear_gae.model import *\n",
    "from linear_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from linear_gae.preprocessing import *\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384cb05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:08:25.110676Z",
     "start_time": "2022-05-08T12:08:25.092729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x2402f9ebbe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')   # 添加的，不报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19800c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:08:25.803398Z",
     "start_time": "2022-05-08T12:08:25.794667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Available Models:\\n\\n- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\\n          GCN encoder and inner product decoder\\n\\n- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\\n           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\\n           product decoder\\n\\n- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\\n             workshop paper, with linear encoder, and inner product decoder\\n\\n- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\\n              of NeurIPS workshop paper, with Gaussian priors, linear encoders\\n              for mu and sigma, and inner product decoder\\n \\n- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\\n               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\\n               product decoder\\n \\n- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\\n                in section 4 of NeurIPS workshop paper, with Gaussian priors,\\n                3-layer GCN encoders for mu and sigma, and inner product\\n                decoder\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select graph dataset\n",
    "flags.DEFINE_string('dataset', 'email', 'Name of the graph dataset')\n",
    "\n",
    "# Select machine learning task to perform on graph\n",
    "flags.DEFINE_string('task', 'link_prediction', 'Name of the learning task')\n",
    "\n",
    "# Model\n",
    "flags.DEFINE_string('model', 'gcn_ae', 'Name of the model')\n",
    "''' Available Models:\n",
    "\n",
    "- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\n",
    "          GCN encoder and inner product decoder\n",
    "\n",
    "- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\n",
    "           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\n",
    "           product decoder\n",
    "\n",
    "- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\n",
    "             workshop paper, with linear encoder, and inner product decoder\n",
    "\n",
    "- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\n",
    "              of NeurIPS workshop paper, with Gaussian priors, linear encoders\n",
    "              for mu and sigma, and inner product decoder\n",
    " \n",
    "- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\n",
    "               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\n",
    "               product decoder\n",
    " \n",
    "- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\n",
    "                in section 4 of NeurIPS workshop paper, with Gaussian priors,\n",
    "                3-layer GCN encoders for mu and sigma, and inner product\n",
    "                decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eeddc89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:08:26.494633Z",
     "start_time": "2022-05-08T12:08:26.481373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs in training.')\n",
    "flags.DEFINE_boolean('features', False, 'Include node features or not in encoder')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (with Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Number of units in GCN hidden layer(s).')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of encoder output, i.e. \\\n",
    "                                       embedding dimension')\n",
    "\n",
    "# Experimental setup parameters\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 5., 'Proportion of edges in validation set \\\n",
    "                                   (for Link Prediction task)')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      (for Link Prediction task)')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to report validation \\\n",
    "                                           results at each epoch (for \\\n",
    "                                           Link Prediction task)')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print comments details.')\n",
    "\n",
    "# Parameters related to the \"degeneracy framework\" from IJCAI 2019 paper,\n",
    "# aiming at scaling-up Graph AE/VAE by training the model only on the k-core\n",
    "# (smaller) version of the graph, then expanding embedding to remaining nodes\n",
    "# via simpler and faster heuristics\n",
    "flags.DEFINE_boolean('kcore', False, 'Whether to run k-core decomposition \\\n",
    "                                      and use the framework. False = model \\\n",
    "                                      will be trained on the entire graph')\n",
    "flags.DEFINE_integer('k', 2, 'Which k-core to use. Higher k => smaller graphs\\\n",
    "                              and faster (but maybe less accurate) training')\n",
    "flags.DEFINE_integer('nb_iterations', 10, 'Number of fix point iterations in \\\n",
    "                                           algorithm 2 of IJCAI paper. See \\\n",
    "                                           kcore.py file for details')\n",
    "\n",
    "# Lists to collect average results\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    mean_roc = []\n",
    "    mean_ap = []\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    mean_time_kcore = []\n",
    "    mean_time_train = []\n",
    "    mean_time_expand = []\n",
    "    mean_core_size = []\n",
    "mean_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8eb1b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:08:29.071636Z",
     "start_time": "2022-05-08T12:08:29.059667Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename_adj, nodes_numbers):\n",
    "    raw_edges = pd.read_csv(filename_adj,header=None,sep=' ') - 1\n",
    "    \n",
    "    drop_self_loop = raw_edges[raw_edges[0] != raw_edges[1]]\n",
    "    \n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    \n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "        \n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    \n",
    "    features = sp.lil_matrix(np.eye(nodes_numbers))\n",
    "    \n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa41c5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:08:30.439094Z",
     "start_time": "2022-05-08T12:08:29.796310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load graph dataset\n",
    "if FLAGS.verbose:\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "adj_name = 'datasets/ia-email-univ.mtx'\n",
    "nodes_number = 1133\n",
    "adj_init, features_init = load_data(adj_name, nodes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386820fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:09:59.668070Z",
     "start_time": "2022-05-08T12:08:31.169931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73512 time= 0.16733\n",
      "Epoch: 0002 train_loss= 0.73476 time= 0.03391\n",
      "Epoch: 0003 train_loss= 0.73374 time= 0.03591\n",
      "Epoch: 0004 train_loss= 0.73160 time= 0.03491\n",
      "Epoch: 0005 train_loss= 0.72782 time= 0.03790\n",
      "Epoch: 0006 train_loss= 0.72184 time= 0.04720\n",
      "Epoch: 0007 train_loss= 0.71325 time= 0.04441\n",
      "Epoch: 0008 train_loss= 0.70209 time= 0.04683\n",
      "Epoch: 0009 train_loss= 0.68947 time= 0.04508\n",
      "Epoch: 0010 train_loss= 0.67859 time= 0.04564\n",
      "Epoch: 0011 train_loss= 0.67481 time= 0.03585\n",
      "Epoch: 0012 train_loss= 0.67840 time= 0.03690\n",
      "Epoch: 0013 train_loss= 0.67786 time= 0.03499\n",
      "Epoch: 0014 train_loss= 0.67105 time= 0.04538\n",
      "Epoch: 0015 train_loss= 0.66286 time= 0.04484\n",
      "Epoch: 0016 train_loss= 0.65680 time= 0.03869\n",
      "Epoch: 0017 train_loss= 0.65335 time= 0.03732\n",
      "Epoch: 0018 train_loss= 0.65110 time= 0.03628\n",
      "Epoch: 0019 train_loss= 0.64847 time= 0.03752\n",
      "Epoch: 0020 train_loss= 0.64449 time= 0.04049\n",
      "Epoch: 0021 train_loss= 0.63893 time= 0.04292\n",
      "Epoch: 0022 train_loss= 0.63205 time= 0.04015\n",
      "Epoch: 0023 train_loss= 0.62437 time= 0.03777\n",
      "Epoch: 0024 train_loss= 0.61647 time= 0.03477\n",
      "Epoch: 0025 train_loss= 0.60866 time= 0.03268\n",
      "Epoch: 0026 train_loss= 0.60079 time= 0.04013\n",
      "Epoch: 0027 train_loss= 0.59227 time= 0.04432\n",
      "Epoch: 0028 train_loss= 0.58260 time= 0.04078\n",
      "Epoch: 0029 train_loss= 0.57181 time= 0.03804\n",
      "Epoch: 0030 train_loss= 0.56054 time= 0.03591\n",
      "Epoch: 0031 train_loss= 0.54978 time= 0.03427\n",
      "Epoch: 0032 train_loss= 0.54039 time= 0.03466\n",
      "Epoch: 0033 train_loss= 0.53302 time= 0.04501\n",
      "Epoch: 0034 train_loss= 0.52795 time= 0.03998\n",
      "Epoch: 0035 train_loss= 0.52512 time= 0.03691\n",
      "Epoch: 0036 train_loss= 0.52411 time= 0.03727\n",
      "Epoch: 0037 train_loss= 0.52426 time= 0.03693\n",
      "Epoch: 0038 train_loss= 0.52485 time= 0.03498\n",
      "Epoch: 0039 train_loss= 0.52525 time= 0.04129\n",
      "Epoch: 0040 train_loss= 0.52500 time= 0.03989\n",
      "Epoch: 0041 train_loss= 0.52389 time= 0.04034\n",
      "Epoch: 0042 train_loss= 0.52191 time= 0.03409\n",
      "Epoch: 0043 train_loss= 0.51937 time= 0.03625\n",
      "Epoch: 0044 train_loss= 0.51677 time= 0.03720\n",
      "Epoch: 0045 train_loss= 0.51452 time= 0.03990\n",
      "Epoch: 0046 train_loss= 0.51280 time= 0.04385\n",
      "Epoch: 0047 train_loss= 0.51150 time= 0.05835\n",
      "Epoch: 0048 train_loss= 0.51044 time= 0.04205\n",
      "Epoch: 0049 train_loss= 0.50949 time= 0.03590\n",
      "Epoch: 0050 train_loss= 0.50863 time= 0.03645\n",
      "Epoch: 0051 train_loss= 0.50788 time= 0.03815\n",
      "Epoch: 0052 train_loss= 0.50718 time= 0.03618\n",
      "Epoch: 0053 train_loss= 0.50644 time= 0.04305\n",
      "Epoch: 0054 train_loss= 0.50560 time= 0.04248\n",
      "Epoch: 0055 train_loss= 0.50465 time= 0.04820\n",
      "Epoch: 0056 train_loss= 0.50365 time= 0.04617\n",
      "Epoch: 0057 train_loss= 0.50270 time= 0.04815\n",
      "Epoch: 0058 train_loss= 0.50185 time= 0.03810\n",
      "Epoch: 0059 train_loss= 0.50113 time= 0.03505\n",
      "Epoch: 0060 train_loss= 0.50055 time= 0.03502\n",
      "Epoch: 0061 train_loss= 0.50006 time= 0.03768\n",
      "Epoch: 0062 train_loss= 0.49963 time= 0.04071\n",
      "Epoch: 0063 train_loss= 0.49920 time= 0.04469\n",
      "Epoch: 0064 train_loss= 0.49872 time= 0.04596\n",
      "Epoch: 0065 train_loss= 0.49815 time= 0.04276\n",
      "Epoch: 0066 train_loss= 0.49750 time= 0.04089\n",
      "Epoch: 0067 train_loss= 0.49678 time= 0.03881\n",
      "Epoch: 0068 train_loss= 0.49601 time= 0.03492\n",
      "Epoch: 0069 train_loss= 0.49521 time= 0.03835\n",
      "Epoch: 0070 train_loss= 0.49441 time= 0.03855\n",
      "Epoch: 0071 train_loss= 0.49361 time= 0.04035\n",
      "Epoch: 0072 train_loss= 0.49283 time= 0.04238\n",
      "Epoch: 0073 train_loss= 0.49206 time= 0.04980\n",
      "Epoch: 0074 train_loss= 0.49129 time= 0.04508\n",
      "Epoch: 0075 train_loss= 0.49047 time= 0.03902\n",
      "Epoch: 0076 train_loss= 0.48961 time= 0.03970\n",
      "Epoch: 0077 train_loss= 0.48869 time= 0.04101\n",
      "Epoch: 0078 train_loss= 0.48773 time= 0.03931\n",
      "Epoch: 0079 train_loss= 0.48675 time= 0.03636\n",
      "Epoch: 0080 train_loss= 0.48575 time= 0.04621\n",
      "Epoch: 0081 train_loss= 0.48476 time= 0.04178\n",
      "Epoch: 0082 train_loss= 0.48379 time= 0.04582\n",
      "Epoch: 0083 train_loss= 0.48286 time= 0.04240\n",
      "Epoch: 0084 train_loss= 0.48198 time= 0.04000\n",
      "Epoch: 0085 train_loss= 0.48115 time= 0.03690\n",
      "Epoch: 0086 train_loss= 0.48037 time= 0.03735\n",
      "Epoch: 0087 train_loss= 0.47964 time= 0.03703\n",
      "Epoch: 0088 train_loss= 0.47897 time= 0.04089\n",
      "Epoch: 0089 train_loss= 0.47832 time= 0.04514\n",
      "Epoch: 0090 train_loss= 0.47767 time= 0.04163\n",
      "Epoch: 0091 train_loss= 0.47700 time= 0.03590\n",
      "Epoch: 0092 train_loss= 0.47632 time= 0.03690\n",
      "Epoch: 0093 train_loss= 0.47561 time= 0.03690\n",
      "Epoch: 0094 train_loss= 0.47485 time= 0.03903\n",
      "Epoch: 0095 train_loss= 0.47406 time= 0.04171\n",
      "Epoch: 0096 train_loss= 0.47324 time= 0.04645\n",
      "Epoch: 0097 train_loss= 0.47241 time= 0.04488\n",
      "Epoch: 0098 train_loss= 0.47159 time= 0.03591\n",
      "Epoch: 0099 train_loss= 0.47080 time= 0.03609\n",
      "Epoch: 0100 train_loss= 0.47006 time= 0.03541\n",
      "Epoch: 0101 train_loss= 0.46938 time= 0.03456\n",
      "Epoch: 0102 train_loss= 0.46877 time= 0.04089\n",
      "Epoch: 0103 train_loss= 0.46823 time= 0.04686\n",
      "Epoch: 0104 train_loss= 0.46776 time= 0.04012\n",
      "Epoch: 0105 train_loss= 0.46733 time= 0.03368\n",
      "Epoch: 0106 train_loss= 0.46693 time= 0.03590\n",
      "Epoch: 0107 train_loss= 0.46653 time= 0.03577\n",
      "Epoch: 0108 train_loss= 0.46612 time= 0.03890\n",
      "Epoch: 0109 train_loss= 0.46568 time= 0.05117\n",
      "Epoch: 0110 train_loss= 0.46520 time= 0.03989\n",
      "Epoch: 0111 train_loss= 0.46468 time= 0.03832\n",
      "Epoch: 0112 train_loss= 0.46411 time= 0.03490\n",
      "Epoch: 0113 train_loss= 0.46350 time= 0.03689\n",
      "Epoch: 0114 train_loss= 0.46286 time= 0.03468\n",
      "Epoch: 0115 train_loss= 0.46221 time= 0.04089\n",
      "Epoch: 0116 train_loss= 0.46156 time= 0.04288\n",
      "Epoch: 0117 train_loss= 0.46091 time= 0.04488\n",
      "Epoch: 0118 train_loss= 0.46025 time= 0.03679\n",
      "Epoch: 0119 train_loss= 0.45960 time= 0.03751\n",
      "Epoch: 0120 train_loss= 0.45894 time= 0.03491\n",
      "Epoch: 0121 train_loss= 0.45828 time= 0.04285\n",
      "Epoch: 0122 train_loss= 0.45760 time= 0.04687\n",
      "Epoch: 0123 train_loss= 0.45690 time= 0.04987\n",
      "Epoch: 0124 train_loss= 0.45617 time= 0.04089\n",
      "Epoch: 0125 train_loss= 0.45543 time= 0.04751\n",
      "Epoch: 0126 train_loss= 0.45466 time= 0.03885\n",
      "Epoch: 0127 train_loss= 0.45387 time= 0.03495\n",
      "Epoch: 0128 train_loss= 0.45308 time= 0.03989\n",
      "Epoch: 0129 train_loss= 0.45228 time= 0.03491\n",
      "Epoch: 0130 train_loss= 0.45149 time= 0.04189\n",
      "Epoch: 0131 train_loss= 0.45072 time= 0.04289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0132 train_loss= 0.44998 time= 0.05286\n",
      "Epoch: 0133 train_loss= 0.44927 time= 0.04288\n",
      "Epoch: 0134 train_loss= 0.44861 time= 0.04089\n",
      "Epoch: 0135 train_loss= 0.44799 time= 0.03483\n",
      "Epoch: 0136 train_loss= 0.44742 time= 0.03771\n",
      "Epoch: 0137 train_loss= 0.44688 time= 0.03582\n",
      "Epoch: 0138 train_loss= 0.44638 time= 0.03837\n",
      "Epoch: 0139 train_loss= 0.44591 time= 0.04453\n",
      "Epoch: 0140 train_loss= 0.44546 time= 0.04708\n",
      "Epoch: 0141 train_loss= 0.44503 time= 0.04047\n",
      "Epoch: 0142 train_loss= 0.44460 time= 0.04189\n",
      "Epoch: 0143 train_loss= 0.44418 time= 0.03812\n",
      "Epoch: 0144 train_loss= 0.44376 time= 0.03860\n",
      "Epoch: 0145 train_loss= 0.44336 time= 0.03590\n",
      "Epoch: 0146 train_loss= 0.44298 time= 0.03794\n",
      "Epoch: 0147 train_loss= 0.44263 time= 0.04284\n",
      "Epoch: 0148 train_loss= 0.44230 time= 0.04487\n",
      "Epoch: 0149 train_loss= 0.44200 time= 0.05386\n",
      "Epoch: 0150 train_loss= 0.44173 time= 0.04238\n",
      "Epoch: 0151 train_loss= 0.44148 time= 0.04488\n",
      "Epoch: 0152 train_loss= 0.44124 time= 0.03772\n",
      "Epoch: 0153 train_loss= 0.44101 time= 0.03600\n",
      "Epoch: 0154 train_loss= 0.44077 time= 0.03753\n",
      "Epoch: 0155 train_loss= 0.44052 time= 0.04157\n",
      "Epoch: 0156 train_loss= 0.44027 time= 0.04767\n",
      "Epoch: 0157 train_loss= 0.44001 time= 0.04875\n",
      "Epoch: 0158 train_loss= 0.43975 time= 0.04786\n",
      "Epoch: 0159 train_loss= 0.43948 time= 0.04144\n",
      "Epoch: 0160 train_loss= 0.43923 time= 0.04160\n",
      "Epoch: 0161 train_loss= 0.43897 time= 0.03811\n",
      "Epoch: 0162 train_loss= 0.43872 time= 0.03603\n",
      "Epoch: 0163 train_loss= 0.43848 time= 0.03807\n",
      "Epoch: 0164 train_loss= 0.43825 time= 0.03563\n",
      "Epoch: 0165 train_loss= 0.43801 time= 0.04850\n",
      "Epoch: 0166 train_loss= 0.43778 time= 0.04433\n",
      "Epoch: 0167 train_loss= 0.43755 time= 0.03829\n",
      "Epoch: 0168 train_loss= 0.43732 time= 0.03700\n",
      "Epoch: 0169 train_loss= 0.43709 time= 0.03577\n",
      "Epoch: 0170 train_loss= 0.43686 time= 0.03590\n",
      "Epoch: 0171 train_loss= 0.43662 time= 0.03848\n",
      "Epoch: 0172 train_loss= 0.43638 time= 0.04630\n",
      "Epoch: 0173 train_loss= 0.43613 time= 0.03876\n",
      "Epoch: 0174 train_loss= 0.43589 time= 0.03655\n",
      "Epoch: 0175 train_loss= 0.43564 time= 0.03525\n",
      "Epoch: 0176 train_loss= 0.43539 time= 0.03967\n",
      "Epoch: 0177 train_loss= 0.43515 time= 0.04463\n",
      "Epoch: 0178 train_loss= 0.43490 time= 0.04718\n",
      "Epoch: 0179 train_loss= 0.43466 time= 0.04059\n",
      "Epoch: 0180 train_loss= 0.43442 time= 0.03524\n",
      "Epoch: 0181 train_loss= 0.43418 time= 0.03805\n",
      "Epoch: 0182 train_loss= 0.43395 time= 0.03489\n",
      "Epoch: 0183 train_loss= 0.43372 time= 0.03581\n",
      "Epoch: 0184 train_loss= 0.43351 time= 0.04167\n",
      "Epoch: 0185 train_loss= 0.43330 time= 0.04196\n",
      "Epoch: 0186 train_loss= 0.43310 time= 0.03704\n",
      "Epoch: 0187 train_loss= 0.43291 time= 0.03512\n",
      "Epoch: 0188 train_loss= 0.43272 time= 0.03684\n",
      "Epoch: 0189 train_loss= 0.43255 time= 0.04109\n",
      "Epoch: 0190 train_loss= 0.43237 time= 0.04195\n",
      "Epoch: 0191 train_loss= 0.43220 time= 0.04045\n",
      "Epoch: 0192 train_loss= 0.43203 time= 0.03890\n",
      "Epoch: 0193 train_loss= 0.43186 time= 0.03536\n",
      "Epoch: 0194 train_loss= 0.43169 time= 0.03452\n",
      "Epoch: 0195 train_loss= 0.43152 time= 0.03790\n",
      "Epoch: 0196 train_loss= 0.43134 time= 0.03590\n",
      "Epoch: 0197 train_loss= 0.43116 time= 0.04355\n",
      "Epoch: 0198 train_loss= 0.43098 time= 0.04189\n",
      "Epoch: 0199 train_loss= 0.43079 time= 0.04488\n",
      "Epoch: 0200 train_loss= 0.43060 time= 0.04588\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73508 time= 0.14342\n",
      "Epoch: 0002 train_loss= 0.73431 time= 0.03566\n",
      "Epoch: 0003 train_loss= 0.73213 time= 0.03491\n",
      "Epoch: 0004 train_loss= 0.72766 time= 0.03590\n",
      "Epoch: 0005 train_loss= 0.72010 time= 0.03590\n",
      "Epoch: 0006 train_loss= 0.70909 time= 0.04090\n",
      "Epoch: 0007 train_loss= 0.69543 time= 0.04202\n",
      "Epoch: 0008 train_loss= 0.68235 time= 0.04406\n",
      "Epoch: 0009 train_loss= 0.67632 time= 0.04558\n",
      "Epoch: 0010 train_loss= 0.68037 time= 0.04987\n",
      "Epoch: 0011 train_loss= 0.68080 time= 0.03989\n",
      "Epoch: 0012 train_loss= 0.67378 time= 0.03989\n",
      "Epoch: 0013 train_loss= 0.66550 time= 0.03790\n",
      "Epoch: 0014 train_loss= 0.66013 time= 0.03491\n",
      "Epoch: 0015 train_loss= 0.65787 time= 0.04688\n",
      "Epoch: 0016 train_loss= 0.65684 time= 0.04388\n",
      "Epoch: 0017 train_loss= 0.65527 time= 0.03698\n",
      "Epoch: 0018 train_loss= 0.65237 time= 0.03635\n",
      "Epoch: 0019 train_loss= 0.64817 time= 0.03837\n",
      "Epoch: 0020 train_loss= 0.64326 time= 0.03690\n",
      "Epoch: 0021 train_loss= 0.63847 time= 0.04588\n",
      "Epoch: 0022 train_loss= 0.63455 time= 0.04188\n",
      "Epoch: 0023 train_loss= 0.63165 time= 0.03989\n",
      "Epoch: 0024 train_loss= 0.62910 time= 0.03790\n",
      "Epoch: 0025 train_loss= 0.62598 time= 0.03538\n",
      "Epoch: 0026 train_loss= 0.62209 time= 0.03464\n",
      "Epoch: 0027 train_loss= 0.61819 time= 0.04068\n",
      "Epoch: 0028 train_loss= 0.61530 time= 0.04089\n",
      "Epoch: 0029 train_loss= 0.61382 time= 0.03790\n",
      "Epoch: 0030 train_loss= 0.61318 time= 0.03737\n",
      "Epoch: 0031 train_loss= 0.61260 time= 0.03816\n",
      "Epoch: 0032 train_loss= 0.61184 time= 0.03365\n",
      "Epoch: 0033 train_loss= 0.61100 time= 0.04289\n",
      "Epoch: 0034 train_loss= 0.61022 time= 0.04458\n",
      "Epoch: 0035 train_loss= 0.60936 time= 0.04021\n",
      "Epoch: 0036 train_loss= 0.60804 time= 0.03762\n",
      "Epoch: 0037 train_loss= 0.60599 time= 0.03885\n",
      "Epoch: 0038 train_loss= 0.60338 time= 0.03595\n",
      "Epoch: 0039 train_loss= 0.60064 time= 0.03890\n",
      "Epoch: 0040 train_loss= 0.59807 time= 0.04851\n",
      "Epoch: 0041 train_loss= 0.59568 time= 0.04433\n",
      "Epoch: 0042 train_loss= 0.59326 time= 0.03617\n",
      "Epoch: 0043 train_loss= 0.59061 time= 0.03572\n",
      "Epoch: 0044 train_loss= 0.58767 time= 0.03690\n",
      "Epoch: 0045 train_loss= 0.58450 time= 0.03521\n",
      "Epoch: 0046 train_loss= 0.58118 time= 0.04987\n",
      "Epoch: 0047 train_loss= 0.57771 time= 0.04398\n",
      "Epoch: 0048 train_loss= 0.57410 time= 0.04550\n",
      "Epoch: 0049 train_loss= 0.57034 time= 0.04350\n",
      "Epoch: 0050 train_loss= 0.56653 time= 0.03899\n",
      "Epoch: 0051 train_loss= 0.56280 time= 0.04097\n",
      "Epoch: 0052 train_loss= 0.55926 time= 0.03937\n",
      "Epoch: 0053 train_loss= 0.55591 time= 0.03533\n",
      "Epoch: 0054 train_loss= 0.55268 time= 0.03968\n",
      "Epoch: 0055 train_loss= 0.54954 time= 0.04507\n",
      "Epoch: 0056 train_loss= 0.54652 time= 0.04809\n",
      "Epoch: 0057 train_loss= 0.54368 time= 0.04163\n",
      "Epoch: 0058 train_loss= 0.54099 time= 0.04789\n",
      "Epoch: 0059 train_loss= 0.53840 time= 0.03502\n",
      "Epoch: 0060 train_loss= 0.53588 time= 0.04024\n",
      "Epoch: 0061 train_loss= 0.53343 time= 0.03633\n",
      "Epoch: 0062 train_loss= 0.53110 time= 0.03635\n",
      "Epoch: 0063 train_loss= 0.52888 time= 0.04163\n",
      "Epoch: 0064 train_loss= 0.52673 time= 0.04480\n",
      "Epoch: 0065 train_loss= 0.52453 time= 0.04512\n",
      "Epoch: 0066 train_loss= 0.52219 time= 0.04203\n",
      "Epoch: 0067 train_loss= 0.51970 time= 0.04241\n",
      "Epoch: 0068 train_loss= 0.51715 time= 0.03591\n",
      "Epoch: 0069 train_loss= 0.51469 time= 0.03738\n",
      "Epoch: 0070 train_loss= 0.51247 time= 0.03643\n",
      "Epoch: 0071 train_loss= 0.51063 time= 0.03793\n",
      "Epoch: 0072 train_loss= 0.50918 time= 0.04488\n",
      "Epoch: 0073 train_loss= 0.50809 time= 0.04588\n",
      "Epoch: 0074 train_loss= 0.50726 time= 0.05098\n",
      "Epoch: 0075 train_loss= 0.50665 time= 0.04394\n",
      "Epoch: 0076 train_loss= 0.50619 time= 0.04388\n",
      "Epoch: 0077 train_loss= 0.50575 time= 0.03790\n",
      "Epoch: 0078 train_loss= 0.50515 time= 0.03690\n",
      "Epoch: 0079 train_loss= 0.50427 time= 0.03796\n",
      "Epoch: 0080 train_loss= 0.50308 time= 0.03883\n",
      "Epoch: 0081 train_loss= 0.50169 time= 0.04588\n",
      "Epoch: 0082 train_loss= 0.50025 time= 0.04705\n",
      "Epoch: 0083 train_loss= 0.49887 time= 0.04768\n",
      "Epoch: 0084 train_loss= 0.49760 time= 0.04576\n",
      "Epoch: 0085 train_loss= 0.49644 time= 0.04836\n",
      "Epoch: 0086 train_loss= 0.49536 time= 0.03916\n",
      "Epoch: 0087 train_loss= 0.49429 time= 0.03897\n",
      "Epoch: 0088 train_loss= 0.49318 time= 0.03600\n",
      "Epoch: 0089 train_loss= 0.49204 time= 0.03890\n",
      "Epoch: 0090 train_loss= 0.49085 time= 0.04608\n",
      "Epoch: 0091 train_loss= 0.48961 time= 0.04632\n",
      "Epoch: 0092 train_loss= 0.48830 time= 0.03733\n",
      "Epoch: 0093 train_loss= 0.48694 time= 0.03653\n",
      "Epoch: 0094 train_loss= 0.48556 time= 0.03616\n",
      "Epoch: 0095 train_loss= 0.48420 time= 0.03842\n",
      "Epoch: 0096 train_loss= 0.48290 time= 0.04389\n",
      "Epoch: 0097 train_loss= 0.48169 time= 0.04376\n",
      "Epoch: 0098 train_loss= 0.48058 time= 0.03957\n",
      "Epoch: 0099 train_loss= 0.47955 time= 0.03470\n",
      "Epoch: 0100 train_loss= 0.47861 time= 0.03589\n",
      "Epoch: 0101 train_loss= 0.47774 time= 0.03705\n",
      "Epoch: 0102 train_loss= 0.47691 time= 0.04504\n",
      "Epoch: 0103 train_loss= 0.47612 time= 0.04079\n",
      "Epoch: 0104 train_loss= 0.47535 time= 0.03628\n",
      "Epoch: 0105 train_loss= 0.47459 time= 0.03562\n",
      "Epoch: 0106 train_loss= 0.47384 time= 0.03436\n",
      "Epoch: 0107 train_loss= 0.47312 time= 0.03711\n",
      "Epoch: 0108 train_loss= 0.47244 time= 0.04006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0109 train_loss= 0.47181 time= 0.05023\n",
      "Epoch: 0110 train_loss= 0.47122 time= 0.03894\n",
      "Epoch: 0111 train_loss= 0.47067 time= 0.03743\n",
      "Epoch: 0112 train_loss= 0.47014 time= 0.03631\n",
      "Epoch: 0113 train_loss= 0.46963 time= 0.03565\n",
      "Epoch: 0114 train_loss= 0.46911 time= 0.04161\n",
      "Epoch: 0115 train_loss= 0.46857 time= 0.04444\n",
      "Epoch: 0116 train_loss= 0.46800 time= 0.04149\n",
      "Epoch: 0117 train_loss= 0.46740 time= 0.03677\n",
      "Epoch: 0118 train_loss= 0.46678 time= 0.03594\n",
      "Epoch: 0119 train_loss= 0.46614 time= 0.03496\n",
      "Epoch: 0120 train_loss= 0.46550 time= 0.03491\n",
      "Epoch: 0121 train_loss= 0.46486 time= 0.04588\n",
      "Epoch: 0122 train_loss= 0.46423 time= 0.04787\n",
      "Epoch: 0123 train_loss= 0.46360 time= 0.04588\n",
      "Epoch: 0124 train_loss= 0.46299 time= 0.04488\n",
      "Epoch: 0125 train_loss= 0.46238 time= 0.04089\n",
      "Epoch: 0126 train_loss= 0.46178 time= 0.03790\n",
      "Epoch: 0127 train_loss= 0.46119 time= 0.03666\n",
      "Epoch: 0128 train_loss= 0.46061 time= 0.03491\n",
      "Epoch: 0129 train_loss= 0.46004 time= 0.03990\n",
      "Epoch: 0130 train_loss= 0.45949 time= 0.04189\n",
      "Epoch: 0131 train_loss= 0.45897 time= 0.04688\n",
      "Epoch: 0132 train_loss= 0.45848 time= 0.05485\n",
      "Epoch: 0133 train_loss= 0.45803 time= 0.04612\n",
      "Epoch: 0134 train_loss= 0.45761 time= 0.03590\n",
      "Epoch: 0135 train_loss= 0.45724 time= 0.03808\n",
      "Epoch: 0136 train_loss= 0.45690 time= 0.03689\n",
      "Epoch: 0137 train_loss= 0.45659 time= 0.03909\n",
      "Epoch: 0138 train_loss= 0.45629 time= 0.04488\n",
      "Epoch: 0139 train_loss= 0.45600 time= 0.04388\n",
      "Epoch: 0140 train_loss= 0.45571 time= 0.05244\n",
      "Epoch: 0141 train_loss= 0.45540 time= 0.04379\n",
      "Epoch: 0142 train_loss= 0.45507 time= 0.04189\n",
      "Epoch: 0143 train_loss= 0.45472 time= 0.03944\n",
      "Epoch: 0144 train_loss= 0.45436 time= 0.03875\n",
      "Epoch: 0145 train_loss= 0.45399 time= 0.03590\n",
      "Epoch: 0146 train_loss= 0.45361 time= 0.03667\n",
      "Epoch: 0147 train_loss= 0.45323 time= 0.04787\n",
      "Epoch: 0148 train_loss= 0.45285 time= 0.04520\n",
      "Epoch: 0149 train_loss= 0.45246 time= 0.04755\n",
      "Epoch: 0150 train_loss= 0.45208 time= 0.04075\n",
      "Epoch: 0151 train_loss= 0.45170 time= 0.04591\n",
      "Epoch: 0152 train_loss= 0.45131 time= 0.04410\n",
      "Epoch: 0153 train_loss= 0.45091 time= 0.03844\n",
      "Epoch: 0154 train_loss= 0.45050 time= 0.03507\n",
      "Epoch: 0155 train_loss= 0.45008 time= 0.04318\n",
      "Epoch: 0156 train_loss= 0.44965 time= 0.04992\n",
      "Epoch: 0157 train_loss= 0.44920 time= 0.04347\n",
      "Epoch: 0158 train_loss= 0.44875 time= 0.04347\n",
      "Epoch: 0159 train_loss= 0.44828 time= 0.04089\n",
      "Epoch: 0160 train_loss= 0.44781 time= 0.03903\n",
      "Epoch: 0161 train_loss= 0.44733 time= 0.03938\n",
      "Epoch: 0162 train_loss= 0.44684 time= 0.03825\n",
      "Epoch: 0163 train_loss= 0.44635 time= 0.03391\n",
      "Epoch: 0164 train_loss= 0.44586 time= 0.04027\n",
      "Epoch: 0165 train_loss= 0.44538 time= 0.04058\n",
      "Epoch: 0166 train_loss= 0.44489 time= 0.03963\n",
      "Epoch: 0167 train_loss= 0.44442 time= 0.03584\n",
      "Epoch: 0168 train_loss= 0.44396 time= 0.03502\n",
      "Epoch: 0169 train_loss= 0.44350 time= 0.03484\n",
      "Epoch: 0170 train_loss= 0.44306 time= 0.03554\n",
      "Epoch: 0171 train_loss= 0.44264 time= 0.04067\n",
      "Epoch: 0172 train_loss= 0.44223 time= 0.04439\n",
      "Epoch: 0173 train_loss= 0.44184 time= 0.04021\n",
      "Epoch: 0174 train_loss= 0.44146 time= 0.03891\n",
      "Epoch: 0175 train_loss= 0.44111 time= 0.04089\n",
      "Epoch: 0176 train_loss= 0.44078 time= 0.03989\n",
      "Epoch: 0177 train_loss= 0.44046 time= 0.04189\n",
      "Epoch: 0178 train_loss= 0.44016 time= 0.04210\n",
      "Epoch: 0179 train_loss= 0.43988 time= 0.03789\n",
      "Epoch: 0180 train_loss= 0.43959 time= 0.03691\n",
      "Epoch: 0181 train_loss= 0.43931 time= 0.04050\n",
      "Epoch: 0182 train_loss= 0.43903 time= 0.03789\n",
      "Epoch: 0183 train_loss= 0.43873 time= 0.04026\n",
      "Epoch: 0184 train_loss= 0.43843 time= 0.04564\n",
      "Epoch: 0185 train_loss= 0.43813 time= 0.04076\n",
      "Epoch: 0186 train_loss= 0.43781 time= 0.03989\n",
      "Epoch: 0187 train_loss= 0.43750 time= 0.03491\n",
      "Epoch: 0188 train_loss= 0.43719 time= 0.03651\n",
      "Epoch: 0189 train_loss= 0.43688 time= 0.03479\n",
      "Epoch: 0190 train_loss= 0.43657 time= 0.04057\n",
      "Epoch: 0191 train_loss= 0.43628 time= 0.04320\n",
      "Epoch: 0192 train_loss= 0.43599 time= 0.04089\n",
      "Epoch: 0193 train_loss= 0.43571 time= 0.03491\n",
      "Epoch: 0194 train_loss= 0.43543 time= 0.03690\n",
      "Epoch: 0195 train_loss= 0.43517 time= 0.03491\n",
      "Epoch: 0196 train_loss= 0.43491 time= 0.03391\n",
      "Epoch: 0197 train_loss= 0.43466 time= 0.04089\n",
      "Epoch: 0198 train_loss= 0.43442 time= 0.04388\n",
      "Epoch: 0199 train_loss= 0.43419 time= 0.04558\n",
      "Epoch: 0200 train_loss= 0.43396 time= 0.03901\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73512 time= 0.17563\n",
      "Epoch: 0002 train_loss= 0.73477 time= 0.03386\n",
      "Epoch: 0003 train_loss= 0.73367 time= 0.04406\n",
      "Epoch: 0004 train_loss= 0.73132 time= 0.04529\n",
      "Epoch: 0005 train_loss= 0.72709 time= 0.04632\n",
      "Epoch: 0006 train_loss= 0.72043 time= 0.04335\n",
      "Epoch: 0007 train_loss= 0.71093 time= 0.04404\n",
      "Epoch: 0008 train_loss= 0.69883 time= 0.03690\n",
      "Epoch: 0009 train_loss= 0.68571 time= 0.03790\n",
      "Epoch: 0010 train_loss= 0.67549 time= 0.03690\n",
      "Epoch: 0011 train_loss= 0.67362 time= 0.03910\n",
      "Epoch: 0012 train_loss= 0.67712 time= 0.04398\n",
      "Epoch: 0013 train_loss= 0.67465 time= 0.04595\n",
      "Epoch: 0014 train_loss= 0.66666 time= 0.03989\n",
      "Epoch: 0015 train_loss= 0.65834 time= 0.03782\n",
      "Epoch: 0016 train_loss= 0.65277 time= 0.03678\n",
      "Epoch: 0017 train_loss= 0.64983 time= 0.03590\n",
      "Epoch: 0018 train_loss= 0.64781 time= 0.04289\n",
      "Epoch: 0019 train_loss= 0.64507 time= 0.04516\n",
      "Epoch: 0020 train_loss= 0.64084 time= 0.03989\n",
      "Epoch: 0021 train_loss= 0.63513 time= 0.04089\n",
      "Epoch: 0022 train_loss= 0.62858 time= 0.04224\n",
      "Epoch: 0023 train_loss= 0.62208 time= 0.03989\n",
      "Epoch: 0024 train_loss= 0.61650 time= 0.03790\n",
      "Epoch: 0025 train_loss= 0.61216 time= 0.04802\n",
      "Epoch: 0026 train_loss= 0.60858 time= 0.04374\n",
      "Epoch: 0027 train_loss= 0.60493 time= 0.03861\n",
      "Epoch: 0028 train_loss= 0.60095 time= 0.03727\n",
      "Epoch: 0029 train_loss= 0.59723 time= 0.03893\n",
      "Epoch: 0030 train_loss= 0.59446 time= 0.04113\n",
      "Epoch: 0031 train_loss= 0.59247 time= 0.04477\n",
      "Epoch: 0032 train_loss= 0.59015 time= 0.05186\n",
      "Epoch: 0033 train_loss= 0.58657 time= 0.03869\n",
      "Epoch: 0034 train_loss= 0.58167 time= 0.03402\n",
      "Epoch: 0035 train_loss= 0.57609 time= 0.03790\n",
      "Epoch: 0036 train_loss= 0.57045 time= 0.03590\n",
      "Epoch: 0037 train_loss= 0.56498 time= 0.03590\n",
      "Epoch: 0038 train_loss= 0.55961 time= 0.04688\n",
      "Epoch: 0039 train_loss= 0.55431 time= 0.04412\n",
      "Epoch: 0040 train_loss= 0.54939 time= 0.03958\n",
      "Epoch: 0041 train_loss= 0.54537 time= 0.04052\n",
      "Epoch: 0042 train_loss= 0.54275 time= 0.04189\n",
      "Epoch: 0043 train_loss= 0.54163 time= 0.04047\n",
      "Epoch: 0044 train_loss= 0.54153 time= 0.03786\n",
      "Epoch: 0045 train_loss= 0.54157 time= 0.04780\n",
      "Epoch: 0046 train_loss= 0.54098 time= 0.04588\n",
      "Epoch: 0047 train_loss= 0.53951 time= 0.04761\n",
      "Epoch: 0048 train_loss= 0.53739 time= 0.04094\n",
      "Epoch: 0049 train_loss= 0.53495 time= 0.03699\n",
      "Epoch: 0050 train_loss= 0.53240 time= 0.03803\n",
      "Epoch: 0051 train_loss= 0.52986 time= 0.03879\n",
      "Epoch: 0052 train_loss= 0.52745 time= 0.03703\n",
      "Epoch: 0053 train_loss= 0.52527 time= 0.03930\n",
      "Epoch: 0054 train_loss= 0.52337 time= 0.04525\n",
      "Epoch: 0055 train_loss= 0.52169 time= 0.04156\n",
      "Epoch: 0056 train_loss= 0.52010 time= 0.04598\n",
      "Epoch: 0057 train_loss= 0.51850 time= 0.04587\n",
      "Epoch: 0058 train_loss= 0.51686 time= 0.03790\n",
      "Epoch: 0059 train_loss= 0.51526 time= 0.03669\n",
      "Epoch: 0060 train_loss= 0.51381 time= 0.03783\n",
      "Epoch: 0061 train_loss= 0.51256 time= 0.04285\n",
      "Epoch: 0062 train_loss= 0.51155 time= 0.03989\n",
      "Epoch: 0063 train_loss= 0.51074 time= 0.04513\n",
      "Epoch: 0064 train_loss= 0.51008 time= 0.04978\n",
      "Epoch: 0065 train_loss= 0.50946 time= 0.04489\n",
      "Epoch: 0066 train_loss= 0.50878 time= 0.04388\n",
      "Epoch: 0067 train_loss= 0.50796 time= 0.03989\n",
      "Epoch: 0068 train_loss= 0.50699 time= 0.03945\n",
      "Epoch: 0069 train_loss= 0.50591 time= 0.03690\n",
      "Epoch: 0070 train_loss= 0.50474 time= 0.04064\n",
      "Epoch: 0071 train_loss= 0.50354 time= 0.04967\n",
      "Epoch: 0072 train_loss= 0.50234 time= 0.04488\n",
      "Epoch: 0073 train_loss= 0.50120 time= 0.04970\n",
      "Epoch: 0074 train_loss= 0.50010 time= 0.04136\n",
      "Epoch: 0075 train_loss= 0.49903 time= 0.04762\n",
      "Epoch: 0076 train_loss= 0.49794 time= 0.03880\n",
      "Epoch: 0077 train_loss= 0.49682 time= 0.03669\n",
      "Epoch: 0078 train_loss= 0.49569 time= 0.03662\n",
      "Epoch: 0079 train_loss= 0.49455 time= 0.03590\n",
      "Epoch: 0080 train_loss= 0.49340 time= 0.04333\n",
      "Epoch: 0081 train_loss= 0.49225 time= 0.05170\n",
      "Epoch: 0082 train_loss= 0.49112 time= 0.05485\n",
      "Epoch: 0083 train_loss= 0.49004 time= 0.05975\n",
      "Epoch: 0084 train_loss= 0.48905 time= 0.04739\n",
      "Epoch: 0085 train_loss= 0.48816 time= 0.04829\n",
      "Epoch: 0086 train_loss= 0.48737 time= 0.03986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 train_loss= 0.48666 time= 0.03889\n",
      "Epoch: 0088 train_loss= 0.48601 time= 0.03872\n",
      "Epoch: 0089 train_loss= 0.48540 time= 0.04258\n",
      "Epoch: 0090 train_loss= 0.48478 time= 0.05053\n",
      "Epoch: 0091 train_loss= 0.48411 time= 0.04740\n",
      "Epoch: 0092 train_loss= 0.48335 time= 0.03821\n",
      "Epoch: 0093 train_loss= 0.48251 time= 0.03859\n",
      "Epoch: 0094 train_loss= 0.48159 time= 0.03945\n",
      "Epoch: 0095 train_loss= 0.48061 time= 0.03624\n",
      "Epoch: 0096 train_loss= 0.47957 time= 0.04625\n",
      "Epoch: 0097 train_loss= 0.47850 time= 0.04728\n",
      "Epoch: 0098 train_loss= 0.47744 time= 0.03889\n",
      "Epoch: 0099 train_loss= 0.47640 time= 0.03682\n",
      "Epoch: 0100 train_loss= 0.47539 time= 0.03484\n",
      "Epoch: 0101 train_loss= 0.47443 time= 0.03675\n",
      "Epoch: 0102 train_loss= 0.47351 time= 0.04591\n",
      "Epoch: 0103 train_loss= 0.47266 time= 0.04426\n",
      "Epoch: 0104 train_loss= 0.47188 time= 0.04447\n",
      "Epoch: 0105 train_loss= 0.47117 time= 0.03989\n",
      "Epoch: 0106 train_loss= 0.47054 time= 0.03989\n",
      "Epoch: 0107 train_loss= 0.46997 time= 0.03890\n",
      "Epoch: 0108 train_loss= 0.46945 time= 0.03990\n",
      "Epoch: 0109 train_loss= 0.46896 time= 0.04987\n",
      "Epoch: 0110 train_loss= 0.46848 time= 0.04504\n",
      "Epoch: 0111 train_loss= 0.46799 time= 0.03889\n",
      "Epoch: 0112 train_loss= 0.46747 time= 0.03690\n",
      "Epoch: 0113 train_loss= 0.46692 time= 0.03989\n",
      "Epoch: 0114 train_loss= 0.46635 time= 0.04189\n",
      "Epoch: 0115 train_loss= 0.46576 time= 0.04040\n",
      "Epoch: 0116 train_loss= 0.46515 time= 0.04787\n",
      "Epoch: 0117 train_loss= 0.46455 time= 0.04686\n",
      "Epoch: 0118 train_loss= 0.46396 time= 0.03833\n",
      "Epoch: 0119 train_loss= 0.46337 time= 0.03715\n",
      "Epoch: 0120 train_loss= 0.46280 time= 0.03865\n",
      "Epoch: 0121 train_loss= 0.46225 time= 0.04089\n",
      "Epoch: 0122 train_loss= 0.46170 time= 0.05086\n",
      "Epoch: 0123 train_loss= 0.46115 time= 0.04687\n",
      "Epoch: 0124 train_loss= 0.46061 time= 0.05014\n",
      "Epoch: 0125 train_loss= 0.46006 time= 0.04676\n",
      "Epoch: 0126 train_loss= 0.45952 time= 0.04771\n",
      "Epoch: 0127 train_loss= 0.45898 time= 0.03997\n",
      "Epoch: 0128 train_loss= 0.45844 time= 0.03890\n",
      "Epoch: 0129 train_loss= 0.45792 time= 0.03881\n",
      "Epoch: 0130 train_loss= 0.45742 time= 0.03990\n",
      "Epoch: 0131 train_loss= 0.45695 time= 0.04987\n",
      "Epoch: 0132 train_loss= 0.45650 time= 0.05186\n",
      "Epoch: 0133 train_loss= 0.45608 time= 0.05581\n",
      "Epoch: 0134 train_loss= 0.45568 time= 0.04477\n",
      "Epoch: 0135 train_loss= 0.45531 time= 0.04588\n",
      "Epoch: 0136 train_loss= 0.45496 time= 0.03989\n",
      "Epoch: 0137 train_loss= 0.45461 time= 0.04089\n",
      "Epoch: 0138 train_loss= 0.45426 time= 0.04189\n",
      "Epoch: 0139 train_loss= 0.45390 time= 0.03990\n",
      "Epoch: 0140 train_loss= 0.45352 time= 0.04814\n",
      "Epoch: 0141 train_loss= 0.45311 time= 0.04830\n",
      "Epoch: 0142 train_loss= 0.45268 time= 0.05060\n",
      "Epoch: 0143 train_loss= 0.45221 time= 0.04424\n",
      "Epoch: 0144 train_loss= 0.45173 time= 0.04116\n",
      "Epoch: 0145 train_loss= 0.45123 time= 0.03848\n",
      "Epoch: 0146 train_loss= 0.45071 time= 0.03890\n",
      "Epoch: 0147 train_loss= 0.45018 time= 0.03639\n",
      "Epoch: 0148 train_loss= 0.44965 time= 0.03952\n",
      "Epoch: 0149 train_loss= 0.44911 time= 0.04224\n",
      "Epoch: 0150 train_loss= 0.44857 time= 0.04884\n",
      "Epoch: 0151 train_loss= 0.44802 time= 0.04563\n",
      "Epoch: 0152 train_loss= 0.44748 time= 0.04398\n",
      "Epoch: 0153 train_loss= 0.44695 time= 0.03989\n",
      "Epoch: 0154 train_loss= 0.44643 time= 0.03805\n",
      "Epoch: 0155 train_loss= 0.44594 time= 0.03476\n",
      "Epoch: 0156 train_loss= 0.44548 time= 0.04588\n",
      "Epoch: 0157 train_loss= 0.44506 time= 0.04887\n",
      "Epoch: 0158 train_loss= 0.44468 time= 0.04688\n",
      "Epoch: 0159 train_loss= 0.44434 time= 0.04474\n",
      "Epoch: 0160 train_loss= 0.44402 time= 0.03876\n",
      "Epoch: 0161 train_loss= 0.44372 time= 0.03989\n",
      "Epoch: 0162 train_loss= 0.44341 time= 0.03590\n",
      "Epoch: 0163 train_loss= 0.44309 time= 0.03535\n",
      "Epoch: 0164 train_loss= 0.44274 time= 0.03592\n",
      "Epoch: 0165 train_loss= 0.44236 time= 0.04488\n",
      "Epoch: 0166 train_loss= 0.44196 time= 0.04688\n",
      "Epoch: 0167 train_loss= 0.44155 time= 0.04388\n",
      "Epoch: 0168 train_loss= 0.44114 time= 0.03690\n",
      "Epoch: 0169 train_loss= 0.44073 time= 0.04289\n",
      "Epoch: 0170 train_loss= 0.44033 time= 0.03764\n",
      "Epoch: 0171 train_loss= 0.43995 time= 0.04183\n",
      "Epoch: 0172 train_loss= 0.43959 time= 0.04152\n",
      "Epoch: 0173 train_loss= 0.43924 time= 0.03890\n",
      "Epoch: 0174 train_loss= 0.43891 time= 0.03488\n",
      "Epoch: 0175 train_loss= 0.43860 time= 0.03491\n",
      "Epoch: 0176 train_loss= 0.43829 time= 0.03491\n",
      "Epoch: 0177 train_loss= 0.43799 time= 0.03491\n",
      "Epoch: 0178 train_loss= 0.43770 time= 0.04388\n",
      "Epoch: 0179 train_loss= 0.43740 time= 0.04089\n",
      "Epoch: 0180 train_loss= 0.43711 time= 0.03889\n",
      "Epoch: 0181 train_loss= 0.43682 time= 0.03749\n",
      "Epoch: 0182 train_loss= 0.43653 time= 0.03387\n",
      "Epoch: 0183 train_loss= 0.43623 time= 0.03690\n",
      "Epoch: 0184 train_loss= 0.43593 time= 0.04319\n",
      "Epoch: 0185 train_loss= 0.43563 time= 0.04304\n",
      "Epoch: 0186 train_loss= 0.43533 time= 0.03590\n",
      "Epoch: 0187 train_loss= 0.43503 time= 0.03591\n",
      "Epoch: 0188 train_loss= 0.43473 time= 0.03502\n",
      "Epoch: 0189 train_loss= 0.43444 time= 0.03678\n",
      "Epoch: 0190 train_loss= 0.43417 time= 0.04089\n",
      "Epoch: 0191 train_loss= 0.43390 time= 0.03794\n",
      "Epoch: 0192 train_loss= 0.43365 time= 0.03939\n",
      "Epoch: 0193 train_loss= 0.43340 time= 0.03393\n",
      "Epoch: 0194 train_loss= 0.43317 time= 0.03908\n",
      "Epoch: 0195 train_loss= 0.43295 time= 0.03690\n",
      "Epoch: 0196 train_loss= 0.43274 time= 0.03523\n",
      "Epoch: 0197 train_loss= 0.43253 time= 0.04333\n",
      "Epoch: 0198 train_loss= 0.43232 time= 0.04638\n",
      "Epoch: 0199 train_loss= 0.43211 time= 0.04864\n",
      "Epoch: 0200 train_loss= 0.43190 time= 0.04281\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73511 time= 0.15159\n",
      "Epoch: 0002 train_loss= 0.73470 time= 0.04551\n",
      "Epoch: 0003 train_loss= 0.73346 time= 0.04983\n",
      "Epoch: 0004 train_loss= 0.73072 time= 0.04788\n",
      "Epoch: 0005 train_loss= 0.72566 time= 0.04181\n",
      "Epoch: 0006 train_loss= 0.71755 time= 0.03525\n",
      "Epoch: 0007 train_loss= 0.70612 time= 0.03574\n",
      "Epoch: 0008 train_loss= 0.69254 time= 0.03790\n",
      "Epoch: 0009 train_loss= 0.68095 time= 0.03626\n",
      "Epoch: 0010 train_loss= 0.67883 time= 0.04688\n",
      "Epoch: 0011 train_loss= 0.68406 time= 0.03946\n",
      "Epoch: 0012 train_loss= 0.68168 time= 0.03376\n",
      "Epoch: 0013 train_loss= 0.67371 time= 0.03690\n",
      "Epoch: 0014 train_loss= 0.66666 time= 0.03737\n",
      "Epoch: 0015 train_loss= 0.66318 time= 0.03690\n",
      "Epoch: 0016 train_loss= 0.66225 time= 0.04173\n",
      "Epoch: 0017 train_loss= 0.66176 time= 0.04288\n",
      "Epoch: 0018 train_loss= 0.66037 time= 0.03989\n",
      "Epoch: 0019 train_loss= 0.65770 time= 0.03889\n",
      "Epoch: 0020 train_loss= 0.65406 time= 0.03812\n",
      "Epoch: 0021 train_loss= 0.65014 time= 0.03608\n",
      "Epoch: 0022 train_loss= 0.64673 time= 0.03751\n",
      "Epoch: 0023 train_loss= 0.64430 time= 0.04289\n",
      "Epoch: 0024 train_loss= 0.64254 time= 0.04388\n",
      "Epoch: 0025 train_loss= 0.64052 time= 0.03453\n",
      "Epoch: 0026 train_loss= 0.63748 time= 0.03573\n",
      "Epoch: 0027 train_loss= 0.63343 time= 0.03645\n",
      "Epoch: 0028 train_loss= 0.62896 time= 0.03623\n",
      "Epoch: 0029 train_loss= 0.62462 time= 0.04244\n",
      "Epoch: 0030 train_loss= 0.62047 time= 0.04488\n",
      "Epoch: 0031 train_loss= 0.61608 time= 0.04127\n",
      "Epoch: 0032 train_loss= 0.61100 time= 0.03631\n",
      "Epoch: 0033 train_loss= 0.60506 time= 0.03450\n",
      "Epoch: 0034 train_loss= 0.59843 time= 0.03783\n",
      "Epoch: 0035 train_loss= 0.59147 time= 0.03890\n",
      "Epoch: 0036 train_loss= 0.58453 time= 0.04289\n",
      "Epoch: 0037 train_loss= 0.57780 time= 0.04488\n",
      "Epoch: 0038 train_loss= 0.57132 time= 0.03590\n",
      "Epoch: 0039 train_loss= 0.56516 time= 0.03690\n",
      "Epoch: 0040 train_loss= 0.55955 time= 0.03535\n",
      "Epoch: 0041 train_loss= 0.55483 time= 0.04344\n",
      "Epoch: 0042 train_loss= 0.55126 time= 0.04817\n",
      "Epoch: 0043 train_loss= 0.54880 time= 0.04957\n",
      "Epoch: 0044 train_loss= 0.54709 time= 0.04388\n",
      "Epoch: 0045 train_loss= 0.54565 time= 0.04288\n",
      "Epoch: 0046 train_loss= 0.54417 time= 0.03822\n",
      "Epoch: 0047 train_loss= 0.54254 time= 0.04089\n",
      "Epoch: 0048 train_loss= 0.54083 time= 0.03989\n",
      "Epoch: 0049 train_loss= 0.53902 time= 0.03590\n",
      "Epoch: 0050 train_loss= 0.53698 time= 0.04119\n",
      "Epoch: 0051 train_loss= 0.53452 time= 0.04302\n",
      "Epoch: 0052 train_loss= 0.53162 time= 0.04687\n",
      "Epoch: 0053 train_loss= 0.52845 time= 0.04043\n",
      "Epoch: 0054 train_loss= 0.52521 time= 0.04146\n",
      "Epoch: 0055 train_loss= 0.52202 time= 0.03847\n",
      "Epoch: 0056 train_loss= 0.51891 time= 0.04104\n",
      "Epoch: 0057 train_loss= 0.51591 time= 0.03690\n",
      "Epoch: 0058 train_loss= 0.51307 time= 0.03371\n",
      "Epoch: 0059 train_loss= 0.51046 time= 0.04189\n",
      "Epoch: 0060 train_loss= 0.50808 time= 0.04388\n",
      "Epoch: 0061 train_loss= 0.50588 time= 0.04602\n",
      "Epoch: 0062 train_loss= 0.50377 time= 0.03989\n",
      "Epoch: 0063 train_loss= 0.50172 time= 0.03989\n",
      "Epoch: 0064 train_loss= 0.49976 time= 0.03890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0065 train_loss= 0.49791 time= 0.03944\n",
      "Epoch: 0066 train_loss= 0.49615 time= 0.04025\n",
      "Epoch: 0067 train_loss= 0.49444 time= 0.04255\n",
      "Epoch: 0068 train_loss= 0.49275 time= 0.04488\n",
      "Epoch: 0069 train_loss= 0.49113 time= 0.04289\n",
      "Epoch: 0070 train_loss= 0.48966 time= 0.04588\n",
      "Epoch: 0071 train_loss= 0.48835 time= 0.04787\n",
      "Epoch: 0072 train_loss= 0.48722 time= 0.03790\n",
      "Epoch: 0073 train_loss= 0.48622 time= 0.04089\n",
      "Epoch: 0074 train_loss= 0.48533 time= 0.03841\n",
      "Epoch: 0075 train_loss= 0.48449 time= 0.03405\n",
      "Epoch: 0076 train_loss= 0.48362 time= 0.04341\n",
      "Epoch: 0077 train_loss= 0.48266 time= 0.04045\n",
      "Epoch: 0078 train_loss= 0.48157 time= 0.04767\n",
      "Epoch: 0079 train_loss= 0.48040 time= 0.04139\n",
      "Epoch: 0080 train_loss= 0.47922 time= 0.04347\n",
      "Epoch: 0081 train_loss= 0.47806 time= 0.03532\n",
      "Epoch: 0082 train_loss= 0.47698 time= 0.03536\n",
      "Epoch: 0083 train_loss= 0.47599 time= 0.03690\n",
      "Epoch: 0084 train_loss= 0.47510 time= 0.03591\n",
      "Epoch: 0085 train_loss= 0.47430 time= 0.04687\n",
      "Epoch: 0086 train_loss= 0.47360 time= 0.04272\n",
      "Epoch: 0087 train_loss= 0.47299 time= 0.03771\n",
      "Epoch: 0088 train_loss= 0.47245 time= 0.03989\n",
      "Epoch: 0089 train_loss= 0.47197 time= 0.03743\n",
      "Epoch: 0090 train_loss= 0.47154 time= 0.03574\n",
      "Epoch: 0091 train_loss= 0.47116 time= 0.04289\n",
      "Epoch: 0092 train_loss= 0.47084 time= 0.04158\n",
      "Epoch: 0093 train_loss= 0.47056 time= 0.03827\n",
      "Epoch: 0094 train_loss= 0.47029 time= 0.03696\n",
      "Epoch: 0095 train_loss= 0.47000 time= 0.03685\n",
      "Epoch: 0096 train_loss= 0.46970 time= 0.03391\n",
      "Epoch: 0097 train_loss= 0.46939 time= 0.04089\n",
      "Epoch: 0098 train_loss= 0.46907 time= 0.04039\n",
      "Epoch: 0099 train_loss= 0.46872 time= 0.03889\n",
      "Epoch: 0100 train_loss= 0.46837 time= 0.03790\n",
      "Epoch: 0101 train_loss= 0.46802 time= 0.04133\n",
      "Epoch: 0102 train_loss= 0.46770 time= 0.03590\n",
      "Epoch: 0103 train_loss= 0.46741 time= 0.03491\n",
      "Epoch: 0104 train_loss= 0.46714 time= 0.04488\n",
      "Epoch: 0105 train_loss= 0.46691 time= 0.04189\n",
      "Epoch: 0106 train_loss= 0.46669 time= 0.03877\n",
      "Epoch: 0107 train_loss= 0.46650 time= 0.03790\n",
      "Epoch: 0108 train_loss= 0.46631 time= 0.03870\n",
      "Epoch: 0109 train_loss= 0.46612 time= 0.04089\n",
      "Epoch: 0110 train_loss= 0.46594 time= 0.04388\n",
      "Epoch: 0111 train_loss= 0.46574 time= 0.04089\n",
      "Epoch: 0112 train_loss= 0.46554 time= 0.03491\n",
      "Epoch: 0113 train_loss= 0.46533 time= 0.03590\n",
      "Epoch: 0114 train_loss= 0.46512 time= 0.03299\n",
      "Epoch: 0115 train_loss= 0.46491 time= 0.03435\n",
      "Epoch: 0116 train_loss= 0.46470 time= 0.04059\n",
      "Epoch: 0117 train_loss= 0.46449 time= 0.04703\n",
      "Epoch: 0118 train_loss= 0.46429 time= 0.04986\n",
      "Epoch: 0119 train_loss= 0.46409 time= 0.04035\n",
      "Epoch: 0120 train_loss= 0.46390 time= 0.04288\n",
      "Epoch: 0121 train_loss= 0.46371 time= 0.03690\n",
      "Epoch: 0122 train_loss= 0.46352 time= 0.03532\n",
      "Epoch: 0123 train_loss= 0.46332 time= 0.03400\n",
      "Epoch: 0124 train_loss= 0.46312 time= 0.03962\n",
      "Epoch: 0125 train_loss= 0.46292 time= 0.04623\n",
      "Epoch: 0126 train_loss= 0.46271 time= 0.05022\n",
      "Epoch: 0127 train_loss= 0.46249 time= 0.04634\n",
      "Epoch: 0128 train_loss= 0.46227 time= 0.04477\n",
      "Epoch: 0129 train_loss= 0.46204 time= 0.03453\n",
      "Epoch: 0130 train_loss= 0.46181 time= 0.03541\n",
      "Epoch: 0131 train_loss= 0.46157 time= 0.03498\n",
      "Epoch: 0132 train_loss= 0.46132 time= 0.03708\n",
      "Epoch: 0133 train_loss= 0.46107 time= 0.04189\n",
      "Epoch: 0134 train_loss= 0.46081 time= 0.04588\n",
      "Epoch: 0135 train_loss= 0.46054 time= 0.04488\n",
      "Epoch: 0136 train_loss= 0.46027 time= 0.04508\n",
      "Epoch: 0137 train_loss= 0.45999 time= 0.03670\n",
      "Epoch: 0138 train_loss= 0.45969 time= 0.03690\n",
      "Epoch: 0139 train_loss= 0.45939 time= 0.03590\n",
      "Epoch: 0140 train_loss= 0.45909 time= 0.03590\n",
      "Epoch: 0141 train_loss= 0.45877 time= 0.04289\n",
      "Epoch: 0142 train_loss= 0.45846 time= 0.05038\n",
      "Epoch: 0143 train_loss= 0.45814 time= 0.04981\n",
      "Epoch: 0144 train_loss= 0.45781 time= 0.04468\n",
      "Epoch: 0145 train_loss= 0.45749 time= 0.04271\n",
      "Epoch: 0146 train_loss= 0.45717 time= 0.03491\n",
      "Epoch: 0147 train_loss= 0.45685 time= 0.03590\n",
      "Epoch: 0148 train_loss= 0.45654 time= 0.03491\n",
      "Epoch: 0149 train_loss= 0.45623 time= 0.03790\n",
      "Epoch: 0150 train_loss= 0.45594 time= 0.04688\n",
      "Epoch: 0151 train_loss= 0.45565 time= 0.04695\n",
      "Epoch: 0152 train_loss= 0.45537 time= 0.03782\n",
      "Epoch: 0153 train_loss= 0.45510 time= 0.03990\n",
      "Epoch: 0154 train_loss= 0.45484 time= 0.04628\n",
      "Epoch: 0155 train_loss= 0.45458 time= 0.04008\n",
      "Epoch: 0156 train_loss= 0.45432 time= 0.03391\n",
      "Epoch: 0157 train_loss= 0.45406 time= 0.03460\n",
      "Epoch: 0158 train_loss= 0.45379 time= 0.03590\n",
      "Epoch: 0159 train_loss= 0.45352 time= 0.04388\n",
      "Epoch: 0160 train_loss= 0.45324 time= 0.03989\n",
      "Epoch: 0161 train_loss= 0.45294 time= 0.03605\n",
      "Epoch: 0162 train_loss= 0.45263 time= 0.03775\n",
      "Epoch: 0163 train_loss= 0.45231 time= 0.03990\n",
      "Epoch: 0164 train_loss= 0.45197 time= 0.03491\n",
      "Epoch: 0165 train_loss= 0.45162 time= 0.04687\n",
      "Epoch: 0166 train_loss= 0.45125 time= 0.04289\n",
      "Epoch: 0167 train_loss= 0.45086 time= 0.03391\n",
      "Epoch: 0168 train_loss= 0.45046 time= 0.03890\n",
      "Epoch: 0169 train_loss= 0.45005 time= 0.03790\n",
      "Epoch: 0170 train_loss= 0.44962 time= 0.03690\n",
      "Epoch: 0171 train_loss= 0.44918 time= 0.04214\n",
      "Epoch: 0172 train_loss= 0.44872 time= 0.04759\n",
      "Epoch: 0173 train_loss= 0.44825 time= 0.03830\n",
      "Epoch: 0174 train_loss= 0.44778 time= 0.03703\n",
      "Epoch: 0175 train_loss= 0.44731 time= 0.03548\n",
      "Epoch: 0176 train_loss= 0.44684 time= 0.03620\n",
      "Epoch: 0177 train_loss= 0.44638 time= 0.04020\n",
      "Epoch: 0178 train_loss= 0.44593 time= 0.04466\n",
      "Epoch: 0179 train_loss= 0.44551 time= 0.03890\n",
      "Epoch: 0180 train_loss= 0.44512 time= 0.03790\n",
      "Epoch: 0181 train_loss= 0.44475 time= 0.03649\n",
      "Epoch: 0182 train_loss= 0.44441 time= 0.03690\n",
      "Epoch: 0183 train_loss= 0.44409 time= 0.04488\n",
      "Epoch: 0184 train_loss= 0.44379 time= 0.04396\n",
      "Epoch: 0185 train_loss= 0.44349 time= 0.04275\n",
      "Epoch: 0186 train_loss= 0.44318 time= 0.03844\n",
      "Epoch: 0187 train_loss= 0.44286 time= 0.03635\n",
      "Epoch: 0188 train_loss= 0.44252 time= 0.03790\n",
      "Epoch: 0189 train_loss= 0.44216 time= 0.03943\n",
      "Epoch: 0190 train_loss= 0.44179 time= 0.04787\n",
      "Epoch: 0191 train_loss= 0.44141 time= 0.04487\n",
      "Epoch: 0192 train_loss= 0.44104 time= 0.04541\n",
      "Epoch: 0193 train_loss= 0.44069 time= 0.04189\n",
      "Epoch: 0194 train_loss= 0.44035 time= 0.04089\n",
      "Epoch: 0195 train_loss= 0.44002 time= 0.03807\n",
      "Epoch: 0196 train_loss= 0.43972 time= 0.03889\n",
      "Epoch: 0197 train_loss= 0.43944 time= 0.03806\n",
      "Epoch: 0198 train_loss= 0.43918 time= 0.04472\n",
      "Epoch: 0199 train_loss= 0.43892 time= 0.04887\n",
      "Epoch: 0200 train_loss= 0.43868 time= 0.04684\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73510 time= 0.15052\n",
      "Epoch: 0002 train_loss= 0.73461 time= 0.03650\n",
      "Epoch: 0003 train_loss= 0.73318 time= 0.03446\n",
      "Epoch: 0004 train_loss= 0.73018 time= 0.04110\n",
      "Epoch: 0005 train_loss= 0.72489 time= 0.04127\n",
      "Epoch: 0006 train_loss= 0.71671 time= 0.03984\n",
      "Epoch: 0007 train_loss= 0.70549 time= 0.03549\n",
      "Epoch: 0008 train_loss= 0.69224 time= 0.03475\n",
      "Epoch: 0009 train_loss= 0.68027 time= 0.03513\n",
      "Epoch: 0010 train_loss= 0.67570 time= 0.03579\n",
      "Epoch: 0011 train_loss= 0.67972 time= 0.04149\n",
      "Epoch: 0012 train_loss= 0.67942 time= 0.04061\n",
      "Epoch: 0013 train_loss= 0.67249 time= 0.03386\n",
      "Epoch: 0014 train_loss= 0.66451 time= 0.03512\n",
      "Epoch: 0015 train_loss= 0.65924 time= 0.03483\n",
      "Epoch: 0016 train_loss= 0.65691 time= 0.03571\n",
      "Epoch: 0017 train_loss= 0.65579 time= 0.04283\n",
      "Epoch: 0018 train_loss= 0.65421 time= 0.04047\n",
      "Epoch: 0019 train_loss= 0.65138 time= 0.03663\n",
      "Epoch: 0020 train_loss= 0.64730 time= 0.03915\n",
      "Epoch: 0021 train_loss= 0.64244 time= 0.03702\n",
      "Epoch: 0022 train_loss= 0.63754 time= 0.03484\n",
      "Epoch: 0023 train_loss= 0.63327 time= 0.04428\n",
      "Epoch: 0024 train_loss= 0.62984 time= 0.03707\n",
      "Epoch: 0025 train_loss= 0.62676 time= 0.03642\n",
      "Epoch: 0026 train_loss= 0.62319 time= 0.03597\n",
      "Epoch: 0027 train_loss= 0.61873 time= 0.03503\n",
      "Epoch: 0028 train_loss= 0.61377 time= 0.03493\n",
      "Epoch: 0029 train_loss= 0.60916 time= 0.04434\n",
      "Epoch: 0030 train_loss= 0.60557 time= 0.04905\n",
      "Epoch: 0031 train_loss= 0.60300 time= 0.04642\n",
      "Epoch: 0032 train_loss= 0.60094 time= 0.04727\n",
      "Epoch: 0033 train_loss= 0.59896 time= 0.03681\n",
      "Epoch: 0034 train_loss= 0.59708 time= 0.03643\n",
      "Epoch: 0035 train_loss= 0.59546 time= 0.03509\n",
      "Epoch: 0036 train_loss= 0.59406 time= 0.03737\n",
      "Epoch: 0037 train_loss= 0.59245 time= 0.03986\n",
      "Epoch: 0038 train_loss= 0.59005 time= 0.04572\n",
      "Epoch: 0039 train_loss= 0.58659 time= 0.05020\n",
      "Epoch: 0040 train_loss= 0.58228 time= 0.04374\n",
      "Epoch: 0041 train_loss= 0.57757 time= 0.04273\n",
      "Epoch: 0042 train_loss= 0.57274 time= 0.03804\n",
      "Epoch: 0043 train_loss= 0.56779 time= 0.03610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0044 train_loss= 0.56259 time= 0.03557\n",
      "Epoch: 0045 train_loss= 0.55703 time= 0.03399\n",
      "Epoch: 0046 train_loss= 0.55118 time= 0.04538\n",
      "Epoch: 0047 train_loss= 0.54520 time= 0.04455\n",
      "Epoch: 0048 train_loss= 0.53925 time= 0.05310\n",
      "Epoch: 0049 train_loss= 0.53345 time= 0.04470\n",
      "Epoch: 0050 train_loss= 0.52793 time= 0.04389\n",
      "Epoch: 0051 train_loss= 0.52287 time= 0.04260\n",
      "Epoch: 0052 train_loss= 0.51855 time= 0.04340\n",
      "Epoch: 0053 train_loss= 0.51521 time= 0.03739\n",
      "Epoch: 0054 train_loss= 0.51297 time= 0.03690\n",
      "Epoch: 0055 train_loss= 0.51163 time= 0.04357\n",
      "Epoch: 0056 train_loss= 0.51080 time= 0.04393\n",
      "Epoch: 0057 train_loss= 0.51001 time= 0.05186\n",
      "Epoch: 0058 train_loss= 0.50893 time= 0.04261\n",
      "Epoch: 0059 train_loss= 0.50737 time= 0.04049\n",
      "Epoch: 0060 train_loss= 0.50530 time= 0.03790\n",
      "Epoch: 0061 train_loss= 0.50279 time= 0.03662\n",
      "Epoch: 0062 train_loss= 0.49999 time= 0.03790\n",
      "Epoch: 0063 train_loss= 0.49717 time= 0.04189\n",
      "Epoch: 0064 train_loss= 0.49462 time= 0.04687\n",
      "Epoch: 0065 train_loss= 0.49256 time= 0.04668\n",
      "Epoch: 0066 train_loss= 0.49106 time= 0.04787\n",
      "Epoch: 0067 train_loss= 0.49003 time= 0.04178\n",
      "Epoch: 0068 train_loss= 0.48929 time= 0.04140\n",
      "Epoch: 0069 train_loss= 0.48872 time= 0.03690\n",
      "Epoch: 0070 train_loss= 0.48821 time= 0.03901\n",
      "Epoch: 0071 train_loss= 0.48769 time= 0.03686\n",
      "Epoch: 0072 train_loss= 0.48710 time= 0.04179\n",
      "Epoch: 0073 train_loss= 0.48634 time= 0.04687\n",
      "Epoch: 0074 train_loss= 0.48539 time= 0.03890\n",
      "Epoch: 0075 train_loss= 0.48431 time= 0.03690\n",
      "Epoch: 0076 train_loss= 0.48317 time= 0.03790\n",
      "Epoch: 0077 train_loss= 0.48204 time= 0.03890\n",
      "Epoch: 0078 train_loss= 0.48094 time= 0.03807\n",
      "Epoch: 0079 train_loss= 0.47989 time= 0.04512\n",
      "Epoch: 0080 train_loss= 0.47890 time= 0.04055\n",
      "Epoch: 0081 train_loss= 0.47799 time= 0.03690\n",
      "Epoch: 0082 train_loss= 0.47716 time= 0.03890\n",
      "Epoch: 0083 train_loss= 0.47637 time= 0.03989\n",
      "Epoch: 0084 train_loss= 0.47562 time= 0.03889\n",
      "Epoch: 0085 train_loss= 0.47486 time= 0.04110\n",
      "Epoch: 0086 train_loss= 0.47411 time= 0.03952\n",
      "Epoch: 0087 train_loss= 0.47335 time= 0.03697\n",
      "Epoch: 0088 train_loss= 0.47260 time= 0.03683\n",
      "Epoch: 0089 train_loss= 0.47188 time= 0.03690\n",
      "Epoch: 0090 train_loss= 0.47121 time= 0.03799\n",
      "Epoch: 0091 train_loss= 0.47059 time= 0.04778\n",
      "Epoch: 0092 train_loss= 0.47004 time= 0.04289\n",
      "Epoch: 0093 train_loss= 0.46954 time= 0.04005\n",
      "Epoch: 0094 train_loss= 0.46909 time= 0.03575\n",
      "Epoch: 0095 train_loss= 0.46867 time= 0.03491\n",
      "Epoch: 0096 train_loss= 0.46826 time= 0.03787\n",
      "Epoch: 0097 train_loss= 0.46783 time= 0.04192\n",
      "Epoch: 0098 train_loss= 0.46737 time= 0.04402\n",
      "Epoch: 0099 train_loss= 0.46687 time= 0.03758\n",
      "Epoch: 0100 train_loss= 0.46635 time= 0.03790\n",
      "Epoch: 0101 train_loss= 0.46580 time= 0.03591\n",
      "Epoch: 0102 train_loss= 0.46524 time= 0.03811\n",
      "Epoch: 0103 train_loss= 0.46468 time= 0.03838\n",
      "Epoch: 0104 train_loss= 0.46412 time= 0.04830\n",
      "Epoch: 0105 train_loss= 0.46357 time= 0.04761\n",
      "Epoch: 0106 train_loss= 0.46303 time= 0.04229\n",
      "Epoch: 0107 train_loss= 0.46250 time= 0.04115\n",
      "Epoch: 0108 train_loss= 0.46197 time= 0.03538\n",
      "Epoch: 0109 train_loss= 0.46144 time= 0.03788\n",
      "Epoch: 0110 train_loss= 0.46090 time= 0.03778\n",
      "Epoch: 0111 train_loss= 0.46036 time= 0.04189\n",
      "Epoch: 0112 train_loss= 0.45981 time= 0.04488\n",
      "Epoch: 0113 train_loss= 0.45925 time= 0.04787\n",
      "Epoch: 0114 train_loss= 0.45867 time= 0.03965\n",
      "Epoch: 0115 train_loss= 0.45809 time= 0.04203\n",
      "Epoch: 0116 train_loss= 0.45751 time= 0.04074\n",
      "Epoch: 0117 train_loss= 0.45693 time= 0.03890\n",
      "Epoch: 0118 train_loss= 0.45637 time= 0.03700\n",
      "Epoch: 0119 train_loss= 0.45582 time= 0.03701\n",
      "Epoch: 0120 train_loss= 0.45528 time= 0.04888\n",
      "Epoch: 0121 train_loss= 0.45478 time= 0.04488\n",
      "Epoch: 0122 train_loss= 0.45429 time= 0.04788\n",
      "Epoch: 0123 train_loss= 0.45383 time= 0.03989\n",
      "Epoch: 0124 train_loss= 0.45339 time= 0.04089\n",
      "Epoch: 0125 train_loss= 0.45296 time= 0.03647\n",
      "Epoch: 0126 train_loss= 0.45255 time= 0.03734\n",
      "Epoch: 0127 train_loss= 0.45214 time= 0.03690\n",
      "Epoch: 0128 train_loss= 0.45174 time= 0.03904\n",
      "Epoch: 0129 train_loss= 0.45134 time= 0.04573\n",
      "Epoch: 0130 train_loss= 0.45093 time= 0.04388\n",
      "Epoch: 0131 train_loss= 0.45052 time= 0.04688\n",
      "Epoch: 0132 train_loss= 0.45012 time= 0.04427\n",
      "Epoch: 0133 train_loss= 0.44971 time= 0.03690\n",
      "Epoch: 0134 train_loss= 0.44932 time= 0.03590\n",
      "Epoch: 0135 train_loss= 0.44894 time= 0.03591\n",
      "Epoch: 0136 train_loss= 0.44857 time= 0.03889\n",
      "Epoch: 0137 train_loss= 0.44821 time= 0.04887\n",
      "Epoch: 0138 train_loss= 0.44788 time= 0.04687\n",
      "Epoch: 0139 train_loss= 0.44756 time= 0.05087\n",
      "Epoch: 0140 train_loss= 0.44726 time= 0.04510\n",
      "Epoch: 0141 train_loss= 0.44697 time= 0.03590\n",
      "Epoch: 0142 train_loss= 0.44669 time= 0.03674\n",
      "Epoch: 0143 train_loss= 0.44641 time= 0.03670\n",
      "Epoch: 0144 train_loss= 0.44614 time= 0.03844\n",
      "Epoch: 0145 train_loss= 0.44587 time= 0.03945\n",
      "Epoch: 0146 train_loss= 0.44559 time= 0.04184\n",
      "Epoch: 0147 train_loss= 0.44530 time= 0.04133\n",
      "Epoch: 0148 train_loss= 0.44502 time= 0.03698\n",
      "Epoch: 0149 train_loss= 0.44472 time= 0.03585\n",
      "Epoch: 0150 train_loss= 0.44443 time= 0.03699\n",
      "Epoch: 0151 train_loss= 0.44413 time= 0.03788\n",
      "Epoch: 0152 train_loss= 0.44383 time= 0.03590\n",
      "Epoch: 0153 train_loss= 0.44353 time= 0.03773\n",
      "Epoch: 0154 train_loss= 0.44323 time= 0.03606\n",
      "Epoch: 0155 train_loss= 0.44294 time= 0.03626\n",
      "Epoch: 0156 train_loss= 0.44265 time= 0.03291\n",
      "Epoch: 0157 train_loss= 0.44237 time= 0.03391\n",
      "Epoch: 0158 train_loss= 0.44210 time= 0.03490\n",
      "Epoch: 0159 train_loss= 0.44184 time= 0.03590\n",
      "Epoch: 0160 train_loss= 0.44158 time= 0.03590\n",
      "Epoch: 0161 train_loss= 0.44133 time= 0.03491\n",
      "Epoch: 0162 train_loss= 0.44110 time= 0.03491\n",
      "Epoch: 0163 train_loss= 0.44087 time= 0.03590\n",
      "Epoch: 0164 train_loss= 0.44065 time= 0.03491\n",
      "Epoch: 0165 train_loss= 0.44045 time= 0.03391\n",
      "Epoch: 0166 train_loss= 0.44025 time= 0.03690\n",
      "Epoch: 0167 train_loss= 0.44007 time= 0.03717\n",
      "Epoch: 0168 train_loss= 0.43989 time= 0.03799\n",
      "Epoch: 0169 train_loss= 0.43971 time= 0.03555\n",
      "Epoch: 0170 train_loss= 0.43954 time= 0.03391\n",
      "Epoch: 0171 train_loss= 0.43937 time= 0.03291\n",
      "Epoch: 0172 train_loss= 0.43920 time= 0.03590\n",
      "Epoch: 0173 train_loss= 0.43903 time= 0.03813\n",
      "Epoch: 0174 train_loss= 0.43885 time= 0.03291\n",
      "Epoch: 0175 train_loss= 0.43867 time= 0.03490\n",
      "Epoch: 0176 train_loss= 0.43849 time= 0.03790\n",
      "Epoch: 0177 train_loss= 0.43830 time= 0.03391\n",
      "Epoch: 0178 train_loss= 0.43811 time= 0.03491\n",
      "Epoch: 0179 train_loss= 0.43791 time= 0.03405\n",
      "Epoch: 0180 train_loss= 0.43772 time= 0.03477\n",
      "Epoch: 0181 train_loss= 0.43752 time= 0.03491\n",
      "Epoch: 0182 train_loss= 0.43732 time= 0.03548\n",
      "Epoch: 0183 train_loss= 0.43712 time= 0.03932\n",
      "Epoch: 0184 train_loss= 0.43692 time= 0.03527\n",
      "Epoch: 0185 train_loss= 0.43671 time= 0.03491\n",
      "Epoch: 0186 train_loss= 0.43650 time= 0.03591\n",
      "Epoch: 0187 train_loss= 0.43628 time= 0.03763\n",
      "Epoch: 0188 train_loss= 0.43606 time= 0.03417\n",
      "Epoch: 0189 train_loss= 0.43584 time= 0.03388\n",
      "Epoch: 0190 train_loss= 0.43561 time= 0.03149\n",
      "Epoch: 0191 train_loss= 0.43537 time= 0.03263\n",
      "Epoch: 0192 train_loss= 0.43513 time= 0.03331\n",
      "Epoch: 0193 train_loss= 0.43489 time= 0.03237\n",
      "Epoch: 0194 train_loss= 0.43464 time= 0.03325\n",
      "Epoch: 0195 train_loss= 0.43440 time= 0.03391\n",
      "Epoch: 0196 train_loss= 0.43415 time= 0.03406\n",
      "Epoch: 0197 train_loss= 0.43390 time= 0.03590\n",
      "Epoch: 0198 train_loss= 0.43366 time= 0.03391\n",
      "Epoch: 0199 train_loss= 0.43342 time= 0.03325\n",
      "Epoch: 0200 train_loss= 0.43320 time= 0.03328\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73512 time= 0.10073\n",
      "Epoch: 0002 train_loss= 0.73481 time= 0.03790\n",
      "Epoch: 0003 train_loss= 0.73389 time= 0.03573\n",
      "Epoch: 0004 train_loss= 0.73188 time= 0.03491\n",
      "Epoch: 0005 train_loss= 0.72820 time= 0.03590\n",
      "Epoch: 0006 train_loss= 0.72222 time= 0.03391\n",
      "Epoch: 0007 train_loss= 0.71344 time= 0.03520\n",
      "Epoch: 0008 train_loss= 0.70186 time= 0.03361\n",
      "Epoch: 0009 train_loss= 0.68873 time= 0.03180\n",
      "Epoch: 0010 train_loss= 0.67763 time= 0.03436\n",
      "Epoch: 0011 train_loss= 0.67438 time= 0.03200\n",
      "Epoch: 0012 train_loss= 0.67825 time= 0.03287\n",
      "Epoch: 0013 train_loss= 0.67686 time= 0.03429\n",
      "Epoch: 0014 train_loss= 0.66934 time= 0.03749\n",
      "Epoch: 0015 train_loss= 0.66109 time= 0.03490\n",
      "Epoch: 0016 train_loss= 0.65555 time= 0.03369\n",
      "Epoch: 0017 train_loss= 0.65279 time= 0.03293\n",
      "Epoch: 0018 train_loss= 0.65110 time= 0.03489\n",
      "Epoch: 0019 train_loss= 0.64884 time= 0.03391\n",
      "Epoch: 0020 train_loss= 0.64522 time= 0.03391\n",
      "Epoch: 0021 train_loss= 0.64022 time= 0.03352\n",
      "Epoch: 0022 train_loss= 0.63432 time= 0.03284\n",
      "Epoch: 0023 train_loss= 0.62825 time= 0.03208\n",
      "Epoch: 0024 train_loss= 0.62266 time= 0.03521\n",
      "Epoch: 0025 train_loss= 0.61773 time= 0.03401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0026 train_loss= 0.61301 time= 0.03195\n",
      "Epoch: 0027 train_loss= 0.60774 time= 0.03481\n",
      "Epoch: 0028 train_loss= 0.60150 time= 0.03413\n",
      "Epoch: 0029 train_loss= 0.59461 time= 0.03239\n",
      "Epoch: 0030 train_loss= 0.58783 time= 0.03403\n",
      "Epoch: 0031 train_loss= 0.58180 time= 0.03591\n",
      "Epoch: 0032 train_loss= 0.57664 time= 0.03427\n",
      "Epoch: 0033 train_loss= 0.57192 time= 0.03322\n",
      "Epoch: 0034 train_loss= 0.56723 time= 0.03429\n",
      "Epoch: 0035 train_loss= 0.56257 time= 0.03067\n",
      "Epoch: 0036 train_loss= 0.55816 time= 0.03453\n",
      "Epoch: 0037 train_loss= 0.55417 time= 0.03496\n",
      "Epoch: 0038 train_loss= 0.55045 time= 0.03373\n",
      "Epoch: 0039 train_loss= 0.54662 time= 0.03312\n",
      "Epoch: 0040 train_loss= 0.54244 time= 0.03289\n",
      "Epoch: 0041 train_loss= 0.53808 time= 0.03142\n",
      "Epoch: 0042 train_loss= 0.53391 time= 0.03332\n",
      "Epoch: 0043 train_loss= 0.53018 time= 0.03641\n",
      "Epoch: 0044 train_loss= 0.52685 time= 0.03456\n",
      "Epoch: 0045 train_loss= 0.52377 time= 0.03315\n",
      "Epoch: 0046 train_loss= 0.52089 time= 0.03406\n",
      "Epoch: 0047 train_loss= 0.51830 time= 0.03420\n",
      "Epoch: 0048 train_loss= 0.51606 time= 0.03591\n",
      "Epoch: 0049 train_loss= 0.51409 time= 0.03290\n",
      "Epoch: 0050 train_loss= 0.51220 time= 0.03413\n",
      "Epoch: 0051 train_loss= 0.51025 time= 0.03240\n",
      "Epoch: 0052 train_loss= 0.50826 time= 0.03106\n",
      "Epoch: 0053 train_loss= 0.50621 time= 0.03560\n",
      "Epoch: 0054 train_loss= 0.50409 time= 0.03491\n",
      "Epoch: 0055 train_loss= 0.50184 time= 0.03291\n",
      "Epoch: 0056 train_loss= 0.49950 time= 0.03499\n",
      "Epoch: 0057 train_loss= 0.49721 time= 0.03324\n",
      "Epoch: 0058 train_loss= 0.49510 time= 0.03366\n",
      "Epoch: 0059 train_loss= 0.49327 time= 0.03265\n",
      "Epoch: 0060 train_loss= 0.49173 time= 0.03131\n",
      "Epoch: 0061 train_loss= 0.49044 time= 0.03292\n",
      "Epoch: 0062 train_loss= 0.48938 time= 0.03313\n",
      "Epoch: 0063 train_loss= 0.48857 time= 0.03348\n",
      "Epoch: 0064 train_loss= 0.48800 time= 0.03215\n",
      "Epoch: 0065 train_loss= 0.48764 time= 0.03490\n",
      "Epoch: 0066 train_loss= 0.48737 time= 0.03270\n",
      "Epoch: 0067 train_loss= 0.48711 time= 0.03314\n",
      "Epoch: 0068 train_loss= 0.48678 time= 0.03307\n",
      "Epoch: 0069 train_loss= 0.48635 time= 0.03223\n",
      "Epoch: 0070 train_loss= 0.48583 time= 0.03320\n",
      "Epoch: 0071 train_loss= 0.48521 time= 0.03291\n",
      "Epoch: 0072 train_loss= 0.48452 time= 0.03291\n",
      "Epoch: 0073 train_loss= 0.48379 time= 0.03378\n",
      "Epoch: 0074 train_loss= 0.48306 time= 0.03404\n",
      "Epoch: 0075 train_loss= 0.48236 time= 0.03375\n",
      "Epoch: 0076 train_loss= 0.48170 time= 0.03197\n",
      "Epoch: 0077 train_loss= 0.48107 time= 0.03313\n",
      "Epoch: 0078 train_loss= 0.48048 time= 0.03458\n",
      "Epoch: 0079 train_loss= 0.47993 time= 0.03928\n",
      "Epoch: 0080 train_loss= 0.47940 time= 0.04267\n",
      "Epoch: 0081 train_loss= 0.47890 time= 0.03445\n",
      "Epoch: 0082 train_loss= 0.47839 time= 0.03536\n",
      "Epoch: 0083 train_loss= 0.47788 time= 0.03617\n",
      "Epoch: 0084 train_loss= 0.47736 time= 0.03469\n",
      "Epoch: 0085 train_loss= 0.47681 time= 0.03887\n",
      "Epoch: 0086 train_loss= 0.47622 time= 0.03754\n",
      "Epoch: 0087 train_loss= 0.47559 time= 0.03690\n",
      "Epoch: 0088 train_loss= 0.47492 time= 0.03690\n",
      "Epoch: 0089 train_loss= 0.47423 time= 0.03490\n",
      "Epoch: 0090 train_loss= 0.47353 time= 0.03491\n",
      "Epoch: 0091 train_loss= 0.47284 time= 0.03491\n",
      "Epoch: 0092 train_loss= 0.47216 time= 0.03690\n",
      "Epoch: 0093 train_loss= 0.47150 time= 0.03387\n",
      "Epoch: 0094 train_loss= 0.47088 time= 0.04061\n",
      "Epoch: 0095 train_loss= 0.47029 time= 0.03584\n",
      "Epoch: 0096 train_loss= 0.46973 time= 0.03590\n",
      "Epoch: 0097 train_loss= 0.46919 time= 0.03491\n",
      "Epoch: 0098 train_loss= 0.46869 time= 0.04189\n",
      "Epoch: 0099 train_loss= 0.46821 time= 0.04488\n",
      "Epoch: 0100 train_loss= 0.46775 time= 0.04901\n",
      "Epoch: 0101 train_loss= 0.46731 time= 0.04987\n",
      "Epoch: 0102 train_loss= 0.46687 time= 0.04288\n",
      "Epoch: 0103 train_loss= 0.46642 time= 0.04588\n",
      "Epoch: 0104 train_loss= 0.46596 time= 0.03590\n",
      "Epoch: 0105 train_loss= 0.46549 time= 0.03679\n",
      "Epoch: 0106 train_loss= 0.46498 time= 0.03391\n",
      "Epoch: 0107 train_loss= 0.46444 time= 0.03491\n",
      "Epoch: 0108 train_loss= 0.46387 time= 0.04588\n",
      "Epoch: 0109 train_loss= 0.46327 time= 0.04473\n",
      "Epoch: 0110 train_loss= 0.46265 time= 0.05386\n",
      "Epoch: 0111 train_loss= 0.46202 time= 0.03971\n",
      "Epoch: 0112 train_loss= 0.46137 time= 0.04089\n",
      "Epoch: 0113 train_loss= 0.46071 time= 0.03690\n",
      "Epoch: 0114 train_loss= 0.46006 time= 0.03820\n",
      "Epoch: 0115 train_loss= 0.45940 time= 0.04189\n",
      "Epoch: 0116 train_loss= 0.45876 time= 0.04155\n",
      "Epoch: 0117 train_loss= 0.45812 time= 0.04487\n",
      "Epoch: 0118 train_loss= 0.45749 time= 0.04494\n",
      "Epoch: 0119 train_loss= 0.45688 time= 0.05281\n",
      "Epoch: 0120 train_loss= 0.45628 time= 0.04524\n",
      "Epoch: 0121 train_loss= 0.45571 time= 0.04317\n",
      "Epoch: 0122 train_loss= 0.45515 time= 0.03670\n",
      "Epoch: 0123 train_loss= 0.45462 time= 0.03690\n",
      "Epoch: 0124 train_loss= 0.45410 time= 0.03590\n",
      "Epoch: 0125 train_loss= 0.45359 time= 0.03890\n",
      "Epoch: 0126 train_loss= 0.45310 time= 0.04189\n",
      "Epoch: 0127 train_loss= 0.45261 time= 0.04787\n",
      "Epoch: 0128 train_loss= 0.45212 time= 0.04928\n",
      "Epoch: 0129 train_loss= 0.45162 time= 0.04048\n",
      "Epoch: 0130 train_loss= 0.45113 time= 0.04388\n",
      "Epoch: 0131 train_loss= 0.45063 time= 0.03854\n",
      "Epoch: 0132 train_loss= 0.45014 time= 0.03690\n",
      "Epoch: 0133 train_loss= 0.44967 time= 0.03591\n",
      "Epoch: 0134 train_loss= 0.44921 time= 0.03790\n",
      "Epoch: 0135 train_loss= 0.44877 time= 0.03990\n",
      "Epoch: 0136 train_loss= 0.44836 time= 0.04495\n",
      "Epoch: 0137 train_loss= 0.44797 time= 0.04490\n",
      "Epoch: 0138 train_loss= 0.44760 time= 0.04949\n",
      "Epoch: 0139 train_loss= 0.44724 time= 0.03948\n",
      "Epoch: 0140 train_loss= 0.44687 time= 0.04285\n",
      "Epoch: 0141 train_loss= 0.44650 time= 0.03799\n",
      "Epoch: 0142 train_loss= 0.44610 time= 0.03298\n",
      "Epoch: 0143 train_loss= 0.44569 time= 0.03580\n",
      "Epoch: 0144 train_loss= 0.44526 time= 0.03896\n",
      "Epoch: 0145 train_loss= 0.44481 time= 0.05159\n",
      "Epoch: 0146 train_loss= 0.44435 time= 0.04289\n",
      "Epoch: 0147 train_loss= 0.44388 time= 0.04288\n",
      "Epoch: 0148 train_loss= 0.44340 time= 0.03790\n",
      "Epoch: 0149 train_loss= 0.44292 time= 0.03790\n",
      "Epoch: 0150 train_loss= 0.44244 time= 0.03589\n",
      "Epoch: 0151 train_loss= 0.44197 time= 0.04031\n",
      "Epoch: 0152 train_loss= 0.44150 time= 0.04409\n",
      "Epoch: 0153 train_loss= 0.44104 time= 0.04268\n",
      "Epoch: 0154 train_loss= 0.44059 time= 0.03700\n",
      "Epoch: 0155 train_loss= 0.44016 time= 0.03554\n",
      "Epoch: 0156 train_loss= 0.43975 time= 0.03590\n",
      "Epoch: 0157 train_loss= 0.43937 time= 0.03372\n",
      "Epoch: 0158 train_loss= 0.43900 time= 0.04189\n",
      "Epoch: 0159 train_loss= 0.43867 time= 0.04289\n",
      "Epoch: 0160 train_loss= 0.43835 time= 0.04188\n",
      "Epoch: 0161 train_loss= 0.43806 time= 0.03790\n",
      "Epoch: 0162 train_loss= 0.43777 time= 0.03790\n",
      "Epoch: 0163 train_loss= 0.43750 time= 0.03690\n",
      "Epoch: 0164 train_loss= 0.43722 time= 0.03591\n",
      "Epoch: 0165 train_loss= 0.43694 time= 0.04488\n",
      "Epoch: 0166 train_loss= 0.43666 time= 0.04114\n",
      "Epoch: 0167 train_loss= 0.43638 time= 0.03590\n",
      "Epoch: 0168 train_loss= 0.43609 time= 0.03890\n",
      "Epoch: 0169 train_loss= 0.43581 time= 0.03590\n",
      "Epoch: 0170 train_loss= 0.43553 time= 0.03504\n",
      "Epoch: 0171 train_loss= 0.43527 time= 0.03977\n",
      "Epoch: 0172 train_loss= 0.43502 time= 0.04388\n",
      "Epoch: 0173 train_loss= 0.43479 time= 0.04588\n",
      "Epoch: 0174 train_loss= 0.43457 time= 0.03690\n",
      "Epoch: 0175 train_loss= 0.43436 time= 0.03797\n",
      "Epoch: 0176 train_loss= 0.43417 time= 0.03483\n",
      "Epoch: 0177 train_loss= 0.43399 time= 0.03591\n",
      "Epoch: 0178 train_loss= 0.43381 time= 0.04505\n",
      "Epoch: 0179 train_loss= 0.43363 time= 0.04557\n",
      "Epoch: 0180 train_loss= 0.43346 time= 0.04907\n",
      "Epoch: 0181 train_loss= 0.43328 time= 0.04288\n",
      "Epoch: 0182 train_loss= 0.43310 time= 0.04102\n",
      "Epoch: 0183 train_loss= 0.43293 time= 0.03585\n",
      "Epoch: 0184 train_loss= 0.43275 time= 0.03781\n",
      "Epoch: 0185 train_loss= 0.43257 time= 0.03690\n",
      "Epoch: 0186 train_loss= 0.43239 time= 0.03805\n",
      "Epoch: 0187 train_loss= 0.43221 time= 0.04089\n",
      "Epoch: 0188 train_loss= 0.43203 time= 0.04788\n",
      "Epoch: 0189 train_loss= 0.43185 time= 0.04850\n",
      "Epoch: 0190 train_loss= 0.43168 time= 0.04333\n",
      "Epoch: 0191 train_loss= 0.43151 time= 0.03776\n",
      "Epoch: 0192 train_loss= 0.43133 time= 0.03488\n",
      "Epoch: 0193 train_loss= 0.43116 time= 0.03859\n",
      "Epoch: 0194 train_loss= 0.43099 time= 0.03495\n",
      "Epoch: 0195 train_loss= 0.43082 time= 0.03576\n",
      "Epoch: 0196 train_loss= 0.43065 time= 0.04491\n",
      "Epoch: 0197 train_loss= 0.43048 time= 0.04787\n",
      "Epoch: 0198 train_loss= 0.43030 time= 0.05186\n",
      "Epoch: 0199 train_loss= 0.43013 time= 0.04288\n",
      "Epoch: 0200 train_loss= 0.42995 time= 0.04548\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73509 time= 0.16507\n",
      "Epoch: 0002 train_loss= 0.73466 time= 0.03614\n",
      "Epoch: 0003 train_loss= 0.73331 time= 0.03491\n",
      "Epoch: 0004 train_loss= 0.73031 time= 0.04289\n",
      "Epoch: 0005 train_loss= 0.72480 time= 0.04188\n",
      "Epoch: 0006 train_loss= 0.71605 time= 0.03741\n",
      "Epoch: 0007 train_loss= 0.70394 time= 0.03690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0008 train_loss= 0.68998 time= 0.03838\n",
      "Epoch: 0009 train_loss= 0.67889 time= 0.03890\n",
      "Epoch: 0010 train_loss= 0.67817 time= 0.04388\n",
      "Epoch: 0011 train_loss= 0.68283 time= 0.03989\n",
      "Epoch: 0012 train_loss= 0.67929 time= 0.04089\n",
      "Epoch: 0013 train_loss= 0.67096 time= 0.03491\n",
      "Epoch: 0014 train_loss= 0.66411 time= 0.03920\n",
      "Epoch: 0015 train_loss= 0.66089 time= 0.03660\n",
      "Epoch: 0016 train_loss= 0.66002 time= 0.03552\n",
      "Epoch: 0017 train_loss= 0.65937 time= 0.04587\n",
      "Epoch: 0018 train_loss= 0.65768 time= 0.05186\n",
      "Epoch: 0019 train_loss= 0.65470 time= 0.04887\n",
      "Epoch: 0020 train_loss= 0.65078 time= 0.04123\n",
      "Epoch: 0021 train_loss= 0.64669 time= 0.04306\n",
      "Epoch: 0022 train_loss= 0.64322 time= 0.03526\n",
      "Epoch: 0023 train_loss= 0.64077 time= 0.03707\n",
      "Epoch: 0024 train_loss= 0.63894 time= 0.03601\n",
      "Epoch: 0025 train_loss= 0.63675 time= 0.03449\n",
      "Epoch: 0026 train_loss= 0.63350 time= 0.04322\n",
      "Epoch: 0027 train_loss= 0.62938 time= 0.04457\n",
      "Epoch: 0028 train_loss= 0.62509 time= 0.04794\n",
      "Epoch: 0029 train_loss= 0.62116 time= 0.03983\n",
      "Epoch: 0030 train_loss= 0.61757 time= 0.04089\n",
      "Epoch: 0031 train_loss= 0.61382 time= 0.03911\n",
      "Epoch: 0032 train_loss= 0.60946 time= 0.03491\n",
      "Epoch: 0033 train_loss= 0.60441 time= 0.03836\n",
      "Epoch: 0034 train_loss= 0.59897 time= 0.04460\n",
      "Epoch: 0035 train_loss= 0.59353 time= 0.04796\n",
      "Epoch: 0036 train_loss= 0.58832 time= 0.04744\n",
      "Epoch: 0037 train_loss= 0.58327 time= 0.04089\n",
      "Epoch: 0038 train_loss= 0.57821 time= 0.04020\n",
      "Epoch: 0039 train_loss= 0.57311 time= 0.04041\n",
      "Epoch: 0040 train_loss= 0.56819 time= 0.03700\n",
      "Epoch: 0041 train_loss= 0.56382 time= 0.03680\n",
      "Epoch: 0042 train_loss= 0.56020 time= 0.03686\n",
      "Epoch: 0043 train_loss= 0.55740 time= 0.04303\n",
      "Epoch: 0044 train_loss= 0.55530 time= 0.04189\n",
      "Epoch: 0045 train_loss= 0.55372 time= 0.05246\n",
      "Epoch: 0046 train_loss= 0.55237 time= 0.04299\n",
      "Epoch: 0047 train_loss= 0.55088 time= 0.03985\n",
      "Epoch: 0048 train_loss= 0.54889 time= 0.03989\n",
      "Epoch: 0049 train_loss= 0.54613 time= 0.04189\n",
      "Epoch: 0050 train_loss= 0.54262 time= 0.03989\n",
      "Epoch: 0051 train_loss= 0.53855 time= 0.03490\n",
      "Epoch: 0052 train_loss= 0.53425 time= 0.04688\n",
      "Epoch: 0053 train_loss= 0.53007 time= 0.04189\n",
      "Epoch: 0054 train_loss= 0.52622 time= 0.04688\n",
      "Epoch: 0055 train_loss= 0.52280 time= 0.04289\n",
      "Epoch: 0056 train_loss= 0.51982 time= 0.04086\n",
      "Epoch: 0057 train_loss= 0.51727 time= 0.03866\n",
      "Epoch: 0058 train_loss= 0.51519 time= 0.03590\n",
      "Epoch: 0059 train_loss= 0.51362 time= 0.03790\n",
      "Epoch: 0060 train_loss= 0.51255 time= 0.03890\n",
      "Epoch: 0061 train_loss= 0.51192 time= 0.04771\n",
      "Epoch: 0062 train_loss= 0.51156 time= 0.04721\n",
      "Epoch: 0063 train_loss= 0.51126 time= 0.04388\n",
      "Epoch: 0064 train_loss= 0.51078 time= 0.03575\n",
      "Epoch: 0065 train_loss= 0.50997 time= 0.03662\n",
      "Epoch: 0066 train_loss= 0.50881 time= 0.03490\n",
      "Epoch: 0067 train_loss= 0.50742 time= 0.03491\n",
      "Epoch: 0068 train_loss= 0.50597 time= 0.04672\n",
      "Epoch: 0069 train_loss= 0.50461 time= 0.04004\n",
      "Epoch: 0070 train_loss= 0.50345 time= 0.03817\n",
      "Epoch: 0071 train_loss= 0.50248 time= 0.03676\n",
      "Epoch: 0072 train_loss= 0.50165 time= 0.03697\n",
      "Epoch: 0073 train_loss= 0.50088 time= 0.03528\n",
      "Epoch: 0074 train_loss= 0.50012 time= 0.03694\n",
      "Epoch: 0075 train_loss= 0.49932 time= 0.04618\n",
      "Epoch: 0076 train_loss= 0.49847 time= 0.04188\n",
      "Epoch: 0077 train_loss= 0.49755 time= 0.03690\n",
      "Epoch: 0078 train_loss= 0.49657 time= 0.03590\n",
      "Epoch: 0079 train_loss= 0.49553 time= 0.03590\n",
      "Epoch: 0080 train_loss= 0.49446 time= 0.03591\n",
      "Epoch: 0081 train_loss= 0.49339 time= 0.04686\n",
      "Epoch: 0082 train_loss= 0.49234 time= 0.04189\n",
      "Epoch: 0083 train_loss= 0.49131 time= 0.04089\n",
      "Epoch: 0084 train_loss= 0.49033 time= 0.03690\n",
      "Epoch: 0085 train_loss= 0.48941 time= 0.03573\n",
      "Epoch: 0086 train_loss= 0.48856 time= 0.03690\n",
      "Epoch: 0087 train_loss= 0.48781 time= 0.04786\n",
      "Epoch: 0088 train_loss= 0.48712 time= 0.03917\n",
      "Epoch: 0089 train_loss= 0.48652 time= 0.03817\n",
      "Epoch: 0090 train_loss= 0.48597 time= 0.03595\n",
      "Epoch: 0091 train_loss= 0.48548 time= 0.03758\n",
      "Epoch: 0092 train_loss= 0.48503 time= 0.03590\n",
      "Epoch: 0093 train_loss= 0.48459 time= 0.03989\n",
      "Epoch: 0094 train_loss= 0.48414 time= 0.04488\n",
      "Epoch: 0095 train_loss= 0.48367 time= 0.04488\n",
      "Epoch: 0096 train_loss= 0.48317 time= 0.04258\n",
      "Epoch: 0097 train_loss= 0.48263 time= 0.04250\n",
      "Epoch: 0098 train_loss= 0.48204 time= 0.04288\n",
      "Epoch: 0099 train_loss= 0.48141 time= 0.03741\n",
      "Epoch: 0100 train_loss= 0.48076 time= 0.03598\n",
      "Epoch: 0101 train_loss= 0.48009 time= 0.03531\n",
      "Epoch: 0102 train_loss= 0.47942 time= 0.03991\n",
      "Epoch: 0103 train_loss= 0.47875 time= 0.04486\n",
      "Epoch: 0104 train_loss= 0.47809 time= 0.05155\n",
      "Epoch: 0105 train_loss= 0.47742 time= 0.04068\n",
      "Epoch: 0106 train_loss= 0.47674 time= 0.04506\n",
      "Epoch: 0107 train_loss= 0.47606 time= 0.04090\n",
      "Epoch: 0108 train_loss= 0.47536 time= 0.04188\n",
      "Epoch: 0109 train_loss= 0.47464 time= 0.03631\n",
      "Epoch: 0110 train_loss= 0.47389 time= 0.03450\n",
      "Epoch: 0111 train_loss= 0.47313 time= 0.04019\n",
      "Epoch: 0112 train_loss= 0.47235 time= 0.04160\n",
      "Epoch: 0113 train_loss= 0.47155 time= 0.04688\n",
      "Epoch: 0114 train_loss= 0.47075 time= 0.04288\n",
      "Epoch: 0115 train_loss= 0.46995 time= 0.04389\n",
      "Epoch: 0116 train_loss= 0.46917 time= 0.03960\n",
      "Epoch: 0117 train_loss= 0.46841 time= 0.03613\n",
      "Epoch: 0118 train_loss= 0.46770 time= 0.03481\n",
      "Epoch: 0119 train_loss= 0.46702 time= 0.03687\n",
      "Epoch: 0120 train_loss= 0.46638 time= 0.04128\n",
      "Epoch: 0121 train_loss= 0.46577 time= 0.04141\n",
      "Epoch: 0122 train_loss= 0.46518 time= 0.04939\n",
      "Epoch: 0123 train_loss= 0.46460 time= 0.04194\n",
      "Epoch: 0124 train_loss= 0.46401 time= 0.04062\n",
      "Epoch: 0125 train_loss= 0.46340 time= 0.03790\n",
      "Epoch: 0126 train_loss= 0.46277 time= 0.03690\n",
      "Epoch: 0127 train_loss= 0.46211 time= 0.03890\n",
      "Epoch: 0128 train_loss= 0.46144 time= 0.03989\n",
      "Epoch: 0129 train_loss= 0.46078 time= 0.05186\n",
      "Epoch: 0130 train_loss= 0.46013 time= 0.04289\n",
      "Epoch: 0131 train_loss= 0.45951 time= 0.04727\n",
      "Epoch: 0132 train_loss= 0.45893 time= 0.04239\n",
      "Epoch: 0133 train_loss= 0.45840 time= 0.04189\n",
      "Epoch: 0134 train_loss= 0.45792 time= 0.03690\n",
      "Epoch: 0135 train_loss= 0.45749 time= 0.03990\n",
      "Epoch: 0136 train_loss= 0.45710 time= 0.03789\n",
      "Epoch: 0137 train_loss= 0.45673 time= 0.03468\n",
      "Epoch: 0138 train_loss= 0.45639 time= 0.04030\n",
      "Epoch: 0139 train_loss= 0.45605 time= 0.04675\n",
      "Epoch: 0140 train_loss= 0.45570 time= 0.04189\n",
      "Epoch: 0141 train_loss= 0.45535 time= 0.03590\n",
      "Epoch: 0142 train_loss= 0.45497 time= 0.03690\n",
      "Epoch: 0143 train_loss= 0.45459 time= 0.03563\n",
      "Epoch: 0144 train_loss= 0.45419 time= 0.03375\n",
      "Epoch: 0145 train_loss= 0.45379 time= 0.04189\n",
      "Epoch: 0146 train_loss= 0.45338 time= 0.04289\n",
      "Epoch: 0147 train_loss= 0.45297 time= 0.03690\n",
      "Epoch: 0148 train_loss= 0.45257 time= 0.03690\n",
      "Epoch: 0149 train_loss= 0.45217 time= 0.03690\n",
      "Epoch: 0150 train_loss= 0.45178 time= 0.03690\n",
      "Epoch: 0151 train_loss= 0.45139 time= 0.04013\n",
      "Epoch: 0152 train_loss= 0.45101 time= 0.04423\n",
      "Epoch: 0153 train_loss= 0.45063 time= 0.03989\n",
      "Epoch: 0154 train_loss= 0.45026 time= 0.03690\n",
      "Epoch: 0155 train_loss= 0.44990 time= 0.03741\n",
      "Epoch: 0156 train_loss= 0.44955 time= 0.03529\n",
      "Epoch: 0157 train_loss= 0.44922 time= 0.03590\n",
      "Epoch: 0158 train_loss= 0.44890 time= 0.04674\n",
      "Epoch: 0159 train_loss= 0.44860 time= 0.04046\n",
      "Epoch: 0160 train_loss= 0.44832 time= 0.03459\n",
      "Epoch: 0161 train_loss= 0.44805 time= 0.03590\n",
      "Epoch: 0162 train_loss= 0.44780 time= 0.03790\n",
      "Epoch: 0163 train_loss= 0.44757 time= 0.03630\n",
      "Epoch: 0164 train_loss= 0.44734 time= 0.04089\n",
      "Epoch: 0165 train_loss= 0.44712 time= 0.03890\n",
      "Epoch: 0166 train_loss= 0.44689 time= 0.03838\n",
      "Epoch: 0167 train_loss= 0.44667 time= 0.03724\n",
      "Epoch: 0168 train_loss= 0.44644 time= 0.03384\n",
      "Epoch: 0169 train_loss= 0.44621 time= 0.03613\n",
      "Epoch: 0170 train_loss= 0.44597 time= 0.03810\n",
      "Epoch: 0171 train_loss= 0.44572 time= 0.04386\n",
      "Epoch: 0172 train_loss= 0.44547 time= 0.04216\n",
      "Epoch: 0173 train_loss= 0.44522 time= 0.04562\n",
      "Epoch: 0174 train_loss= 0.44496 time= 0.04360\n",
      "Epoch: 0175 train_loss= 0.44471 time= 0.03925\n",
      "Epoch: 0176 train_loss= 0.44445 time= 0.03396\n",
      "Epoch: 0177 train_loss= 0.44419 time= 0.03889\n",
      "Epoch: 0178 train_loss= 0.44394 time= 0.04161\n",
      "Epoch: 0179 train_loss= 0.44368 time= 0.03890\n",
      "Epoch: 0180 train_loss= 0.44341 time= 0.04488\n",
      "Epoch: 0181 train_loss= 0.44315 time= 0.04887\n",
      "Epoch: 0182 train_loss= 0.44288 time= 0.04278\n",
      "Epoch: 0183 train_loss= 0.44261 time= 0.04388\n",
      "Epoch: 0184 train_loss= 0.44234 time= 0.03790\n",
      "Epoch: 0185 train_loss= 0.44206 time= 0.03990\n",
      "Epoch: 0186 train_loss= 0.44178 time= 0.03900\n",
      "Epoch: 0187 train_loss= 0.44151 time= 0.03481\n",
      "Epoch: 0188 train_loss= 0.44123 time= 0.04986\n",
      "Epoch: 0189 train_loss= 0.44095 time= 0.04488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0190 train_loss= 0.44068 time= 0.04900\n",
      "Epoch: 0191 train_loss= 0.44041 time= 0.04076\n",
      "Epoch: 0192 train_loss= 0.44014 time= 0.03790\n",
      "Epoch: 0193 train_loss= 0.43988 time= 0.03619\n",
      "Epoch: 0194 train_loss= 0.43962 time= 0.03591\n",
      "Epoch: 0195 train_loss= 0.43936 time= 0.03690\n",
      "Epoch: 0196 train_loss= 0.43912 time= 0.03491\n",
      "Epoch: 0197 train_loss= 0.43887 time= 0.03890\n",
      "Epoch: 0198 train_loss= 0.43863 time= 0.04389\n",
      "Epoch: 0199 train_loss= 0.43840 time= 0.04400\n",
      "Epoch: 0200 train_loss= 0.43816 time= 0.04354\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73514 time= 0.17652\n",
      "Epoch: 0002 train_loss= 0.73491 time= 0.04756\n",
      "Epoch: 0003 train_loss= 0.73422 time= 0.04074\n",
      "Epoch: 0004 train_loss= 0.73268 time= 0.03502\n",
      "Epoch: 0005 train_loss= 0.72977 time= 0.03617\n",
      "Epoch: 0006 train_loss= 0.72489 time= 0.03863\n",
      "Epoch: 0007 train_loss= 0.71747 time= 0.03989\n",
      "Epoch: 0008 train_loss= 0.70724 time= 0.04623\n",
      "Epoch: 0009 train_loss= 0.69478 time= 0.04787\n",
      "Epoch: 0010 train_loss= 0.68250 time= 0.05585\n",
      "Epoch: 0011 train_loss= 0.67562 time= 0.04189\n",
      "Epoch: 0012 train_loss= 0.67769 time= 0.03696\n",
      "Epoch: 0013 train_loss= 0.67864 time= 0.03984\n",
      "Epoch: 0014 train_loss= 0.67264 time= 0.03790\n",
      "Epoch: 0015 train_loss= 0.66391 time= 0.03829\n",
      "Epoch: 0016 train_loss= 0.65671 time= 0.04350\n",
      "Epoch: 0017 train_loss= 0.65215 time= 0.04388\n",
      "Epoch: 0018 train_loss= 0.64905 time= 0.04931\n",
      "Epoch: 0019 train_loss= 0.64572 time= 0.04189\n",
      "Epoch: 0020 train_loss= 0.64111 time= 0.04288\n",
      "Epoch: 0021 train_loss= 0.63493 time= 0.03690\n",
      "Epoch: 0022 train_loss= 0.62745 time= 0.03690\n",
      "Epoch: 0023 train_loss= 0.61921 time= 0.03890\n",
      "Epoch: 0024 train_loss= 0.61078 time= 0.03890\n",
      "Epoch: 0025 train_loss= 0.60254 time= 0.04289\n",
      "Epoch: 0026 train_loss= 0.59456 time= 0.04488\n",
      "Epoch: 0027 train_loss= 0.58677 time= 0.05173\n",
      "Epoch: 0028 train_loss= 0.57944 time= 0.03974\n",
      "Epoch: 0029 train_loss= 0.57329 time= 0.03989\n",
      "Epoch: 0030 train_loss= 0.56908 time= 0.03698\n",
      "Epoch: 0031 train_loss= 0.56686 time= 0.03443\n",
      "Epoch: 0032 train_loss= 0.56551 time= 0.03491\n",
      "Epoch: 0033 train_loss= 0.56365 time= 0.03391\n",
      "Epoch: 0034 train_loss= 0.56062 time= 0.04402\n",
      "Epoch: 0035 train_loss= 0.55661 time= 0.04375\n",
      "Epoch: 0036 train_loss= 0.55221 time= 0.05186\n",
      "Epoch: 0037 train_loss= 0.54793 time= 0.03951\n",
      "Epoch: 0038 train_loss= 0.54418 time= 0.04795\n",
      "Epoch: 0039 train_loss= 0.54121 time= 0.03790\n",
      "Epoch: 0040 train_loss= 0.53913 time= 0.04108\n",
      "Epoch: 0041 train_loss= 0.53792 time= 0.03824\n",
      "Epoch: 0042 train_loss= 0.53739 time= 0.03790\n",
      "Epoch: 0043 train_loss= 0.53713 time= 0.04189\n",
      "Epoch: 0044 train_loss= 0.53671 time= 0.04488\n",
      "Epoch: 0045 train_loss= 0.53582 time= 0.05446\n",
      "Epoch: 0046 train_loss= 0.53442 time= 0.04044\n",
      "Epoch: 0047 train_loss= 0.53263 time= 0.04145\n",
      "Epoch: 0048 train_loss= 0.53064 time= 0.03709\n",
      "Epoch: 0049 train_loss= 0.52855 time= 0.03591\n",
      "Epoch: 0050 train_loss= 0.52644 time= 0.03428\n",
      "Epoch: 0051 train_loss= 0.52430 time= 0.03835\n",
      "Epoch: 0052 train_loss= 0.52214 time= 0.04572\n",
      "Epoch: 0053 train_loss= 0.52001 time= 0.03989\n",
      "Epoch: 0054 train_loss= 0.51790 time= 0.04388\n",
      "Epoch: 0055 train_loss= 0.51577 time= 0.03790\n",
      "Epoch: 0056 train_loss= 0.51359 time= 0.03591\n",
      "Epoch: 0057 train_loss= 0.51141 time= 0.03584\n",
      "Epoch: 0058 train_loss= 0.50935 time= 0.03590\n",
      "Epoch: 0059 train_loss= 0.50759 time= 0.04388\n",
      "Epoch: 0060 train_loss= 0.50627 time= 0.04388\n",
      "Epoch: 0061 train_loss= 0.50540 time= 0.03590\n",
      "Epoch: 0062 train_loss= 0.50490 time= 0.03790\n",
      "Epoch: 0063 train_loss= 0.50462 time= 0.03610\n",
      "Epoch: 0064 train_loss= 0.50440 time= 0.03571\n",
      "Epoch: 0065 train_loss= 0.50412 time= 0.04089\n",
      "Epoch: 0066 train_loss= 0.50365 time= 0.04388\n",
      "Epoch: 0067 train_loss= 0.50294 time= 0.03882\n",
      "Epoch: 0068 train_loss= 0.50200 time= 0.03690\n",
      "Epoch: 0069 train_loss= 0.50090 time= 0.03690\n",
      "Epoch: 0070 train_loss= 0.49974 time= 0.03690\n",
      "Epoch: 0071 train_loss= 0.49862 time= 0.04189\n",
      "Epoch: 0072 train_loss= 0.49761 time= 0.04389\n",
      "Epoch: 0073 train_loss= 0.49670 time= 0.04388\n",
      "Epoch: 0074 train_loss= 0.49588 time= 0.03633\n",
      "Epoch: 0075 train_loss= 0.49509 time= 0.03548\n",
      "Epoch: 0076 train_loss= 0.49429 time= 0.03889\n",
      "Epoch: 0077 train_loss= 0.49348 time= 0.03790\n",
      "Epoch: 0078 train_loss= 0.49263 time= 0.04788\n",
      "Epoch: 0079 train_loss= 0.49174 time= 0.04141\n",
      "Epoch: 0080 train_loss= 0.49082 time= 0.04129\n",
      "Epoch: 0081 train_loss= 0.48986 time= 0.03548\n",
      "Epoch: 0082 train_loss= 0.48888 time= 0.03693\n",
      "Epoch: 0083 train_loss= 0.48788 time= 0.03491\n",
      "Epoch: 0084 train_loss= 0.48689 time= 0.03890\n",
      "Epoch: 0085 train_loss= 0.48591 time= 0.04687\n",
      "Epoch: 0086 train_loss= 0.48498 time= 0.04677\n",
      "Epoch: 0087 train_loss= 0.48411 time= 0.04787\n",
      "Epoch: 0088 train_loss= 0.48333 time= 0.03990\n",
      "Epoch: 0089 train_loss= 0.48264 time= 0.03877\n",
      "Epoch: 0090 train_loss= 0.48202 time= 0.03690\n",
      "Epoch: 0091 train_loss= 0.48145 time= 0.03746\n",
      "Epoch: 0092 train_loss= 0.48093 time= 0.03834\n",
      "Epoch: 0093 train_loss= 0.48044 time= 0.04388\n",
      "Epoch: 0094 train_loss= 0.47998 time= 0.04862\n",
      "Epoch: 0095 train_loss= 0.47951 time= 0.04787\n",
      "Epoch: 0096 train_loss= 0.47900 time= 0.04787\n",
      "Epoch: 0097 train_loss= 0.47842 time= 0.04365\n",
      "Epoch: 0098 train_loss= 0.47779 time= 0.03725\n",
      "Epoch: 0099 train_loss= 0.47710 time= 0.03713\n",
      "Epoch: 0100 train_loss= 0.47637 time= 0.03938\n",
      "Epoch: 0101 train_loss= 0.47561 time= 0.03566\n",
      "Epoch: 0102 train_loss= 0.47481 time= 0.04089\n",
      "Epoch: 0103 train_loss= 0.47399 time= 0.04488\n",
      "Epoch: 0104 train_loss= 0.47316 time= 0.04968\n",
      "Epoch: 0105 train_loss= 0.47232 time= 0.04488\n",
      "Epoch: 0106 train_loss= 0.47149 time= 0.04212\n",
      "Epoch: 0107 train_loss= 0.47065 time= 0.03881\n",
      "Epoch: 0108 train_loss= 0.46980 time= 0.04022\n",
      "Epoch: 0109 train_loss= 0.46895 time= 0.03632\n",
      "Epoch: 0110 train_loss= 0.46809 time= 0.03690\n",
      "Epoch: 0111 train_loss= 0.46724 time= 0.04687\n",
      "Epoch: 0112 train_loss= 0.46640 time= 0.04687\n",
      "Epoch: 0113 train_loss= 0.46557 time= 0.04820\n",
      "Epoch: 0114 train_loss= 0.46476 time= 0.03788\n",
      "Epoch: 0115 train_loss= 0.46399 time= 0.04089\n",
      "Epoch: 0116 train_loss= 0.46326 time= 0.03590\n",
      "Epoch: 0117 train_loss= 0.46257 time= 0.03804\n",
      "Epoch: 0118 train_loss= 0.46194 time= 0.03887\n",
      "Epoch: 0119 train_loss= 0.46134 time= 0.04403\n",
      "Epoch: 0120 train_loss= 0.46080 time= 0.05072\n",
      "Epoch: 0121 train_loss= 0.46030 time= 0.04288\n",
      "Epoch: 0122 train_loss= 0.45984 time= 0.04687\n",
      "Epoch: 0123 train_loss= 0.45941 time= 0.03989\n",
      "Epoch: 0124 train_loss= 0.45903 time= 0.04549\n",
      "Epoch: 0125 train_loss= 0.45868 time= 0.03558\n",
      "Epoch: 0126 train_loss= 0.45836 time= 0.03524\n",
      "Epoch: 0127 train_loss= 0.45806 time= 0.03656\n",
      "Epoch: 0128 train_loss= 0.45777 time= 0.03491\n",
      "Epoch: 0129 train_loss= 0.45747 time= 0.04289\n",
      "Epoch: 0130 train_loss= 0.45717 time= 0.04488\n",
      "Epoch: 0131 train_loss= 0.45684 time= 0.04002\n",
      "Epoch: 0132 train_loss= 0.45650 time= 0.03788\n",
      "Epoch: 0133 train_loss= 0.45614 time= 0.03790\n",
      "Epoch: 0134 train_loss= 0.45578 time= 0.03690\n",
      "Epoch: 0135 train_loss= 0.45541 time= 0.03826\n",
      "Epoch: 0136 train_loss= 0.45506 time= 0.04499\n",
      "Epoch: 0137 train_loss= 0.45472 time= 0.04178\n",
      "Epoch: 0138 train_loss= 0.45440 time= 0.03491\n",
      "Epoch: 0139 train_loss= 0.45408 time= 0.03589\n",
      "Epoch: 0140 train_loss= 0.45377 time= 0.03610\n",
      "Epoch: 0141 train_loss= 0.45346 time= 0.03378\n",
      "Epoch: 0142 train_loss= 0.45316 time= 0.04146\n",
      "Epoch: 0143 train_loss= 0.45284 time= 0.04189\n",
      "Epoch: 0144 train_loss= 0.45253 time= 0.04225\n",
      "Epoch: 0145 train_loss= 0.45220 time= 0.03697\n",
      "Epoch: 0146 train_loss= 0.45187 time= 0.03895\n",
      "Epoch: 0147 train_loss= 0.45153 time= 0.03811\n",
      "Epoch: 0148 train_loss= 0.45118 time= 0.04082\n",
      "Epoch: 0149 train_loss= 0.45083 time= 0.03911\n",
      "Epoch: 0150 train_loss= 0.45047 time= 0.03853\n",
      "Epoch: 0151 train_loss= 0.45010 time= 0.03690\n",
      "Epoch: 0152 train_loss= 0.44973 time= 0.03621\n",
      "Epoch: 0153 train_loss= 0.44935 time= 0.03959\n",
      "Epoch: 0154 train_loss= 0.44897 time= 0.04289\n",
      "Epoch: 0155 train_loss= 0.44859 time= 0.04196\n",
      "Epoch: 0156 train_loss= 0.44820 time= 0.04581\n",
      "Epoch: 0157 train_loss= 0.44780 time= 0.03737\n",
      "Epoch: 0158 train_loss= 0.44741 time= 0.03686\n",
      "Epoch: 0159 train_loss= 0.44702 time= 0.03756\n",
      "Epoch: 0160 train_loss= 0.44663 time= 0.03634\n",
      "Epoch: 0161 train_loss= 0.44625 time= 0.04536\n",
      "Epoch: 0162 train_loss= 0.44589 time= 0.04787\n",
      "Epoch: 0163 train_loss= 0.44553 time= 0.05086\n",
      "Epoch: 0164 train_loss= 0.44519 time= 0.03990\n",
      "Epoch: 0165 train_loss= 0.44486 time= 0.03889\n",
      "Epoch: 0166 train_loss= 0.44455 time= 0.03790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0167 train_loss= 0.44424 time= 0.03890\n",
      "Epoch: 0168 train_loss= 0.44394 time= 0.03790\n",
      "Epoch: 0169 train_loss= 0.44365 time= 0.04238\n",
      "Epoch: 0170 train_loss= 0.44335 time= 0.04289\n",
      "Epoch: 0171 train_loss= 0.44304 time= 0.04288\n",
      "Epoch: 0172 train_loss= 0.44272 time= 0.04488\n",
      "Epoch: 0173 train_loss= 0.44240 time= 0.04189\n",
      "Epoch: 0174 train_loss= 0.44205 time= 0.04358\n",
      "Epoch: 0175 train_loss= 0.44170 time= 0.03590\n",
      "Epoch: 0176 train_loss= 0.44134 time= 0.03690\n",
      "Epoch: 0177 train_loss= 0.44098 time= 0.03817\n",
      "Epoch: 0178 train_loss= 0.44061 time= 0.04461\n",
      "Epoch: 0179 train_loss= 0.44024 time= 0.05186\n",
      "Epoch: 0180 train_loss= 0.43988 time= 0.05386\n",
      "Epoch: 0181 train_loss= 0.43952 time= 0.04189\n",
      "Epoch: 0182 train_loss= 0.43917 time= 0.04364\n",
      "Epoch: 0183 train_loss= 0.43883 time= 0.04319\n",
      "Epoch: 0184 train_loss= 0.43850 time= 0.03949\n",
      "Epoch: 0185 train_loss= 0.43818 time= 0.03989\n",
      "Epoch: 0186 train_loss= 0.43788 time= 0.03639\n",
      "Epoch: 0187 train_loss= 0.43758 time= 0.04488\n",
      "Epoch: 0188 train_loss= 0.43730 time= 0.04688\n",
      "Epoch: 0189 train_loss= 0.43702 time= 0.05429\n",
      "Epoch: 0190 train_loss= 0.43676 time= 0.04661\n",
      "Epoch: 0191 train_loss= 0.43651 time= 0.04188\n",
      "Epoch: 0192 train_loss= 0.43626 time= 0.04000\n",
      "Epoch: 0193 train_loss= 0.43602 time= 0.03780\n",
      "Epoch: 0194 train_loss= 0.43579 time= 0.03690\n",
      "Epoch: 0195 train_loss= 0.43556 time= 0.03865\n",
      "Epoch: 0196 train_loss= 0.43534 time= 0.04902\n",
      "Epoch: 0197 train_loss= 0.43512 time= 0.04831\n",
      "Epoch: 0198 train_loss= 0.43491 time= 0.04991\n",
      "Epoch: 0199 train_loss= 0.43470 time= 0.04446\n",
      "Epoch: 0200 train_loss= 0.43449 time= 0.04401\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73507 time= 0.21343\n",
      "Epoch: 0002 train_loss= 0.73440 time= 0.03591\n",
      "Epoch: 0003 train_loss= 0.73248 time= 0.03690\n",
      "Epoch: 0004 train_loss= 0.72846 time= 0.03690\n",
      "Epoch: 0005 train_loss= 0.72151 time= 0.04788\n",
      "Epoch: 0006 train_loss= 0.71108 time= 0.04787\n",
      "Epoch: 0007 train_loss= 0.69766 time= 0.04587\n",
      "Epoch: 0008 train_loss= 0.68399 time= 0.04508\n",
      "Epoch: 0009 train_loss= 0.67639 time= 0.04687\n",
      "Epoch: 0010 train_loss= 0.67999 time= 0.03952\n",
      "Epoch: 0011 train_loss= 0.68184 time= 0.03690\n",
      "Epoch: 0012 train_loss= 0.67563 time= 0.04034\n",
      "Epoch: 0013 train_loss= 0.66743 time= 0.04244\n",
      "Epoch: 0014 train_loss= 0.66202 time= 0.04787\n",
      "Epoch: 0015 train_loss= 0.65995 time= 0.04588\n",
      "Epoch: 0016 train_loss= 0.65934 time= 0.05148\n",
      "Epoch: 0017 train_loss= 0.65836 time= 0.04483\n",
      "Epoch: 0018 train_loss= 0.65614 time= 0.03954\n",
      "Epoch: 0019 train_loss= 0.65266 time= 0.04163\n",
      "Epoch: 0020 train_loss= 0.64845 time= 0.03866\n",
      "Epoch: 0021 train_loss= 0.64431 time= 0.03696\n",
      "Epoch: 0022 train_loss= 0.64097 time= 0.04383\n",
      "Epoch: 0023 train_loss= 0.63866 time= 0.04588\n",
      "Epoch: 0024 train_loss= 0.63675 time= 0.05186\n",
      "Epoch: 0025 train_loss= 0.63424 time= 0.04189\n",
      "Epoch: 0026 train_loss= 0.63067 time= 0.04189\n",
      "Epoch: 0027 train_loss= 0.62644 time= 0.04289\n",
      "Epoch: 0028 train_loss= 0.62235 time= 0.03839\n",
      "Epoch: 0029 train_loss= 0.61887 time= 0.03491\n",
      "Epoch: 0030 train_loss= 0.61581 time= 0.03490\n",
      "Epoch: 0031 train_loss= 0.61258 time= 0.04157\n",
      "Epoch: 0032 train_loss= 0.60883 time= 0.04844\n",
      "Epoch: 0033 train_loss= 0.60470 time= 0.05325\n",
      "Epoch: 0034 train_loss= 0.60061 time= 0.04015\n",
      "Epoch: 0035 train_loss= 0.59697 time= 0.04380\n",
      "Epoch: 0036 train_loss= 0.59384 time= 0.04089\n",
      "Epoch: 0037 train_loss= 0.59091 time= 0.03790\n",
      "Epoch: 0038 train_loss= 0.58782 time= 0.04289\n",
      "Epoch: 0039 train_loss= 0.58444 time= 0.04189\n",
      "Epoch: 0040 train_loss= 0.58082 time= 0.04588\n",
      "Epoch: 0041 train_loss= 0.57700 time= 0.04388\n",
      "Epoch: 0042 train_loss= 0.57294 time= 0.04189\n",
      "Epoch: 0043 train_loss= 0.56860 time= 0.04089\n",
      "Epoch: 0044 train_loss= 0.56400 time= 0.03690\n",
      "Epoch: 0045 train_loss= 0.55928 time= 0.03790\n",
      "Epoch: 0046 train_loss= 0.55463 time= 0.03576\n",
      "Epoch: 0047 train_loss= 0.55024 time= 0.04488\n",
      "Epoch: 0048 train_loss= 0.54623 time= 0.03990\n",
      "Epoch: 0049 train_loss= 0.54266 time= 0.03796\n",
      "Epoch: 0050 train_loss= 0.53948 time= 0.03716\n",
      "Epoch: 0051 train_loss= 0.53657 time= 0.03557\n",
      "Epoch: 0052 train_loss= 0.53381 time= 0.03642\n",
      "Epoch: 0053 train_loss= 0.53109 time= 0.04288\n",
      "Epoch: 0054 train_loss= 0.52837 time= 0.04388\n",
      "Epoch: 0055 train_loss= 0.52567 time= 0.04288\n",
      "Epoch: 0056 train_loss= 0.52304 time= 0.03690\n",
      "Epoch: 0057 train_loss= 0.52053 time= 0.03584\n",
      "Epoch: 0058 train_loss= 0.51810 time= 0.04149\n",
      "Epoch: 0059 train_loss= 0.51574 time= 0.03884\n",
      "Epoch: 0060 train_loss= 0.51341 time= 0.04682\n",
      "Epoch: 0061 train_loss= 0.51114 time= 0.04671\n",
      "Epoch: 0062 train_loss= 0.50899 time= 0.04048\n",
      "Epoch: 0063 train_loss= 0.50701 time= 0.04069\n",
      "Epoch: 0064 train_loss= 0.50520 time= 0.03690\n",
      "Epoch: 0065 train_loss= 0.50355 time= 0.03690\n",
      "Epoch: 0066 train_loss= 0.50203 time= 0.05147\n",
      "Epoch: 0067 train_loss= 0.50062 time= 0.04335\n",
      "Epoch: 0068 train_loss= 0.49924 time= 0.04189\n",
      "Epoch: 0069 train_loss= 0.49778 time= 0.03848\n",
      "Epoch: 0070 train_loss= 0.49612 time= 0.03690\n",
      "Epoch: 0071 train_loss= 0.49423 time= 0.03612\n",
      "Epoch: 0072 train_loss= 0.49216 time= 0.03714\n",
      "Epoch: 0073 train_loss= 0.49003 time= 0.04488\n",
      "Epoch: 0074 train_loss= 0.48794 time= 0.04687\n",
      "Epoch: 0075 train_loss= 0.48596 time= 0.04588\n",
      "Epoch: 0076 train_loss= 0.48413 time= 0.04388\n",
      "Epoch: 0077 train_loss= 0.48249 time= 0.04289\n",
      "Epoch: 0078 train_loss= 0.48104 time= 0.03698\n",
      "Epoch: 0079 train_loss= 0.47982 time= 0.04081\n",
      "Epoch: 0080 train_loss= 0.47880 time= 0.03989\n",
      "Epoch: 0081 train_loss= 0.47794 time= 0.04089\n",
      "Epoch: 0082 train_loss= 0.47720 time= 0.04320\n",
      "Epoch: 0083 train_loss= 0.47650 time= 0.04571\n",
      "Epoch: 0084 train_loss= 0.47577 time= 0.05054\n",
      "Epoch: 0085 train_loss= 0.47497 time= 0.04180\n",
      "Epoch: 0086 train_loss= 0.47412 time= 0.03922\n",
      "Epoch: 0087 train_loss= 0.47325 time= 0.03598\n",
      "Epoch: 0088 train_loss= 0.47240 time= 0.03721\n",
      "Epoch: 0089 train_loss= 0.47157 time= 0.03596\n",
      "Epoch: 0090 train_loss= 0.47080 time= 0.03485\n",
      "Epoch: 0091 train_loss= 0.47009 time= 0.04787\n",
      "Epoch: 0092 train_loss= 0.46946 time= 0.04603\n",
      "Epoch: 0093 train_loss= 0.46891 time= 0.04972\n",
      "Epoch: 0094 train_loss= 0.46841 time= 0.04189\n",
      "Epoch: 0095 train_loss= 0.46793 time= 0.04189\n",
      "Epoch: 0096 train_loss= 0.46743 time= 0.03989\n",
      "Epoch: 0097 train_loss= 0.46690 time= 0.03845\n",
      "Epoch: 0098 train_loss= 0.46636 time= 0.03590\n",
      "Epoch: 0099 train_loss= 0.46578 time= 0.03812\n",
      "Epoch: 0100 train_loss= 0.46517 time= 0.04306\n",
      "Epoch: 0101 train_loss= 0.46455 time= 0.04887\n",
      "Epoch: 0102 train_loss= 0.46393 time= 0.05286\n",
      "Epoch: 0103 train_loss= 0.46333 time= 0.04389\n",
      "Epoch: 0104 train_loss= 0.46276 time= 0.04330\n",
      "Epoch: 0105 train_loss= 0.46223 time= 0.04023\n",
      "Epoch: 0106 train_loss= 0.46176 time= 0.03580\n",
      "Epoch: 0107 train_loss= 0.46133 time= 0.03894\n",
      "Epoch: 0108 train_loss= 0.46095 time= 0.04042\n",
      "Epoch: 0109 train_loss= 0.46062 time= 0.04510\n",
      "Epoch: 0110 train_loss= 0.46033 time= 0.04589\n",
      "Epoch: 0111 train_loss= 0.46006 time= 0.05305\n",
      "Epoch: 0112 train_loss= 0.45982 time= 0.03833\n",
      "Epoch: 0113 train_loss= 0.45957 time= 0.04288\n",
      "Epoch: 0114 train_loss= 0.45933 time= 0.03695\n",
      "Epoch: 0115 train_loss= 0.45907 time= 0.03527\n",
      "Epoch: 0116 train_loss= 0.45880 time= 0.03526\n",
      "Epoch: 0117 train_loss= 0.45852 time= 0.03637\n",
      "Epoch: 0118 train_loss= 0.45822 time= 0.04522\n",
      "Epoch: 0119 train_loss= 0.45792 time= 0.04316\n",
      "Epoch: 0120 train_loss= 0.45763 time= 0.04035\n",
      "Epoch: 0121 train_loss= 0.45733 time= 0.03990\n",
      "Epoch: 0122 train_loss= 0.45704 time= 0.03791\n",
      "Epoch: 0123 train_loss= 0.45677 time= 0.03705\n",
      "Epoch: 0124 train_loss= 0.45651 time= 0.03790\n",
      "Epoch: 0125 train_loss= 0.45625 time= 0.04487\n",
      "Epoch: 0126 train_loss= 0.45601 time= 0.03989\n",
      "Epoch: 0127 train_loss= 0.45577 time= 0.03761\n",
      "Epoch: 0128 train_loss= 0.45554 time= 0.03585\n",
      "Epoch: 0129 train_loss= 0.45531 time= 0.03618\n",
      "Epoch: 0130 train_loss= 0.45507 time= 0.03745\n",
      "Epoch: 0131 train_loss= 0.45483 time= 0.04775\n",
      "Epoch: 0132 train_loss= 0.45459 time= 0.04118\n",
      "Epoch: 0133 train_loss= 0.45433 time= 0.03992\n",
      "Epoch: 0134 train_loss= 0.45407 time= 0.03769\n",
      "Epoch: 0135 train_loss= 0.45381 time= 0.03507\n",
      "Epoch: 0136 train_loss= 0.45354 time= 0.03429\n",
      "Epoch: 0137 train_loss= 0.45327 time= 0.04427\n",
      "Epoch: 0138 train_loss= 0.45299 time= 0.04096\n",
      "Epoch: 0139 train_loss= 0.45271 time= 0.03873\n",
      "Epoch: 0140 train_loss= 0.45242 time= 0.03590\n",
      "Epoch: 0141 train_loss= 0.45213 time= 0.03680\n",
      "Epoch: 0142 train_loss= 0.45184 time= 0.03690\n",
      "Epoch: 0143 train_loss= 0.45154 time= 0.04488\n",
      "Epoch: 0144 train_loss= 0.45124 time= 0.04488\n",
      "Epoch: 0145 train_loss= 0.45093 time= 0.04994\n",
      "Epoch: 0146 train_loss= 0.45061 time= 0.04260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0147 train_loss= 0.45030 time= 0.04367\n",
      "Epoch: 0148 train_loss= 0.44997 time= 0.04378\n",
      "Epoch: 0149 train_loss= 0.44965 time= 0.03968\n",
      "Epoch: 0150 train_loss= 0.44932 time= 0.04488\n",
      "Epoch: 0151 train_loss= 0.44900 time= 0.04488\n",
      "Epoch: 0152 train_loss= 0.44867 time= 0.04488\n",
      "Epoch: 0153 train_loss= 0.44835 time= 0.04787\n",
      "Epoch: 0154 train_loss= 0.44803 time= 0.04190\n",
      "Epoch: 0155 train_loss= 0.44772 time= 0.03988\n",
      "Epoch: 0156 train_loss= 0.44741 time= 0.03674\n",
      "Epoch: 0157 train_loss= 0.44711 time= 0.03801\n",
      "Epoch: 0158 train_loss= 0.44682 time= 0.03979\n",
      "Epoch: 0159 train_loss= 0.44653 time= 0.04151\n",
      "Epoch: 0160 train_loss= 0.44626 time= 0.04674\n",
      "Epoch: 0161 train_loss= 0.44599 time= 0.04566\n",
      "Epoch: 0162 train_loss= 0.44573 time= 0.04488\n",
      "Epoch: 0163 train_loss= 0.44547 time= 0.04318\n",
      "Epoch: 0164 train_loss= 0.44521 time= 0.04688\n",
      "Epoch: 0165 train_loss= 0.44495 time= 0.04189\n",
      "Epoch: 0166 train_loss= 0.44469 time= 0.04098\n",
      "Epoch: 0167 train_loss= 0.44442 time= 0.03541\n",
      "Epoch: 0168 train_loss= 0.44415 time= 0.03790\n",
      "Epoch: 0169 train_loss= 0.44386 time= 0.04588\n",
      "Epoch: 0170 train_loss= 0.44357 time= 0.04150\n",
      "Epoch: 0171 train_loss= 0.44328 time= 0.04825\n",
      "Epoch: 0172 train_loss= 0.44297 time= 0.04189\n",
      "Epoch: 0173 train_loss= 0.44266 time= 0.04289\n",
      "Epoch: 0174 train_loss= 0.44235 time= 0.03491\n",
      "Epoch: 0175 train_loss= 0.44203 time= 0.03634\n",
      "Epoch: 0176 train_loss= 0.44171 time= 0.03790\n",
      "Epoch: 0177 train_loss= 0.44139 time= 0.03690\n",
      "Epoch: 0178 train_loss= 0.44107 time= 0.05352\n",
      "Epoch: 0179 train_loss= 0.44076 time= 0.04536\n",
      "Epoch: 0180 train_loss= 0.44044 time= 0.04779\n",
      "Epoch: 0181 train_loss= 0.44013 time= 0.04392\n",
      "Epoch: 0182 train_loss= 0.43982 time= 0.04302\n",
      "Epoch: 0183 train_loss= 0.43951 time= 0.04143\n",
      "Epoch: 0184 train_loss= 0.43922 time= 0.03989\n",
      "Epoch: 0185 train_loss= 0.43893 time= 0.03625\n",
      "Epoch: 0186 train_loss= 0.43865 time= 0.03650\n",
      "Epoch: 0187 train_loss= 0.43838 time= 0.04585\n",
      "Epoch: 0188 train_loss= 0.43812 time= 0.05189\n",
      "Epoch: 0189 train_loss= 0.43786 time= 0.05485\n",
      "Epoch: 0190 train_loss= 0.43761 time= 0.04593\n",
      "Epoch: 0191 train_loss= 0.43735 time= 0.04619\n",
      "Epoch: 0192 train_loss= 0.43710 time= 0.03753\n",
      "Epoch: 0193 train_loss= 0.43684 time= 0.04019\n",
      "Epoch: 0194 train_loss= 0.43658 time= 0.03461\n",
      "Epoch: 0195 train_loss= 0.43632 time= 0.03790\n",
      "Epoch: 0196 train_loss= 0.43605 time= 0.04299\n",
      "Epoch: 0197 train_loss= 0.43577 time= 0.04288\n",
      "Epoch: 0198 train_loss= 0.43549 time= 0.04223\n",
      "Epoch: 0199 train_loss= 0.43520 time= 0.04102\n",
      "Epoch: 0200 train_loss= 0.43490 time= 0.03972\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.73510 time= 0.24747\n",
      "Epoch: 0002 train_loss= 0.73464 time= 0.03786\n",
      "Epoch: 0003 train_loss= 0.73324 time= 0.03694\n",
      "Epoch: 0004 train_loss= 0.73024 time= 0.03705\n",
      "Epoch: 0005 train_loss= 0.72499 time= 0.03503\n",
      "Epoch: 0006 train_loss= 0.71690 time= 0.04162\n",
      "Epoch: 0007 train_loss= 0.70586 time= 0.04389\n",
      "Epoch: 0008 train_loss= 0.69278 time= 0.05585\n",
      "Epoch: 0009 train_loss= 0.68068 time= 0.04987\n",
      "Epoch: 0010 train_loss= 0.67516 time= 0.04588\n",
      "Epoch: 0011 train_loss= 0.67802 time= 0.04189\n",
      "Epoch: 0012 train_loss= 0.67803 time= 0.03989\n",
      "Epoch: 0013 train_loss= 0.67146 time= 0.03590\n",
      "Epoch: 0014 train_loss= 0.66302 time= 0.03690\n",
      "Epoch: 0015 train_loss= 0.65658 time= 0.04289\n",
      "Epoch: 0016 train_loss= 0.65288 time= 0.04493\n",
      "Epoch: 0017 train_loss= 0.65051 time= 0.04954\n",
      "Epoch: 0018 train_loss= 0.64784 time= 0.04940\n",
      "Epoch: 0019 train_loss= 0.64397 time= 0.04189\n",
      "Epoch: 0020 train_loss= 0.63876 time= 0.04388\n",
      "Epoch: 0021 train_loss= 0.63261 time= 0.03963\n",
      "Epoch: 0022 train_loss= 0.62623 time= 0.03690\n",
      "Epoch: 0023 train_loss= 0.62039 time= 0.03877\n",
      "Epoch: 0024 train_loss= 0.61559 time= 0.04109\n",
      "Epoch: 0025 train_loss= 0.61179 time= 0.04888\n",
      "Epoch: 0026 train_loss= 0.60850 time= 0.04487\n",
      "Epoch: 0027 train_loss= 0.60534 time= 0.03690\n",
      "Epoch: 0028 train_loss= 0.60246 time= 0.03730\n",
      "Epoch: 0029 train_loss= 0.60028 time= 0.03850\n",
      "Epoch: 0030 train_loss= 0.59897 time= 0.03904\n",
      "Epoch: 0031 train_loss= 0.59796 time= 0.04673\n",
      "Epoch: 0032 train_loss= 0.59635 time= 0.04288\n",
      "Epoch: 0033 train_loss= 0.59364 time= 0.04448\n",
      "Epoch: 0034 train_loss= 0.58995 time= 0.03746\n",
      "Epoch: 0035 train_loss= 0.58578 time= 0.03757\n",
      "Epoch: 0036 train_loss= 0.58158 time= 0.03989\n",
      "Epoch: 0037 train_loss= 0.57751 time= 0.04225\n",
      "Epoch: 0038 train_loss= 0.57340 time= 0.04209\n",
      "Epoch: 0039 train_loss= 0.56909 time= 0.04068\n",
      "Epoch: 0040 train_loss= 0.56461 time= 0.03690\n",
      "Epoch: 0041 train_loss= 0.56021 time= 0.03890\n",
      "Epoch: 0042 train_loss= 0.55618 time= 0.03514\n",
      "Epoch: 0043 train_loss= 0.55269 time= 0.03568\n",
      "Epoch: 0044 train_loss= 0.54981 time= 0.05199\n",
      "Epoch: 0045 train_loss= 0.54752 time= 0.04388\n",
      "Epoch: 0046 train_loss= 0.54579 time= 0.03639\n",
      "Epoch: 0047 train_loss= 0.54455 time= 0.03727\n",
      "Epoch: 0048 train_loss= 0.54370 time= 0.03852\n",
      "Epoch: 0049 train_loss= 0.54306 time= 0.04089\n",
      "Epoch: 0050 train_loss= 0.54240 time= 0.04189\n",
      "Epoch: 0051 train_loss= 0.54150 time= 0.04554\n",
      "Epoch: 0052 train_loss= 0.54022 time= 0.04186\n",
      "Epoch: 0053 train_loss= 0.53862 time= 0.03491\n",
      "Epoch: 0054 train_loss= 0.53688 time= 0.03564\n",
      "Epoch: 0055 train_loss= 0.53518 time= 0.04074\n",
      "Epoch: 0056 train_loss= 0.53359 time= 0.03989\n",
      "Epoch: 0057 train_loss= 0.53208 time= 0.04388\n",
      "Epoch: 0058 train_loss= 0.53055 time= 0.05186\n",
      "Epoch: 0059 train_loss= 0.52893 time= 0.05246\n",
      "Epoch: 0060 train_loss= 0.52721 time= 0.04183\n",
      "Epoch: 0061 train_loss= 0.52539 time= 0.04289\n",
      "Epoch: 0062 train_loss= 0.52346 time= 0.03989\n",
      "Epoch: 0063 train_loss= 0.52140 time= 0.03989\n",
      "Epoch: 0064 train_loss= 0.51921 time= 0.03590\n",
      "Epoch: 0065 train_loss= 0.51692 time= 0.04208\n",
      "Epoch: 0066 train_loss= 0.51452 time= 0.04388\n",
      "Epoch: 0067 train_loss= 0.51202 time= 0.05485\n",
      "Epoch: 0068 train_loss= 0.50941 time= 0.04991\n",
      "Epoch: 0069 train_loss= 0.50673 time= 0.04193\n",
      "Epoch: 0070 train_loss= 0.50403 time= 0.03900\n",
      "Epoch: 0071 train_loss= 0.50145 time= 0.03580\n",
      "Epoch: 0072 train_loss= 0.49913 time= 0.03661\n",
      "Epoch: 0073 train_loss= 0.49718 time= 0.03491\n",
      "Epoch: 0074 train_loss= 0.49566 time= 0.03990\n",
      "Epoch: 0075 train_loss= 0.49459 time= 0.05484\n",
      "Epoch: 0076 train_loss= 0.49386 time= 0.04585\n",
      "Epoch: 0077 train_loss= 0.49331 time= 0.04612\n",
      "Epoch: 0078 train_loss= 0.49276 time= 0.03962\n",
      "Epoch: 0079 train_loss= 0.49206 time= 0.04058\n",
      "Epoch: 0080 train_loss= 0.49116 time= 0.03989\n",
      "Epoch: 0081 train_loss= 0.49007 time= 0.03989\n",
      "Epoch: 0082 train_loss= 0.48884 time= 0.03790\n",
      "Epoch: 0083 train_loss= 0.48755 time= 0.04104\n",
      "Epoch: 0084 train_loss= 0.48623 time= 0.04606\n",
      "Epoch: 0085 train_loss= 0.48494 time= 0.04588\n",
      "Epoch: 0086 train_loss= 0.48369 time= 0.04688\n",
      "Epoch: 0087 train_loss= 0.48251 time= 0.04787\n",
      "Epoch: 0088 train_loss= 0.48141 time= 0.04588\n",
      "Epoch: 0089 train_loss= 0.48037 time= 0.03837\n",
      "Epoch: 0090 train_loss= 0.47938 time= 0.03643\n",
      "Epoch: 0091 train_loss= 0.47843 time= 0.03790\n",
      "Epoch: 0092 train_loss= 0.47750 time= 0.03765\n",
      "Epoch: 0093 train_loss= 0.47659 time= 0.04987\n",
      "Epoch: 0094 train_loss= 0.47571 time= 0.04363\n",
      "Epoch: 0095 train_loss= 0.47485 time= 0.05113\n",
      "Epoch: 0096 train_loss= 0.47405 time= 0.04719\n",
      "Epoch: 0097 train_loss= 0.47332 time= 0.04236\n",
      "Epoch: 0098 train_loss= 0.47269 time= 0.03883\n",
      "Epoch: 0099 train_loss= 0.47216 time= 0.03690\n",
      "Epoch: 0100 train_loss= 0.47173 time= 0.03491\n",
      "Epoch: 0101 train_loss= 0.47135 time= 0.03889\n",
      "Epoch: 0102 train_loss= 0.47101 time= 0.04488\n",
      "Epoch: 0103 train_loss= 0.47067 time= 0.04388\n",
      "Epoch: 0104 train_loss= 0.47030 time= 0.03989\n",
      "Epoch: 0105 train_loss= 0.46988 time= 0.03591\n",
      "Epoch: 0106 train_loss= 0.46943 time= 0.03590\n",
      "Epoch: 0107 train_loss= 0.46894 time= 0.03790\n",
      "Epoch: 0108 train_loss= 0.46844 time= 0.03690\n",
      "Epoch: 0109 train_loss= 0.46793 time= 0.04423\n",
      "Epoch: 0110 train_loss= 0.46744 time= 0.04088\n",
      "Epoch: 0111 train_loss= 0.46696 time= 0.03590\n",
      "Epoch: 0112 train_loss= 0.46651 time= 0.03890\n",
      "Epoch: 0113 train_loss= 0.46608 time= 0.03910\n",
      "Epoch: 0114 train_loss= 0.46566 time= 0.03790\n",
      "Epoch: 0115 train_loss= 0.46525 time= 0.04887\n",
      "Epoch: 0116 train_loss= 0.46484 time= 0.04122\n",
      "Epoch: 0117 train_loss= 0.46443 time= 0.04189\n",
      "Epoch: 0118 train_loss= 0.46399 time= 0.03864\n",
      "Epoch: 0119 train_loss= 0.46354 time= 0.03906\n",
      "Epoch: 0120 train_loss= 0.46306 time= 0.03790\n",
      "Epoch: 0121 train_loss= 0.46257 time= 0.03844\n",
      "Epoch: 0122 train_loss= 0.46205 time= 0.04930\n",
      "Epoch: 0123 train_loss= 0.46152 time= 0.04315\n",
      "Epoch: 0124 train_loss= 0.46099 time= 0.04181\n",
      "Epoch: 0125 train_loss= 0.46045 time= 0.03832\n",
      "Epoch: 0126 train_loss= 0.45991 time= 0.03815\n",
      "Epoch: 0127 train_loss= 0.45938 time= 0.03947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0128 train_loss= 0.45885 time= 0.04449\n",
      "Epoch: 0129 train_loss= 0.45834 time= 0.04011\n",
      "Epoch: 0130 train_loss= 0.45784 time= 0.04073\n",
      "Epoch: 0131 train_loss= 0.45735 time= 0.03463\n",
      "Epoch: 0132 train_loss= 0.45687 time= 0.03742\n",
      "Epoch: 0133 train_loss= 0.45641 time= 0.04196\n",
      "Epoch: 0134 train_loss= 0.45597 time= 0.04789\n",
      "Epoch: 0135 train_loss= 0.45554 time= 0.04825\n",
      "Epoch: 0136 train_loss= 0.45513 time= 0.04614\n",
      "Epoch: 0137 train_loss= 0.45472 time= 0.04592\n",
      "Epoch: 0138 train_loss= 0.45431 time= 0.04370\n",
      "Epoch: 0139 train_loss= 0.45388 time= 0.04359\n",
      "Epoch: 0140 train_loss= 0.45344 time= 0.04095\n",
      "Epoch: 0141 train_loss= 0.45298 time= 0.03881\n",
      "Epoch: 0142 train_loss= 0.45250 time= 0.03657\n",
      "Epoch: 0143 train_loss= 0.45200 time= 0.04007\n",
      "Epoch: 0144 train_loss= 0.45148 time= 0.04848\n",
      "Epoch: 0145 train_loss= 0.45095 time= 0.04992\n",
      "Epoch: 0146 train_loss= 0.45041 time= 0.05196\n",
      "Epoch: 0147 train_loss= 0.44988 time= 0.04918\n",
      "Epoch: 0148 train_loss= 0.44936 time= 0.04106\n",
      "Epoch: 0149 train_loss= 0.44884 time= 0.03913\n",
      "Epoch: 0150 train_loss= 0.44834 time= 0.03695\n",
      "Epoch: 0151 train_loss= 0.44785 time= 0.03613\n",
      "Epoch: 0152 train_loss= 0.44739 time= 0.04368\n",
      "Epoch: 0153 train_loss= 0.44694 time= 0.04365\n",
      "Epoch: 0154 train_loss= 0.44650 time= 0.05160\n",
      "Epoch: 0155 train_loss= 0.44608 time= 0.04219\n",
      "Epoch: 0156 train_loss= 0.44566 time= 0.04170\n",
      "Epoch: 0157 train_loss= 0.44524 time= 0.03695\n",
      "Epoch: 0158 train_loss= 0.44481 time= 0.03619\n",
      "Epoch: 0159 train_loss= 0.44436 time= 0.03722\n",
      "Epoch: 0160 train_loss= 0.44391 time= 0.03724\n",
      "Epoch: 0161 train_loss= 0.44344 time= 0.04289\n",
      "Epoch: 0162 train_loss= 0.44296 time= 0.04297\n",
      "Epoch: 0163 train_loss= 0.44249 time= 0.05084\n",
      "Epoch: 0164 train_loss= 0.44202 time= 0.04280\n",
      "Epoch: 0165 train_loss= 0.44157 time= 0.04847\n",
      "Epoch: 0166 train_loss= 0.44114 time= 0.04068\n",
      "Epoch: 0167 train_loss= 0.44073 time= 0.03726\n",
      "Epoch: 0168 train_loss= 0.44035 time= 0.03958\n",
      "Epoch: 0169 train_loss= 0.44000 time= 0.03592\n",
      "Epoch: 0170 train_loss= 0.43968 time= 0.04318\n",
      "Epoch: 0171 train_loss= 0.43938 time= 0.04729\n",
      "Epoch: 0172 train_loss= 0.43909 time= 0.05669\n",
      "Epoch: 0173 train_loss= 0.43880 time= 0.05017\n",
      "Epoch: 0174 train_loss= 0.43852 time= 0.04494\n",
      "Epoch: 0175 train_loss= 0.43824 time= 0.04702\n",
      "Epoch: 0176 train_loss= 0.43795 time= 0.04088\n",
      "Epoch: 0177 train_loss= 0.43765 time= 0.04102\n",
      "Epoch: 0178 train_loss= 0.43735 time= 0.03656\n",
      "Epoch: 0179 train_loss= 0.43704 time= 0.03599\n",
      "Epoch: 0180 train_loss= 0.43673 time= 0.04901\n",
      "Epoch: 0181 train_loss= 0.43642 time= 0.04328\n",
      "Epoch: 0182 train_loss= 0.43612 time= 0.03690\n",
      "Epoch: 0183 train_loss= 0.43583 time= 0.03733\n",
      "Epoch: 0184 train_loss= 0.43554 time= 0.03629\n",
      "Epoch: 0185 train_loss= 0.43526 time= 0.03692\n",
      "Epoch: 0186 train_loss= 0.43499 time= 0.04421\n",
      "Epoch: 0187 train_loss= 0.43472 time= 0.04725\n",
      "Epoch: 0188 train_loss= 0.43447 time= 0.04123\n",
      "Epoch: 0189 train_loss= 0.43422 time= 0.03592\n",
      "Epoch: 0190 train_loss= 0.43398 time= 0.03626\n",
      "Epoch: 0191 train_loss= 0.43375 time= 0.03666\n",
      "Epoch: 0192 train_loss= 0.43352 time= 0.03598\n",
      "Epoch: 0193 train_loss= 0.43330 time= 0.04469\n",
      "Epoch: 0194 train_loss= 0.43309 time= 0.04159\n",
      "Epoch: 0195 train_loss= 0.43287 time= 0.04358\n",
      "Epoch: 0196 train_loss= 0.43266 time= 0.03734\n",
      "Epoch: 0197 train_loss= 0.43245 time= 0.03890\n",
      "Epoch: 0198 train_loss= 0.43224 time= 0.03690\n",
      "Epoch: 0199 train_loss= 0.43204 time= 0.03790\n",
      "Epoch: 0200 train_loss= 0.43183 time= 0.04688\n",
      "Testing model...\n"
     ]
    }
   ],
   "source": [
    "# The entire training+test process is repeated FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "\n",
    "    if FLAGS.task == 'link_prediction' :\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking test edges...\")\n",
    "        # Edge Masking for Link Prediction: compute Train/Validation/Test set\n",
    "        adj, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "        mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "\n",
    "    # Start computation of running times\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Degeneracy Framework / K-Core Decomposition\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Starting k-core decomposition of the graph\")\n",
    "        # Save adjacency matrix of un-decomposed graph\n",
    "        # (needed to embed nodes that are not in k-core, after GAE training)\n",
    "        adj_orig = adj\n",
    "        # Get the (smaller) adjacency matrix of the k-core subgraph,\n",
    "        # and the corresponding nodes\n",
    "        adj, nodes_kcore = compute_kcore(adj, FLAGS.k)\n",
    "        # Get the (smaller) feature matrix of the nb_core graph\n",
    "        if FLAGS.features:\n",
    "            features = features_init[nodes_kcore,:]\n",
    "        # Flag to compute k-core decomposition's running time\n",
    "        t_core = time.time()\n",
    "    elif FLAGS.features:\n",
    "        features = features_init\n",
    "\n",
    "    # Preprocessing and initialization\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing and Initializing...\")\n",
    "    # Compute number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "    # If features are not used, replace feature matrix by identity matrix\n",
    "    if not FLAGS.features:\n",
    "        features = sp.identity(adj.shape[0])\n",
    "    # Preprocessing on node features\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ())\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = None\n",
    "    if FLAGS.model == 'gcn_ae':\n",
    "        # Standard Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # Standard Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                            features_nonzero)\n",
    "    elif FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes,\n",
    "                               features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_ae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = DeepGCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Variational Autoencoder\n",
    "        model = DeepGCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                                features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "    # Optimizer\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0]\n",
    "                                                - adj.sum()) * 2)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # Optimizer for Non-Variational Autoencoders\n",
    "        if FLAGS.model in ('gcn_ae', 'linear_ae', 'deep_gcn_ae'):\n",
    "            opt = OptimizerAE(preds = model.reconstructions,\n",
    "                              labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                            validate_indices = False), [-1]),\n",
    "                              pos_weight = pos_weight,\n",
    "                              norm = norm)\n",
    "        # Optimizer for Variational Autoencoders\n",
    "        elif FLAGS.model in ('gcn_vae', 'linear_vae', 'deep_gcn_vae'):\n",
    "            opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                               labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                             validate_indices = False), [-1]),\n",
    "                               model = model,\n",
    "                               num_nodes = num_nodes,\n",
    "                               pos_weight = pos_weight,\n",
    "                               norm = norm)\n",
    "\n",
    "    # Normalization and preprocessing on adjacency matrix\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = sparse_to_tuple(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Initialize TF session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Training...\")\n",
    "\n",
    "    for epoch in range(FLAGS.epochs):\n",
    "        # Flag to compute running time for each epoch\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_label, features,\n",
    "                                        placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.accuracy],\n",
    "                        feed_dict = feed_dict)\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display epoch information\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "            # Validation, for Link Prediction\n",
    "            if not FLAGS.kcore and FLAGS.validation and FLAGS.task == 'link_prediction':\n",
    "                feed_dict.update({placeholders['dropout']: 0})\n",
    "                emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "                feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "                val_roc, val_ap = get_roc_score(val_edges, val_edges_false, emb)\n",
    "                print(\"val_roc=\", \"{:.5f}\".format(val_roc), \"val_ap=\", \"{:.5f}\".format(val_ap))\n",
    "\n",
    "    # Flag to compute Graph AE/VAE training time\n",
    "    t_model = time.time()\n",
    "\n",
    "    # Compute embedding\n",
    "\n",
    "    # Get embedding from model\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "\n",
    "    # If k-core is used, only part of the nodes from the original\n",
    "    # graph are embedded. The remaining ones are projected in the\n",
    "    # latent space via the expand_embedding heuristic\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Propagation to remaining nodes...\")\n",
    "        # Project remaining nodes in latent space\n",
    "        emb = expand_embedding(adj_orig, emb, nodes_kcore, FLAGS.nb_iterations)\n",
    "        # Compute mean running times for K-Core, GAE Train and Propagation steps\n",
    "        mean_time_expand.append(time.time() - t_model)\n",
    "        mean_time_train.append(t_model - t_core)\n",
    "        mean_time_kcore.append(t_core - t_start)\n",
    "        # Compute mean size of K-Core graph\n",
    "        # Note: size is fixed if task is node clustering, but will vary if\n",
    "        # task is link prediction due to edge masking\n",
    "        mean_core_size.append(len(nodes_kcore))\n",
    "\n",
    "    # Compute mean total running time\n",
    "    mean_time.append(time.time() - t_start)\n",
    "\n",
    "    # Test model\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing model...\")\n",
    "    # Link Prediction: classification edges/non-edges\n",
    "    if FLAGS.task == 'link_prediction':\n",
    "        # Get ROC and AP scores\n",
    "        ap_score, roc_score = get_roc_score(test_edges, test_edges_false, emb)\n",
    "        # Report scores\n",
    "        mean_ap.append(ap_score)\n",
    "        mean_roc.append(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80215135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T12:09:59.760901Z",
     "start_time": "2022-05-08T12:09:59.745862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results for gcn_ae model on email on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AP scores\n",
      " [0.882687123383034, 0.8864427914849348, 0.896048454155328, 0.881520415367957, 0.8955395569707058, 0.8900340394197731, 0.904384558604975, 0.8850908821661818, 0.8755752825528204, 0.9059179383000182]\n",
      "Mean AP score:  0.8903241042405728 \n",
      "Std of AP scores:  0.00946955469851858 \n",
      " \n",
      "\n",
      "AUC scores\n",
      " [0.8634492046124064, 0.8606514603147883, 0.8740577392475382, 0.8527076845383383, 0.8826563420587493, 0.8544533288443734, 0.8876929551384564, 0.8696237690430099, 0.8585337934517296, 0.8881003282551972]\n",
      "Mean AUC score:  0.8691926605504587 \n",
      "Std of AUC scores:  0.012702123962401127 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Report Final Results ######\n",
    "\n",
    "# Report final results\n",
    "print(\"\\nTest results for\", FLAGS.model,\n",
    "      \"model on\", FLAGS.dataset, \"on\", FLAGS.task, \"\\n\",\n",
    "      \"___________________________________________________\\n\")\n",
    "\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    print(\"AP scores\\n\", mean_ap)\n",
    "    print(\"Mean AP score: \", np.mean(mean_ap),\n",
    "          \"\\nStd of AP scores: \", np.std(mean_ap), \"\\n \\n\")\n",
    "    \n",
    "    print(\"AUC scores\\n\", mean_roc)\n",
    "    print(\"Mean AUC score: \", np.mean(mean_roc),\n",
    "          \"\\nStd of AUC scores: \", np.std(mean_roc), \"\\n \\n\")\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    print(\"Details on degeneracy framework, with k =\", FLAGS.k, \": \\n \\n\")\n",
    "\n",
    "    print(\"Running times for k-core decomposition\\n\", mean_time_kcore)\n",
    "    print(\"Mean: \", np.mean(mean_time_kcore),\n",
    "          \"\\nStd: \", np.std(mean_time_kcore), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for autoencoder training\\n\", mean_time_train)\n",
    "    print(\"Mean: \", np.mean(mean_time_train),\n",
    "          \"\\nStd: \", np.std(mean_time_train), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for propagation\\n\", mean_time_expand)\n",
    "    print(\"Mean: \", np.mean(mean_time_expand),\n",
    "          \"\\nStd: \", np.std(mean_time_expand), \"\\n \\n\")\n",
    "\n",
    "    print(\"Sizes of k-core subgraph\\n\", mean_core_size)\n",
    "    print(\"Mean: \", np.mean(mean_core_size),\n",
    "          \"\\nStd: \", np.std(mean_core_size), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01df94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
