{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcdc1a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:37:45.902333Z",
     "start_time": "2022-05-02T12:37:45.887209Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from linear_gae.evaluation import get_roc_score, clustering_latent_space\n",
    "from linear_gae.kcore import compute_kcore, expand_embedding\n",
    "from linear_gae.model import *\n",
    "from linear_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from linear_gae.preprocessing import *\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0da214c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:37:47.257836Z",
     "start_time": "2022-05-02T12:37:47.247853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x1af2f20bd68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')   # 添加的，不报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca906e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:37:48.328200Z",
     "start_time": "2022-05-02T12:37:48.317761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Available Models:\\n\\n- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\\n          GCN encoder and inner product decoder\\n\\n- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\\n           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\\n           product decoder\\n\\n- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\\n             workshop paper, with linear encoder, and inner product decoder\\n\\n- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\\n              of NeurIPS workshop paper, with Gaussian priors, linear encoders\\n              for mu and sigma, and inner product decoder\\n \\n- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\\n               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\\n               product decoder\\n \\n- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\\n                in section 4 of NeurIPS workshop paper, with Gaussian priors,\\n                3-layer GCN encoders for mu and sigma, and inner product\\n                decoder\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select graph dataset\n",
    "flags.DEFINE_string('dataset', 'cora', 'Name of the graph dataset')\n",
    "\n",
    "# Select machine learning task to perform on graph\n",
    "flags.DEFINE_string('task', 'link_prediction', 'Name of the learning task')\n",
    "\n",
    "# Model\n",
    "flags.DEFINE_string('model', 'gcn_ae', 'Name of the model')\n",
    "''' Available Models:\n",
    "\n",
    "- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\n",
    "          GCN encoder and inner product decoder\n",
    "\n",
    "- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\n",
    "           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\n",
    "           product decoder\n",
    "\n",
    "- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\n",
    "             workshop paper, with linear encoder, and inner product decoder\n",
    "\n",
    "- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\n",
    "              of NeurIPS workshop paper, with Gaussian priors, linear encoders\n",
    "              for mu and sigma, and inner product decoder\n",
    " \n",
    "- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\n",
    "               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\n",
    "               product decoder\n",
    " \n",
    "- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\n",
    "                in section 4 of NeurIPS workshop paper, with Gaussian priors,\n",
    "                3-layer GCN encoders for mu and sigma, and inner product\n",
    "                decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2ea245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:37:49.218173Z",
     "start_time": "2022-05-02T12:37:49.198226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs in training.')\n",
    "flags.DEFINE_boolean('features', False, 'Include node features or not in encoder')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (with Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Number of units in GCN hidden layer(s).')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of encoder output, i.e. \\\n",
    "                                       embedding dimension')\n",
    "\n",
    "# Experimental setup parameters\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 5., 'Proportion of edges in validation set \\\n",
    "                                   (for Link Prediction task)')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      (for Link Prediction task)')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to report validation \\\n",
    "                                           results at each epoch (for \\\n",
    "                                           Link Prediction task)')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print comments details.')\n",
    "\n",
    "# Parameters related to the \"degeneracy framework\" from IJCAI 2019 paper,\n",
    "# aiming at scaling-up Graph AE/VAE by training the model only on the k-core\n",
    "# (smaller) version of the graph, then expanding embedding to remaining nodes\n",
    "# via simpler and faster heuristics\n",
    "flags.DEFINE_boolean('kcore', False, 'Whether to run k-core decomposition \\\n",
    "                                      and use the framework. False = model \\\n",
    "                                      will be trained on the entire graph')\n",
    "flags.DEFINE_integer('k', 2, 'Which k-core to use. Higher k => smaller graphs\\\n",
    "                              and faster (but maybe less accurate) training')\n",
    "flags.DEFINE_integer('nb_iterations', 10, 'Number of fix point iterations in \\\n",
    "                                           algorithm 2 of IJCAI paper. See \\\n",
    "                                           kcore.py file for details')\n",
    "\n",
    "# Lists to collect average results\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    mean_roc = []\n",
    "    mean_ap = []\n",
    "    mean_acc = []\n",
    "    mean_f1 = []\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    mean_time_kcore = []\n",
    "    mean_time_train = []\n",
    "    mean_time_expand = []\n",
    "    mean_core_size = []\n",
    "mean_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54564081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:37:51.050977Z",
     "start_time": "2022-05-02T12:37:51.039055Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data(dataset):\n",
    "    # Load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    graph = nx.from_dict_of_lists(graph)\n",
    "    adj = nx.adjacency_matrix(graph)\n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73510de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:37:52.008339Z",
     "start_time": "2022-05-02T12:37:51.901653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load graph dataset\n",
    "if FLAGS.verbose:\n",
    "    print(\"Loading data...\")\n",
    "adj_init, features_init = load_data(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f59e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:44:27.373475Z",
     "start_time": "2022-05-02T12:37:57.295909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79734 time= 0.28095\n",
      "Epoch: 0002 train_loss= 0.79659 time= 0.20303\n",
      "Epoch: 0003 train_loss= 0.79465 time= 0.20499\n",
      "Epoch: 0004 train_loss= 0.79072 time= 0.20712\n",
      "Epoch: 0005 train_loss= 0.78390 time= 0.21110\n",
      "Epoch: 0006 train_loss= 0.77335 time= 0.19691\n",
      "Epoch: 0007 train_loss= 0.75884 time= 0.20680\n",
      "Epoch: 0008 train_loss= 0.74161 time= 0.20139\n",
      "Epoch: 0009 train_loss= 0.72545 time= 0.19580\n",
      "Epoch: 0010 train_loss= 0.71731 time= 0.19583\n",
      "Epoch: 0011 train_loss= 0.72096 time= 0.20346\n",
      "Epoch: 0012 train_loss= 0.72332 time= 0.20264\n",
      "Epoch: 0013 train_loss= 0.71684 time= 0.20845\n",
      "Epoch: 0014 train_loss= 0.70622 time= 0.19800\n",
      "Epoch: 0015 train_loss= 0.69671 time= 0.19550\n",
      "Epoch: 0016 train_loss= 0.69036 time= 0.20096\n",
      "Epoch: 0017 train_loss= 0.68631 time= 0.18978\n",
      "Epoch: 0018 train_loss= 0.68265 time= 0.18904\n",
      "Epoch: 0019 train_loss= 0.67785 time= 0.19284\n",
      "Epoch: 0020 train_loss= 0.67122 time= 0.18666\n",
      "Epoch: 0021 train_loss= 0.66272 time= 0.18949\n",
      "Epoch: 0022 train_loss= 0.65275 time= 0.19721\n",
      "Epoch: 0023 train_loss= 0.64194 time= 0.19574\n",
      "Epoch: 0024 train_loss= 0.63096 time= 0.19535\n",
      "Epoch: 0025 train_loss= 0.62020 time= 0.19344\n",
      "Epoch: 0026 train_loss= 0.60963 time= 0.19151\n",
      "Epoch: 0027 train_loss= 0.59879 time= 0.18838\n",
      "Epoch: 0028 train_loss= 0.58731 time= 0.19575\n",
      "Epoch: 0029 train_loss= 0.57528 time= 0.19226\n",
      "Epoch: 0030 train_loss= 0.56338 time= 0.19403\n",
      "Epoch: 0031 train_loss= 0.55255 time= 0.20267\n",
      "Epoch: 0032 train_loss= 0.54359 time= 0.20147\n",
      "Epoch: 0033 train_loss= 0.53682 time= 0.20730\n",
      "Epoch: 0034 train_loss= 0.53199 time= 0.20388\n",
      "Epoch: 0035 train_loss= 0.52843 time= 0.21057\n",
      "Epoch: 0036 train_loss= 0.52546 time= 0.19316\n",
      "Epoch: 0037 train_loss= 0.52267 time= 0.19242\n",
      "Epoch: 0038 train_loss= 0.51999 time= 0.18990\n",
      "Epoch: 0039 train_loss= 0.51741 time= 0.19101\n",
      "Epoch: 0040 train_loss= 0.51488 time= 0.19663\n",
      "Epoch: 0041 train_loss= 0.51218 time= 0.20041\n",
      "Epoch: 0042 train_loss= 0.50917 time= 0.19625\n",
      "Epoch: 0043 train_loss= 0.50588 time= 0.18728\n",
      "Epoch: 0044 train_loss= 0.50256 time= 0.19380\n",
      "Epoch: 0045 train_loss= 0.49948 time= 0.19927\n",
      "Epoch: 0046 train_loss= 0.49684 time= 0.19750\n",
      "Epoch: 0047 train_loss= 0.49465 time= 0.20175\n",
      "Epoch: 0048 train_loss= 0.49280 time= 0.20308\n",
      "Epoch: 0049 train_loss= 0.49112 time= 0.19959\n",
      "Epoch: 0050 train_loss= 0.48952 time= 0.18231\n",
      "Epoch: 0051 train_loss= 0.48795 time= 0.19880\n",
      "Epoch: 0052 train_loss= 0.48641 time= 0.18401\n",
      "Epoch: 0053 train_loss= 0.48491 time= 0.18333\n",
      "Epoch: 0054 train_loss= 0.48345 time= 0.19452\n",
      "Epoch: 0055 train_loss= 0.48201 time= 0.19645\n",
      "Epoch: 0056 train_loss= 0.48057 time= 0.19291\n",
      "Epoch: 0057 train_loss= 0.47913 time= 0.19719\n",
      "Epoch: 0058 train_loss= 0.47768 time= 0.19509\n",
      "Epoch: 0059 train_loss= 0.47627 time= 0.20728\n",
      "Epoch: 0060 train_loss= 0.47492 time= 0.19847\n",
      "Epoch: 0061 train_loss= 0.47366 time= 0.19747\n",
      "Epoch: 0062 train_loss= 0.47249 time= 0.20588\n",
      "Epoch: 0063 train_loss= 0.47139 time= 0.19937\n",
      "Epoch: 0064 train_loss= 0.47036 time= 0.19027\n",
      "Epoch: 0065 train_loss= 0.46938 time= 0.19073\n",
      "Epoch: 0066 train_loss= 0.46846 time= 0.20379\n",
      "Epoch: 0067 train_loss= 0.46760 time= 0.20507\n",
      "Epoch: 0068 train_loss= 0.46677 time= 0.20535\n",
      "Epoch: 0069 train_loss= 0.46596 time= 0.19483\n",
      "Epoch: 0070 train_loss= 0.46513 time= 0.20270\n",
      "Epoch: 0071 train_loss= 0.46425 time= 0.19815\n",
      "Epoch: 0072 train_loss= 0.46334 time= 0.19885\n",
      "Epoch: 0073 train_loss= 0.46241 time= 0.19733\n",
      "Epoch: 0074 train_loss= 0.46147 time= 0.19346\n",
      "Epoch: 0075 train_loss= 0.46054 time= 0.19365\n",
      "Epoch: 0076 train_loss= 0.45965 time= 0.19343\n",
      "Epoch: 0077 train_loss= 0.45879 time= 0.19123\n",
      "Epoch: 0078 train_loss= 0.45795 time= 0.18478\n",
      "Epoch: 0079 train_loss= 0.45713 time= 0.18680\n",
      "Epoch: 0080 train_loss= 0.45632 time= 0.18460\n",
      "Epoch: 0081 train_loss= 0.45553 time= 0.18013\n",
      "Epoch: 0082 train_loss= 0.45476 time= 0.18052\n",
      "Epoch: 0083 train_loss= 0.45400 time= 0.18486\n",
      "Epoch: 0084 train_loss= 0.45326 time= 0.19000\n",
      "Epoch: 0085 train_loss= 0.45254 time= 0.18949\n",
      "Epoch: 0086 train_loss= 0.45186 time= 0.18850\n",
      "Epoch: 0087 train_loss= 0.45121 time= 0.19027\n",
      "Epoch: 0088 train_loss= 0.45058 time= 0.18774\n",
      "Epoch: 0089 train_loss= 0.44997 time= 0.20457\n",
      "Epoch: 0090 train_loss= 0.44936 time= 0.19352\n",
      "Epoch: 0091 train_loss= 0.44875 time= 0.18506\n",
      "Epoch: 0092 train_loss= 0.44814 time= 0.18576\n",
      "Epoch: 0093 train_loss= 0.44753 time= 0.19562\n",
      "Epoch: 0094 train_loss= 0.44692 time= 0.18849\n",
      "Epoch: 0095 train_loss= 0.44631 time= 0.21069\n",
      "Epoch: 0096 train_loss= 0.44571 time= 0.21018\n",
      "Epoch: 0097 train_loss= 0.44512 time= 0.20177\n",
      "Epoch: 0098 train_loss= 0.44455 time= 0.18919\n",
      "Epoch: 0099 train_loss= 0.44401 time= 0.18850\n",
      "Epoch: 0100 train_loss= 0.44350 time= 0.18949\n",
      "Epoch: 0101 train_loss= 0.44302 time= 0.18951\n",
      "Epoch: 0102 train_loss= 0.44256 time= 0.19149\n",
      "Epoch: 0103 train_loss= 0.44212 time= 0.19049\n",
      "Epoch: 0104 train_loss= 0.44168 time= 0.18650\n",
      "Epoch: 0105 train_loss= 0.44126 time= 0.18650\n",
      "Epoch: 0106 train_loss= 0.44084 time= 0.19648\n",
      "Epoch: 0107 train_loss= 0.44042 time= 0.18849\n",
      "Epoch: 0108 train_loss= 0.44001 time= 0.19249\n",
      "Epoch: 0109 train_loss= 0.43960 time= 0.19747\n",
      "Epoch: 0110 train_loss= 0.43919 time= 0.19149\n",
      "Epoch: 0111 train_loss= 0.43880 time= 0.18850\n",
      "Epoch: 0112 train_loss= 0.43841 time= 0.18950\n",
      "Epoch: 0113 train_loss= 0.43804 time= 0.18750\n",
      "Epoch: 0114 train_loss= 0.43768 time= 0.19254\n",
      "Epoch: 0115 train_loss= 0.43733 time= 0.19343\n",
      "Epoch: 0116 train_loss= 0.43699 time= 0.19548\n",
      "Epoch: 0117 train_loss= 0.43666 time= 0.19249\n",
      "Epoch: 0118 train_loss= 0.43632 time= 0.19049\n",
      "Epoch: 0119 train_loss= 0.43598 time= 0.18650\n",
      "Epoch: 0120 train_loss= 0.43564 time= 0.19612\n",
      "Epoch: 0121 train_loss= 0.43529 time= 0.19258\n",
      "Epoch: 0122 train_loss= 0.43493 time= 0.19738\n",
      "Epoch: 0123 train_loss= 0.43456 time= 0.19188\n",
      "Epoch: 0124 train_loss= 0.43418 time= 0.18974\n",
      "Epoch: 0125 train_loss= 0.43380 time= 0.19198\n",
      "Epoch: 0126 train_loss= 0.43342 time= 0.19399\n",
      "Epoch: 0127 train_loss= 0.43303 time= 0.19348\n",
      "Epoch: 0128 train_loss= 0.43264 time= 0.18777\n",
      "Epoch: 0129 train_loss= 0.43226 time= 0.18922\n",
      "Epoch: 0130 train_loss= 0.43187 time= 0.18750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0131 train_loss= 0.43149 time= 0.18950\n",
      "Epoch: 0132 train_loss= 0.43112 time= 0.18450\n",
      "Epoch: 0133 train_loss= 0.43075 time= 0.19049\n",
      "Epoch: 0134 train_loss= 0.43039 time= 0.18849\n",
      "Epoch: 0135 train_loss= 0.43003 time= 0.18650\n",
      "Epoch: 0136 train_loss= 0.42967 time= 0.19059\n",
      "Epoch: 0137 train_loss= 0.42932 time= 0.18940\n",
      "Epoch: 0138 train_loss= 0.42897 time= 0.19348\n",
      "Epoch: 0139 train_loss= 0.42862 time= 0.20346\n",
      "Epoch: 0140 train_loss= 0.42827 time= 0.20552\n",
      "Epoch: 0141 train_loss= 0.42792 time= 0.19149\n",
      "Epoch: 0142 train_loss= 0.42756 time= 0.19249\n",
      "Epoch: 0143 train_loss= 0.42721 time= 0.18949\n",
      "Epoch: 0144 train_loss= 0.42686 time= 0.19655\n",
      "Epoch: 0145 train_loss= 0.42652 time= 0.19632\n",
      "Epoch: 0146 train_loss= 0.42617 time= 0.20546\n",
      "Epoch: 0147 train_loss= 0.42583 time= 0.21641\n",
      "Epoch: 0148 train_loss= 0.42550 time= 0.21143\n",
      "Epoch: 0149 train_loss= 0.42517 time= 0.20645\n",
      "Epoch: 0150 train_loss= 0.42486 time= 0.20445\n",
      "Epoch: 0151 train_loss= 0.42455 time= 0.21143\n",
      "Epoch: 0152 train_loss= 0.42425 time= 0.21900\n",
      "Epoch: 0153 train_loss= 0.42395 time= 0.20200\n",
      "Epoch: 0154 train_loss= 0.42367 time= 0.19560\n",
      "Epoch: 0155 train_loss= 0.42339 time= 0.20063\n",
      "Epoch: 0156 train_loss= 0.42311 time= 0.19411\n",
      "Epoch: 0157 train_loss= 0.42284 time= 0.21747\n",
      "Epoch: 0158 train_loss= 0.42257 time= 0.20563\n",
      "Epoch: 0159 train_loss= 0.42230 time= 0.20408\n",
      "Epoch: 0160 train_loss= 0.42204 time= 0.20350\n",
      "Epoch: 0161 train_loss= 0.42178 time= 0.20269\n",
      "Epoch: 0162 train_loss= 0.42152 time= 0.19592\n",
      "Epoch: 0163 train_loss= 0.42127 time= 0.20089\n",
      "Epoch: 0164 train_loss= 0.42102 time= 0.21485\n",
      "Epoch: 0165 train_loss= 0.42078 time= 0.23338\n",
      "Epoch: 0166 train_loss= 0.42054 time= 0.22141\n",
      "Epoch: 0167 train_loss= 0.42031 time= 0.21698\n",
      "Epoch: 0168 train_loss= 0.42008 time= 0.21542\n",
      "Epoch: 0169 train_loss= 0.41984 time= 0.21108\n",
      "Epoch: 0170 train_loss= 0.41961 time= 0.21982\n",
      "Epoch: 0171 train_loss= 0.41938 time= 0.21072\n",
      "Epoch: 0172 train_loss= 0.41914 time= 0.20872\n",
      "Epoch: 0173 train_loss= 0.41890 time= 0.20513\n",
      "Epoch: 0174 train_loss= 0.41866 time= 0.20704\n",
      "Epoch: 0175 train_loss= 0.41841 time= 0.20726\n",
      "Epoch: 0176 train_loss= 0.41817 time= 0.20178\n",
      "Epoch: 0177 train_loss= 0.41792 time= 0.20028\n",
      "Epoch: 0178 train_loss= 0.41767 time= 0.19814\n",
      "Epoch: 0179 train_loss= 0.41742 time= 0.19851\n",
      "Epoch: 0180 train_loss= 0.41717 time= 0.19371\n",
      "Epoch: 0181 train_loss= 0.41692 time= 0.19564\n",
      "Epoch: 0182 train_loss= 0.41667 time= 0.20262\n",
      "Epoch: 0183 train_loss= 0.41642 time= 0.20773\n",
      "Epoch: 0184 train_loss= 0.41618 time= 0.20868\n",
      "Epoch: 0185 train_loss= 0.41594 time= 0.20499\n",
      "Epoch: 0186 train_loss= 0.41570 time= 0.20303\n",
      "Epoch: 0187 train_loss= 0.41547 time= 0.21176\n",
      "Epoch: 0188 train_loss= 0.41524 time= 0.21088\n",
      "Epoch: 0189 train_loss= 0.41502 time= 0.21280\n",
      "Epoch: 0190 train_loss= 0.41480 time= 0.21924\n",
      "Epoch: 0191 train_loss= 0.41459 time= 0.21144\n",
      "Epoch: 0192 train_loss= 0.41438 time= 0.20775\n",
      "Epoch: 0193 train_loss= 0.41417 time= 0.20845\n",
      "Epoch: 0194 train_loss= 0.41397 time= 0.21561\n",
      "Epoch: 0195 train_loss= 0.41377 time= 0.20082\n",
      "Epoch: 0196 train_loss= 0.41357 time= 0.20424\n",
      "Epoch: 0197 train_loss= 0.41338 time= 0.20297\n",
      "Epoch: 0198 train_loss= 0.41319 time= 0.19960\n",
      "Epoch: 0199 train_loss= 0.41300 time= 0.19347\n",
      "Epoch: 0200 train_loss= 0.41281 time= 0.19167\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79735 time= 0.22884\n",
      "Epoch: 0002 train_loss= 0.79671 time= 0.18899\n",
      "Epoch: 0003 train_loss= 0.79514 time= 0.18935\n",
      "Epoch: 0004 train_loss= 0.79209 time= 0.19524\n",
      "Epoch: 0005 train_loss= 0.78687 time= 0.19306\n",
      "Epoch: 0006 train_loss= 0.77876 time= 0.18912\n",
      "Epoch: 0007 train_loss= 0.76720 time= 0.19277\n",
      "Epoch: 0008 train_loss= 0.75239 time= 0.20631\n",
      "Epoch: 0009 train_loss= 0.73601 time= 0.19448\n",
      "Epoch: 0010 train_loss= 0.72228 time= 0.21565\n",
      "Epoch: 0011 train_loss= 0.71755 time= 0.21211\n",
      "Epoch: 0012 train_loss= 0.72117 time= 0.21089\n",
      "Epoch: 0013 train_loss= 0.72097 time= 0.20278\n",
      "Epoch: 0014 train_loss= 0.71350 time= 0.21220\n",
      "Epoch: 0015 train_loss= 0.70275 time= 0.19807\n",
      "Epoch: 0016 train_loss= 0.69266 time= 0.21853\n",
      "Epoch: 0017 train_loss= 0.68476 time= 0.20373\n",
      "Epoch: 0018 train_loss= 0.67845 time= 0.20792\n",
      "Epoch: 0019 train_loss= 0.67235 time= 0.19483\n",
      "Epoch: 0020 train_loss= 0.66526 time= 0.19323\n",
      "Epoch: 0021 train_loss= 0.65652 time= 0.19847\n",
      "Epoch: 0022 train_loss= 0.64602 time= 0.18830\n",
      "Epoch: 0023 train_loss= 0.63404 time= 0.19871\n",
      "Epoch: 0024 train_loss= 0.62110 time= 0.19324\n",
      "Epoch: 0025 train_loss= 0.60776 time= 0.19758\n",
      "Epoch: 0026 train_loss= 0.59452 time= 0.19529\n",
      "Epoch: 0027 train_loss= 0.58171 time= 0.18484\n",
      "Epoch: 0028 train_loss= 0.56942 time= 0.19721\n",
      "Epoch: 0029 train_loss= 0.55763 time= 0.18810\n",
      "Epoch: 0030 train_loss= 0.54632 time= 0.20205\n",
      "Epoch: 0031 train_loss= 0.53562 time= 0.19753\n",
      "Epoch: 0032 train_loss= 0.52575 time= 0.20052\n",
      "Epoch: 0033 train_loss= 0.51698 time= 0.19911\n",
      "Epoch: 0034 train_loss= 0.50952 time= 0.19710\n",
      "Epoch: 0035 train_loss= 0.50347 time= 0.19613\n",
      "Epoch: 0036 train_loss= 0.49883 time= 0.20092\n",
      "Epoch: 0037 train_loss= 0.49551 time= 0.19395\n",
      "Epoch: 0038 train_loss= 0.49328 time= 0.18800\n",
      "Epoch: 0039 train_loss= 0.49177 time= 0.19410\n",
      "Epoch: 0040 train_loss= 0.49054 time= 0.20237\n",
      "Epoch: 0041 train_loss= 0.48920 time= 0.19274\n",
      "Epoch: 0042 train_loss= 0.48746 time= 0.20516\n",
      "Epoch: 0043 train_loss= 0.48525 time= 0.19847\n",
      "Epoch: 0044 train_loss= 0.48265 time= 0.18645\n",
      "Epoch: 0045 train_loss= 0.47985 time= 0.18814\n",
      "Epoch: 0046 train_loss= 0.47707 time= 0.19964\n",
      "Epoch: 0047 train_loss= 0.47444 time= 0.19531\n",
      "Epoch: 0048 train_loss= 0.47207 time= 0.18952\n",
      "Epoch: 0049 train_loss= 0.46998 time= 0.19380\n",
      "Epoch: 0050 train_loss= 0.46816 time= 0.18697\n",
      "Epoch: 0051 train_loss= 0.46656 time= 0.18802\n",
      "Epoch: 0052 train_loss= 0.46512 time= 0.18815\n",
      "Epoch: 0053 train_loss= 0.46380 time= 0.18528\n",
      "Epoch: 0054 train_loss= 0.46256 time= 0.18926\n",
      "Epoch: 0055 train_loss= 0.46138 time= 0.18648\n",
      "Epoch: 0056 train_loss= 0.46027 time= 0.18583\n",
      "Epoch: 0057 train_loss= 0.45922 time= 0.18981\n",
      "Epoch: 0058 train_loss= 0.45823 time= 0.18966\n",
      "Epoch: 0059 train_loss= 0.45731 time= 0.19458\n",
      "Epoch: 0060 train_loss= 0.45649 time= 0.20185\n",
      "Epoch: 0061 train_loss= 0.45576 time= 0.19771\n",
      "Epoch: 0062 train_loss= 0.45513 time= 0.19730\n",
      "Epoch: 0063 train_loss= 0.45458 time= 0.20926\n",
      "Epoch: 0064 train_loss= 0.45409 time= 0.19632\n",
      "Epoch: 0065 train_loss= 0.45362 time= 0.19886\n",
      "Epoch: 0066 train_loss= 0.45316 time= 0.19702\n",
      "Epoch: 0067 train_loss= 0.45269 time= 0.19679\n",
      "Epoch: 0068 train_loss= 0.45221 time= 0.19467\n",
      "Epoch: 0069 train_loss= 0.45171 time= 0.19337\n",
      "Epoch: 0070 train_loss= 0.45120 time= 0.20257\n",
      "Epoch: 0071 train_loss= 0.45069 time= 0.19935\n",
      "Epoch: 0072 train_loss= 0.45018 time= 0.19149\n",
      "Epoch: 0073 train_loss= 0.44968 time= 0.20099\n",
      "Epoch: 0074 train_loss= 0.44923 time= 0.18974\n",
      "Epoch: 0075 train_loss= 0.44883 time= 0.18977\n",
      "Epoch: 0076 train_loss= 0.44848 time= 0.18584\n",
      "Epoch: 0077 train_loss= 0.44817 time= 0.19115\n",
      "Epoch: 0078 train_loss= 0.44789 time= 0.19551\n",
      "Epoch: 0079 train_loss= 0.44762 time= 0.18892\n",
      "Epoch: 0080 train_loss= 0.44735 time= 0.18980\n",
      "Epoch: 0081 train_loss= 0.44708 time= 0.18122\n",
      "Epoch: 0082 train_loss= 0.44681 time= 0.18843\n",
      "Epoch: 0083 train_loss= 0.44655 time= 0.18755\n",
      "Epoch: 0084 train_loss= 0.44629 time= 0.20159\n",
      "Epoch: 0085 train_loss= 0.44602 time= 0.19978\n",
      "Epoch: 0086 train_loss= 0.44576 time= 0.19319\n",
      "Epoch: 0087 train_loss= 0.44551 time= 0.18923\n",
      "Epoch: 0088 train_loss= 0.44526 time= 0.18985\n",
      "Epoch: 0089 train_loss= 0.44502 time= 0.19718\n",
      "Epoch: 0090 train_loss= 0.44478 time= 0.21284\n",
      "Epoch: 0091 train_loss= 0.44453 time= 0.20946\n",
      "Epoch: 0092 train_loss= 0.44429 time= 0.20277\n",
      "Epoch: 0093 train_loss= 0.44405 time= 0.20048\n",
      "Epoch: 0094 train_loss= 0.44380 time= 0.19197\n",
      "Epoch: 0095 train_loss= 0.44354 time= 0.18844\n",
      "Epoch: 0096 train_loss= 0.44328 time= 0.18928\n",
      "Epoch: 0097 train_loss= 0.44301 time= 0.18790\n",
      "Epoch: 0098 train_loss= 0.44273 time= 0.19276\n",
      "Epoch: 0099 train_loss= 0.44244 time= 0.18371\n",
      "Epoch: 0100 train_loss= 0.44213 time= 0.18373\n",
      "Epoch: 0101 train_loss= 0.44182 time= 0.18734\n",
      "Epoch: 0102 train_loss= 0.44150 time= 0.19937\n",
      "Epoch: 0103 train_loss= 0.44117 time= 0.19361\n",
      "Epoch: 0104 train_loss= 0.44083 time= 0.19268\n",
      "Epoch: 0105 train_loss= 0.44049 time= 0.19014\n",
      "Epoch: 0106 train_loss= 0.44014 time= 0.19289\n",
      "Epoch: 0107 train_loss= 0.43979 time= 0.19627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0108 train_loss= 0.43944 time= 0.19282\n",
      "Epoch: 0109 train_loss= 0.43908 time= 0.18685\n",
      "Epoch: 0110 train_loss= 0.43872 time= 0.19646\n",
      "Epoch: 0111 train_loss= 0.43836 time= 0.18498\n",
      "Epoch: 0112 train_loss= 0.43801 time= 0.18998\n",
      "Epoch: 0113 train_loss= 0.43765 time= 0.19477\n",
      "Epoch: 0114 train_loss= 0.43730 time= 0.20163\n",
      "Epoch: 0115 train_loss= 0.43694 time= 0.19746\n",
      "Epoch: 0116 train_loss= 0.43659 time= 0.20224\n",
      "Epoch: 0117 train_loss= 0.43623 time= 0.19865\n",
      "Epoch: 0118 train_loss= 0.43587 time= 0.19258\n",
      "Epoch: 0119 train_loss= 0.43551 time= 0.20001\n",
      "Epoch: 0120 train_loss= 0.43515 time= 0.20052\n",
      "Epoch: 0121 train_loss= 0.43477 time= 0.20451\n",
      "Epoch: 0122 train_loss= 0.43439 time= 0.19662\n",
      "Epoch: 0123 train_loss= 0.43401 time= 0.19267\n",
      "Epoch: 0124 train_loss= 0.43361 time= 0.19847\n",
      "Epoch: 0125 train_loss= 0.43321 time= 0.20047\n",
      "Epoch: 0126 train_loss= 0.43280 time= 0.19993\n",
      "Epoch: 0127 train_loss= 0.43239 time= 0.19878\n",
      "Epoch: 0128 train_loss= 0.43197 time= 0.19079\n",
      "Epoch: 0129 train_loss= 0.43156 time= 0.19915\n",
      "Epoch: 0130 train_loss= 0.43115 time= 0.19327\n",
      "Epoch: 0131 train_loss= 0.43074 time= 0.20653\n",
      "Epoch: 0132 train_loss= 0.43035 time= 0.20094\n",
      "Epoch: 0133 train_loss= 0.42996 time= 0.18739\n",
      "Epoch: 0134 train_loss= 0.42960 time= 0.19448\n",
      "Epoch: 0135 train_loss= 0.42924 time= 0.19924\n",
      "Epoch: 0136 train_loss= 0.42890 time= 0.19487\n",
      "Epoch: 0137 train_loss= 0.42858 time= 0.19037\n",
      "Epoch: 0138 train_loss= 0.42827 time= 0.18643\n",
      "Epoch: 0139 train_loss= 0.42797 time= 0.19805\n",
      "Epoch: 0140 train_loss= 0.42769 time= 0.18722\n",
      "Epoch: 0141 train_loss= 0.42742 time= 0.19518\n",
      "Epoch: 0142 train_loss= 0.42715 time= 0.18458\n",
      "Epoch: 0143 train_loss= 0.42690 time= 0.19394\n",
      "Epoch: 0144 train_loss= 0.42665 time= 0.19475\n",
      "Epoch: 0145 train_loss= 0.42640 time= 0.19557\n",
      "Epoch: 0146 train_loss= 0.42617 time= 0.19705\n",
      "Epoch: 0147 train_loss= 0.42594 time= 0.19672\n",
      "Epoch: 0148 train_loss= 0.42571 time= 0.19521\n",
      "Epoch: 0149 train_loss= 0.42549 time= 0.20367\n",
      "Epoch: 0150 train_loss= 0.42527 time= 0.19110\n",
      "Epoch: 0151 train_loss= 0.42505 time= 0.19028\n",
      "Epoch: 0152 train_loss= 0.42483 time= 0.19197\n",
      "Epoch: 0153 train_loss= 0.42461 time= 0.19394\n",
      "Epoch: 0154 train_loss= 0.42439 time= 0.19646\n",
      "Epoch: 0155 train_loss= 0.42416 time= 0.20246\n",
      "Epoch: 0156 train_loss= 0.42393 time= 0.20073\n",
      "Epoch: 0157 train_loss= 0.42369 time= 0.18984\n",
      "Epoch: 0158 train_loss= 0.42345 time= 0.18695\n",
      "Epoch: 0159 train_loss= 0.42320 time= 0.18686\n",
      "Epoch: 0160 train_loss= 0.42294 time= 0.18756\n",
      "Epoch: 0161 train_loss= 0.42268 time= 0.20224\n",
      "Epoch: 0162 train_loss= 0.42241 time= 0.19371\n",
      "Epoch: 0163 train_loss= 0.42214 time= 0.18432\n",
      "Epoch: 0164 train_loss= 0.42187 time= 0.18691\n",
      "Epoch: 0165 train_loss= 0.42159 time= 0.18986\n",
      "Epoch: 0166 train_loss= 0.42131 time= 0.18365\n",
      "Epoch: 0167 train_loss= 0.42103 time= 0.18490\n",
      "Epoch: 0168 train_loss= 0.42075 time= 0.18107\n",
      "Epoch: 0169 train_loss= 0.42048 time= 0.18134\n",
      "Epoch: 0170 train_loss= 0.42020 time= 0.17970\n",
      "Epoch: 0171 train_loss= 0.41993 time= 0.18100\n",
      "Epoch: 0172 train_loss= 0.41966 time= 0.18539\n",
      "Epoch: 0173 train_loss= 0.41940 time= 0.18398\n",
      "Epoch: 0174 train_loss= 0.41915 time= 0.18509\n",
      "Epoch: 0175 train_loss= 0.41890 time= 0.18446\n",
      "Epoch: 0176 train_loss= 0.41866 time= 0.18624\n",
      "Epoch: 0177 train_loss= 0.41843 time= 0.18561\n",
      "Epoch: 0178 train_loss= 0.41820 time= 0.19222\n",
      "Epoch: 0179 train_loss= 0.41799 time= 0.19513\n",
      "Epoch: 0180 train_loss= 0.41778 time= 0.18559\n",
      "Epoch: 0181 train_loss= 0.41758 time= 0.18611\n",
      "Epoch: 0182 train_loss= 0.41738 time= 0.18759\n",
      "Epoch: 0183 train_loss= 0.41719 time= 0.18856\n",
      "Epoch: 0184 train_loss= 0.41700 time= 0.20103\n",
      "Epoch: 0185 train_loss= 0.41682 time= 0.18795\n",
      "Epoch: 0186 train_loss= 0.41664 time= 0.19315\n",
      "Epoch: 0187 train_loss= 0.41646 time= 0.19841\n",
      "Epoch: 0188 train_loss= 0.41628 time= 0.20322\n",
      "Epoch: 0189 train_loss= 0.41610 time= 0.19615\n",
      "Epoch: 0190 train_loss= 0.41593 time= 0.19809\n",
      "Epoch: 0191 train_loss= 0.41575 time= 0.19885\n",
      "Epoch: 0192 train_loss= 0.41558 time= 0.20138\n",
      "Epoch: 0193 train_loss= 0.41540 time= 0.20133\n",
      "Epoch: 0194 train_loss= 0.41523 time= 0.19770\n",
      "Epoch: 0195 train_loss= 0.41506 time= 0.20060\n",
      "Epoch: 0196 train_loss= 0.41490 time= 0.19267\n",
      "Epoch: 0197 train_loss= 0.41473 time= 0.19560\n",
      "Epoch: 0198 train_loss= 0.41457 time= 0.18917\n",
      "Epoch: 0199 train_loss= 0.41442 time= 0.18683\n",
      "Epoch: 0200 train_loss= 0.41427 time= 0.18568\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79732 time= 0.24437\n",
      "Epoch: 0002 train_loss= 0.79655 time= 0.19784\n",
      "Epoch: 0003 train_loss= 0.79457 time= 0.20094\n",
      "Epoch: 0004 train_loss= 0.79056 time= 0.19222\n",
      "Epoch: 0005 train_loss= 0.78354 time= 0.18991\n",
      "Epoch: 0006 train_loss= 0.77265 time= 0.20211\n",
      "Epoch: 0007 train_loss= 0.75780 time= 0.18898\n",
      "Epoch: 0008 train_loss= 0.74060 time= 0.18924\n",
      "Epoch: 0009 train_loss= 0.72585 time= 0.18885\n",
      "Epoch: 0010 train_loss= 0.72149 time= 0.18865\n",
      "Epoch: 0011 train_loss= 0.72697 time= 0.18745\n",
      "Epoch: 0012 train_loss= 0.72670 time= 0.19213\n",
      "Epoch: 0013 train_loss= 0.71853 time= 0.18811\n",
      "Epoch: 0014 train_loss= 0.70827 time= 0.18167\n",
      "Epoch: 0015 train_loss= 0.70015 time= 0.19374\n",
      "Epoch: 0016 train_loss= 0.69495 time= 0.18596\n",
      "Epoch: 0017 train_loss= 0.69128 time= 0.19532\n",
      "Epoch: 0018 train_loss= 0.68738 time= 0.19726\n",
      "Epoch: 0019 train_loss= 0.68212 time= 0.19215\n",
      "Epoch: 0020 train_loss= 0.67512 time= 0.19313\n",
      "Epoch: 0021 train_loss= 0.66654 time= 0.19200\n",
      "Epoch: 0022 train_loss= 0.65687 time= 0.19642\n",
      "Epoch: 0023 train_loss= 0.64678 time= 0.19640\n",
      "Epoch: 0024 train_loss= 0.63685 time= 0.18529\n",
      "Epoch: 0025 train_loss= 0.62746 time= 0.18816\n",
      "Epoch: 0026 train_loss= 0.61871 time= 0.19248\n",
      "Epoch: 0027 train_loss= 0.61056 time= 0.19205\n",
      "Epoch: 0028 train_loss= 0.60299 time= 0.19432\n",
      "Epoch: 0029 train_loss= 0.59611 time= 0.18967\n",
      "Epoch: 0030 train_loss= 0.59000 time= 0.19122\n",
      "Epoch: 0031 train_loss= 0.58463 time= 0.19842\n",
      "Epoch: 0032 train_loss= 0.57970 time= 0.18783\n",
      "Epoch: 0033 train_loss= 0.57484 time= 0.18896\n",
      "Epoch: 0034 train_loss= 0.56969 time= 0.18517\n",
      "Epoch: 0035 train_loss= 0.56406 time= 0.18427\n",
      "Epoch: 0036 train_loss= 0.55798 time= 0.18692\n",
      "Epoch: 0037 train_loss= 0.55159 time= 0.19548\n",
      "Epoch: 0038 train_loss= 0.54508 time= 0.18818\n",
      "Epoch: 0039 train_loss= 0.53864 time= 0.18828\n",
      "Epoch: 0040 train_loss= 0.53247 time= 0.19107\n",
      "Epoch: 0041 train_loss= 0.52675 time= 0.18994\n",
      "Epoch: 0042 train_loss= 0.52163 time= 0.19359\n",
      "Epoch: 0043 train_loss= 0.51720 time= 0.19213\n",
      "Epoch: 0044 train_loss= 0.51341 time= 0.19258\n",
      "Epoch: 0045 train_loss= 0.51009 time= 0.18457\n",
      "Epoch: 0046 train_loss= 0.50700 time= 0.18642\n",
      "Epoch: 0047 train_loss= 0.50399 time= 0.19113\n",
      "Epoch: 0048 train_loss= 0.50108 time= 0.19540\n",
      "Epoch: 0049 train_loss= 0.49838 time= 0.19416\n",
      "Epoch: 0050 train_loss= 0.49602 time= 0.18637\n",
      "Epoch: 0051 train_loss= 0.49399 time= 0.19246\n",
      "Epoch: 0052 train_loss= 0.49218 time= 0.20127\n",
      "Epoch: 0053 train_loss= 0.49036 time= 0.19003\n",
      "Epoch: 0054 train_loss= 0.48835 time= 0.19202\n",
      "Epoch: 0055 train_loss= 0.48607 time= 0.18738\n",
      "Epoch: 0056 train_loss= 0.48359 time= 0.18702\n",
      "Epoch: 0057 train_loss= 0.48103 time= 0.18620\n",
      "Epoch: 0058 train_loss= 0.47853 time= 0.18658\n",
      "Epoch: 0059 train_loss= 0.47615 time= 0.18829\n",
      "Epoch: 0060 train_loss= 0.47392 time= 0.19319\n",
      "Epoch: 0061 train_loss= 0.47184 time= 0.19082\n",
      "Epoch: 0062 train_loss= 0.46992 time= 0.18859\n",
      "Epoch: 0063 train_loss= 0.46817 time= 0.19171\n",
      "Epoch: 0064 train_loss= 0.46663 time= 0.19805\n",
      "Epoch: 0065 train_loss= 0.46529 time= 0.19261\n",
      "Epoch: 0066 train_loss= 0.46415 time= 0.18787\n",
      "Epoch: 0067 train_loss= 0.46314 time= 0.19033\n",
      "Epoch: 0068 train_loss= 0.46219 time= 0.19392\n",
      "Epoch: 0069 train_loss= 0.46126 time= 0.18793\n",
      "Epoch: 0070 train_loss= 0.46032 time= 0.19069\n",
      "Epoch: 0071 train_loss= 0.45939 time= 0.18904\n",
      "Epoch: 0072 train_loss= 0.45849 time= 0.19053\n",
      "Epoch: 0073 train_loss= 0.45763 time= 0.19523\n",
      "Epoch: 0074 train_loss= 0.45681 time= 0.18975\n",
      "Epoch: 0075 train_loss= 0.45600 time= 0.19383\n",
      "Epoch: 0076 train_loss= 0.45521 time= 0.18617\n",
      "Epoch: 0077 train_loss= 0.45442 time= 0.18474\n",
      "Epoch: 0078 train_loss= 0.45364 time= 0.19049\n",
      "Epoch: 0079 train_loss= 0.45290 time= 0.19609\n",
      "Epoch: 0080 train_loss= 0.45218 time= 0.18767\n",
      "Epoch: 0081 train_loss= 0.45149 time= 0.19624\n",
      "Epoch: 0082 train_loss= 0.45081 time= 0.19228\n",
      "Epoch: 0083 train_loss= 0.45016 time= 0.19220\n",
      "Epoch: 0084 train_loss= 0.44953 time= 0.19911\n",
      "Epoch: 0085 train_loss= 0.44892 time= 0.18974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0086 train_loss= 0.44834 time= 0.19178\n",
      "Epoch: 0087 train_loss= 0.44779 time= 0.18030\n",
      "Epoch: 0088 train_loss= 0.44726 time= 0.18139\n",
      "Epoch: 0089 train_loss= 0.44674 time= 0.18359\n",
      "Epoch: 0090 train_loss= 0.44622 time= 0.19149\n",
      "Epoch: 0091 train_loss= 0.44571 time= 0.19348\n",
      "Epoch: 0092 train_loss= 0.44521 time= 0.19847\n",
      "Epoch: 0093 train_loss= 0.44472 time= 0.19448\n",
      "Epoch: 0094 train_loss= 0.44423 time= 0.19149\n",
      "Epoch: 0095 train_loss= 0.44376 time= 0.18833\n",
      "Epoch: 0096 train_loss= 0.44330 time= 0.18831\n",
      "Epoch: 0097 train_loss= 0.44284 time= 0.18918\n",
      "Epoch: 0098 train_loss= 0.44239 time= 0.18375\n",
      "Epoch: 0099 train_loss= 0.44195 time= 0.19310\n",
      "Epoch: 0100 train_loss= 0.44152 time= 0.18949\n",
      "Epoch: 0101 train_loss= 0.44109 time= 0.19094\n",
      "Epoch: 0102 train_loss= 0.44067 time= 0.19600\n",
      "Epoch: 0103 train_loss= 0.44026 time= 0.19820\n",
      "Epoch: 0104 train_loss= 0.43985 time= 0.18693\n",
      "Epoch: 0105 train_loss= 0.43945 time= 0.19164\n",
      "Epoch: 0106 train_loss= 0.43906 time= 0.19134\n",
      "Epoch: 0107 train_loss= 0.43868 time= 0.19448\n",
      "Epoch: 0108 train_loss= 0.43831 time= 0.18572\n",
      "Epoch: 0109 train_loss= 0.43794 time= 0.19137\n",
      "Epoch: 0110 train_loss= 0.43758 time= 0.19547\n",
      "Epoch: 0111 train_loss= 0.43722 time= 0.19363\n",
      "Epoch: 0112 train_loss= 0.43687 time= 0.20134\n",
      "Epoch: 0113 train_loss= 0.43653 time= 0.21521\n",
      "Epoch: 0114 train_loss= 0.43618 time= 0.20237\n",
      "Epoch: 0115 train_loss= 0.43584 time= 0.19331\n",
      "Epoch: 0116 train_loss= 0.43550 time= 0.18871\n",
      "Epoch: 0117 train_loss= 0.43516 time= 0.18915\n",
      "Epoch: 0118 train_loss= 0.43482 time= 0.18470\n",
      "Epoch: 0119 train_loss= 0.43448 time= 0.18387\n",
      "Epoch: 0120 train_loss= 0.43414 time= 0.18106\n",
      "Epoch: 0121 train_loss= 0.43380 time= 0.19239\n",
      "Epoch: 0122 train_loss= 0.43346 time= 0.19059\n",
      "Epoch: 0123 train_loss= 0.43312 time= 0.20289\n",
      "Epoch: 0124 train_loss= 0.43278 time= 0.18619\n",
      "Epoch: 0125 train_loss= 0.43244 time= 0.19053\n",
      "Epoch: 0126 train_loss= 0.43211 time= 0.18616\n",
      "Epoch: 0127 train_loss= 0.43177 time= 0.18524\n",
      "Epoch: 0128 train_loss= 0.43144 time= 0.18111\n",
      "Epoch: 0129 train_loss= 0.43112 time= 0.18281\n",
      "Epoch: 0130 train_loss= 0.43079 time= 0.19082\n",
      "Epoch: 0131 train_loss= 0.43047 time= 0.18949\n",
      "Epoch: 0132 train_loss= 0.43016 time= 0.19507\n",
      "Epoch: 0133 train_loss= 0.42985 time= 0.19790\n",
      "Epoch: 0134 train_loss= 0.42954 time= 0.18901\n",
      "Epoch: 0135 train_loss= 0.42924 time= 0.18711\n",
      "Epoch: 0136 train_loss= 0.42895 time= 0.19530\n",
      "Epoch: 0137 train_loss= 0.42866 time= 0.19158\n",
      "Epoch: 0138 train_loss= 0.42838 time= 0.19538\n",
      "Epoch: 0139 train_loss= 0.42811 time= 0.19794\n",
      "Epoch: 0140 train_loss= 0.42784 time= 0.20157\n",
      "Epoch: 0141 train_loss= 0.42758 time= 0.20935\n",
      "Epoch: 0142 train_loss= 0.42733 time= 0.19219\n",
      "Epoch: 0143 train_loss= 0.42709 time= 0.18578\n",
      "Epoch: 0144 train_loss= 0.42685 time= 0.18993\n",
      "Epoch: 0145 train_loss= 0.42662 time= 0.19266\n",
      "Epoch: 0146 train_loss= 0.42640 time= 0.18331\n",
      "Epoch: 0147 train_loss= 0.42618 time= 0.19010\n",
      "Epoch: 0148 train_loss= 0.42597 time= 0.18852\n",
      "Epoch: 0149 train_loss= 0.42577 time= 0.18440\n",
      "Epoch: 0150 train_loss= 0.42558 time= 0.18547\n",
      "Epoch: 0151 train_loss= 0.42539 time= 0.18975\n",
      "Epoch: 0152 train_loss= 0.42521 time= 0.19507\n",
      "Epoch: 0153 train_loss= 0.42503 time= 0.18611\n",
      "Epoch: 0154 train_loss= 0.42486 time= 0.19705\n",
      "Epoch: 0155 train_loss= 0.42469 time= 0.18521\n",
      "Epoch: 0156 train_loss= 0.42453 time= 0.19134\n",
      "Epoch: 0157 train_loss= 0.42436 time= 0.20458\n",
      "Epoch: 0158 train_loss= 0.42421 time= 0.19963\n",
      "Epoch: 0159 train_loss= 0.42405 time= 0.19558\n",
      "Epoch: 0160 train_loss= 0.42389 time= 0.19128\n",
      "Epoch: 0161 train_loss= 0.42374 time= 0.18259\n",
      "Epoch: 0162 train_loss= 0.42359 time= 0.19926\n",
      "Epoch: 0163 train_loss= 0.42343 time= 0.19052\n",
      "Epoch: 0164 train_loss= 0.42328 time= 0.18605\n",
      "Epoch: 0165 train_loss= 0.42313 time= 0.19259\n",
      "Epoch: 0166 train_loss= 0.42298 time= 0.18512\n",
      "Epoch: 0167 train_loss= 0.42283 time= 0.19011\n",
      "Epoch: 0168 train_loss= 0.42268 time= 0.19481\n",
      "Epoch: 0169 train_loss= 0.42253 time= 0.18563\n",
      "Epoch: 0170 train_loss= 0.42238 time= 0.19054\n",
      "Epoch: 0171 train_loss= 0.42223 time= 0.19107\n",
      "Epoch: 0172 train_loss= 0.42208 time= 0.19067\n",
      "Epoch: 0173 train_loss= 0.42193 time= 0.18371\n",
      "Epoch: 0174 train_loss= 0.42178 time= 0.19103\n",
      "Epoch: 0175 train_loss= 0.42163 time= 0.18822\n",
      "Epoch: 0176 train_loss= 0.42147 time= 0.18833\n",
      "Epoch: 0177 train_loss= 0.42132 time= 0.18546\n",
      "Epoch: 0178 train_loss= 0.42117 time= 0.18339\n",
      "Epoch: 0179 train_loss= 0.42102 time= 0.18361\n",
      "Epoch: 0180 train_loss= 0.42087 time= 0.19334\n",
      "Epoch: 0181 train_loss= 0.42072 time= 0.18717\n",
      "Epoch: 0182 train_loss= 0.42057 time= 0.18780\n",
      "Epoch: 0183 train_loss= 0.42043 time= 0.18203\n",
      "Epoch: 0184 train_loss= 0.42028 time= 0.18292\n",
      "Epoch: 0185 train_loss= 0.42013 time= 0.18124\n",
      "Epoch: 0186 train_loss= 0.41998 time= 0.18871\n",
      "Epoch: 0187 train_loss= 0.41983 time= 0.18681\n",
      "Epoch: 0188 train_loss= 0.41969 time= 0.18210\n",
      "Epoch: 0189 train_loss= 0.41954 time= 0.19512\n",
      "Epoch: 0190 train_loss= 0.41939 time= 0.19210\n",
      "Epoch: 0191 train_loss= 0.41924 time= 0.18625\n",
      "Epoch: 0192 train_loss= 0.41909 time= 0.18821\n",
      "Epoch: 0193 train_loss= 0.41894 time= 0.18972\n",
      "Epoch: 0194 train_loss= 0.41878 time= 0.18339\n",
      "Epoch: 0195 train_loss= 0.41863 time= 0.18602\n",
      "Epoch: 0196 train_loss= 0.41847 time= 0.19978\n",
      "Epoch: 0197 train_loss= 0.41831 time= 0.19070\n",
      "Epoch: 0198 train_loss= 0.41816 time= 0.20078\n",
      "Epoch: 0199 train_loss= 0.41800 time= 0.18877\n",
      "Epoch: 0200 train_loss= 0.41784 time= 0.19311\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79732 time= 0.23881\n",
      "Epoch: 0002 train_loss= 0.79650 time= 0.18876\n",
      "Epoch: 0003 train_loss= 0.79444 time= 0.18713\n",
      "Epoch: 0004 train_loss= 0.79037 time= 0.18736\n",
      "Epoch: 0005 train_loss= 0.78345 time= 0.18514\n",
      "Epoch: 0006 train_loss= 0.77298 time= 0.19756\n",
      "Epoch: 0007 train_loss= 0.75883 time= 0.18410\n",
      "Epoch: 0008 train_loss= 0.74219 time= 0.19230\n",
      "Epoch: 0009 train_loss= 0.72653 time= 0.19590\n",
      "Epoch: 0010 train_loss= 0.71802 time= 0.19931\n",
      "Epoch: 0011 train_loss= 0.72022 time= 0.19105\n",
      "Epoch: 0012 train_loss= 0.72220 time= 0.19872\n",
      "Epoch: 0013 train_loss= 0.71630 time= 0.19548\n",
      "Epoch: 0014 train_loss= 0.70574 time= 0.22239\n",
      "Epoch: 0015 train_loss= 0.69518 time= 0.20804\n",
      "Epoch: 0016 train_loss= 0.68685 time= 0.19935\n",
      "Epoch: 0017 train_loss= 0.68046 time= 0.20575\n",
      "Epoch: 0018 train_loss= 0.67453 time= 0.20200\n",
      "Epoch: 0019 train_loss= 0.66764 time= 0.20886\n",
      "Epoch: 0020 train_loss= 0.65904 time= 0.20064\n",
      "Epoch: 0021 train_loss= 0.64867 time= 0.19907\n",
      "Epoch: 0022 train_loss= 0.63688 time= 0.19992\n",
      "Epoch: 0023 train_loss= 0.62438 time= 0.19309\n",
      "Epoch: 0024 train_loss= 0.61198 time= 0.19519\n",
      "Epoch: 0025 train_loss= 0.60040 time= 0.20255\n",
      "Epoch: 0026 train_loss= 0.59003 time= 0.20103\n",
      "Epoch: 0027 train_loss= 0.58088 time= 0.20031\n",
      "Epoch: 0028 train_loss= 0.57272 time= 0.20092\n",
      "Epoch: 0029 train_loss= 0.56528 time= 0.19823\n",
      "Epoch: 0030 train_loss= 0.55837 time= 0.20200\n",
      "Epoch: 0031 train_loss= 0.55176 time= 0.19311\n",
      "Epoch: 0032 train_loss= 0.54521 time= 0.20225\n",
      "Epoch: 0033 train_loss= 0.53857 time= 0.19739\n",
      "Epoch: 0034 train_loss= 0.53185 time= 0.18766\n",
      "Epoch: 0035 train_loss= 0.52521 time= 0.19260\n",
      "Epoch: 0036 train_loss= 0.51898 time= 0.19255\n",
      "Epoch: 0037 train_loss= 0.51354 time= 0.19324\n",
      "Epoch: 0038 train_loss= 0.50920 time= 0.19547\n",
      "Epoch: 0039 train_loss= 0.50603 time= 0.19133\n",
      "Epoch: 0040 train_loss= 0.50382 time= 0.19176\n",
      "Epoch: 0041 train_loss= 0.50218 time= 0.18683\n",
      "Epoch: 0042 train_loss= 0.50067 time= 0.19604\n",
      "Epoch: 0043 train_loss= 0.49900 time= 0.19512\n",
      "Epoch: 0044 train_loss= 0.49702 time= 0.19981\n",
      "Epoch: 0045 train_loss= 0.49473 time= 0.19842\n",
      "Epoch: 0046 train_loss= 0.49221 time= 0.19186\n",
      "Epoch: 0047 train_loss= 0.48955 time= 0.18655\n",
      "Epoch: 0048 train_loss= 0.48685 time= 0.19167\n",
      "Epoch: 0049 train_loss= 0.48420 time= 0.19015\n",
      "Epoch: 0050 train_loss= 0.48164 time= 0.18947\n",
      "Epoch: 0051 train_loss= 0.47924 time= 0.19263\n",
      "Epoch: 0052 train_loss= 0.47700 time= 0.18403\n",
      "Epoch: 0053 train_loss= 0.47496 time= 0.19061\n",
      "Epoch: 0054 train_loss= 0.47311 time= 0.19348\n",
      "Epoch: 0055 train_loss= 0.47145 time= 0.18977\n",
      "Epoch: 0056 train_loss= 0.46996 time= 0.19475\n",
      "Epoch: 0057 train_loss= 0.46862 time= 0.18511\n",
      "Epoch: 0058 train_loss= 0.46741 time= 0.20038\n",
      "Epoch: 0059 train_loss= 0.46632 time= 0.19894\n",
      "Epoch: 0060 train_loss= 0.46534 time= 0.19301\n",
      "Epoch: 0061 train_loss= 0.46445 time= 0.20004\n",
      "Epoch: 0062 train_loss= 0.46365 time= 0.18905\n",
      "Epoch: 0063 train_loss= 0.46293 time= 0.19287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0064 train_loss= 0.46226 time= 0.18958\n",
      "Epoch: 0065 train_loss= 0.46163 time= 0.18865\n",
      "Epoch: 0066 train_loss= 0.46102 time= 0.18943\n",
      "Epoch: 0067 train_loss= 0.46041 time= 0.18947\n",
      "Epoch: 0068 train_loss= 0.45980 time= 0.19089\n",
      "Epoch: 0069 train_loss= 0.45919 time= 0.18334\n",
      "Epoch: 0070 train_loss= 0.45858 time= 0.19709\n",
      "Epoch: 0071 train_loss= 0.45796 time= 0.18409\n",
      "Epoch: 0072 train_loss= 0.45734 time= 0.18276\n",
      "Epoch: 0073 train_loss= 0.45673 time= 0.18238\n",
      "Epoch: 0074 train_loss= 0.45613 time= 0.19160\n",
      "Epoch: 0075 train_loss= 0.45554 time= 0.18755\n",
      "Epoch: 0076 train_loss= 0.45497 time= 0.18898\n",
      "Epoch: 0077 train_loss= 0.45443 time= 0.18785\n",
      "Epoch: 0078 train_loss= 0.45389 time= 0.19024\n",
      "Epoch: 0079 train_loss= 0.45337 time= 0.18797\n",
      "Epoch: 0080 train_loss= 0.45285 time= 0.19852\n",
      "Epoch: 0081 train_loss= 0.45234 time= 0.18799\n",
      "Epoch: 0082 train_loss= 0.45182 time= 0.18990\n",
      "Epoch: 0083 train_loss= 0.45130 time= 0.18241\n",
      "Epoch: 0084 train_loss= 0.45076 time= 0.18585\n",
      "Epoch: 0085 train_loss= 0.45022 time= 0.18618\n",
      "Epoch: 0086 train_loss= 0.44968 time= 0.19038\n",
      "Epoch: 0087 train_loss= 0.44914 time= 0.18990\n",
      "Epoch: 0088 train_loss= 0.44860 time= 0.18496\n",
      "Epoch: 0089 train_loss= 0.44808 time= 0.18415\n",
      "Epoch: 0090 train_loss= 0.44757 time= 0.18667\n",
      "Epoch: 0091 train_loss= 0.44709 time= 0.18850\n",
      "Epoch: 0092 train_loss= 0.44662 time= 0.19149\n",
      "Epoch: 0093 train_loss= 0.44617 time= 0.18361\n",
      "Epoch: 0094 train_loss= 0.44575 time= 0.18529\n",
      "Epoch: 0095 train_loss= 0.44534 time= 0.19384\n",
      "Epoch: 0096 train_loss= 0.44494 time= 0.19737\n",
      "Epoch: 0097 train_loss= 0.44456 time= 0.19433\n",
      "Epoch: 0098 train_loss= 0.44417 time= 0.19694\n",
      "Epoch: 0099 train_loss= 0.44379 time= 0.19206\n",
      "Epoch: 0100 train_loss= 0.44339 time= 0.19044\n",
      "Epoch: 0101 train_loss= 0.44298 time= 0.19295\n",
      "Epoch: 0102 train_loss= 0.44256 time= 0.19284\n",
      "Epoch: 0103 train_loss= 0.44213 time= 0.20158\n",
      "Epoch: 0104 train_loss= 0.44169 time= 0.18864\n",
      "Epoch: 0105 train_loss= 0.44123 time= 0.18691\n",
      "Epoch: 0106 train_loss= 0.44078 time= 0.19587\n",
      "Epoch: 0107 train_loss= 0.44031 time= 0.19450\n",
      "Epoch: 0108 train_loss= 0.43985 time= 0.18412\n",
      "Epoch: 0109 train_loss= 0.43939 time= 0.19612\n",
      "Epoch: 0110 train_loss= 0.43893 time= 0.18954\n",
      "Epoch: 0111 train_loss= 0.43846 time= 0.18973\n",
      "Epoch: 0112 train_loss= 0.43800 time= 0.18676\n",
      "Epoch: 0113 train_loss= 0.43755 time= 0.18777\n",
      "Epoch: 0114 train_loss= 0.43709 time= 0.19303\n",
      "Epoch: 0115 train_loss= 0.43664 time= 0.18951\n",
      "Epoch: 0116 train_loss= 0.43620 time= 0.19044\n",
      "Epoch: 0117 train_loss= 0.43577 time= 0.18651\n",
      "Epoch: 0118 train_loss= 0.43536 time= 0.18939\n",
      "Epoch: 0119 train_loss= 0.43496 time= 0.18875\n",
      "Epoch: 0120 train_loss= 0.43459 time= 0.19135\n",
      "Epoch: 0121 train_loss= 0.43423 time= 0.20323\n",
      "Epoch: 0122 train_loss= 0.43390 time= 0.19168\n",
      "Epoch: 0123 train_loss= 0.43358 time= 0.19092\n",
      "Epoch: 0124 train_loss= 0.43329 time= 0.18892\n",
      "Epoch: 0125 train_loss= 0.43301 time= 0.18455\n",
      "Epoch: 0126 train_loss= 0.43275 time= 0.18831\n",
      "Epoch: 0127 train_loss= 0.43250 time= 0.19287\n",
      "Epoch: 0128 train_loss= 0.43226 time= 0.18946\n",
      "Epoch: 0129 train_loss= 0.43203 time= 0.19153\n",
      "Epoch: 0130 train_loss= 0.43180 time= 0.18697\n",
      "Epoch: 0131 train_loss= 0.43157 time= 0.19245\n",
      "Epoch: 0132 train_loss= 0.43135 time= 0.19259\n",
      "Epoch: 0133 train_loss= 0.43112 time= 0.18547\n",
      "Epoch: 0134 train_loss= 0.43090 time= 0.19076\n",
      "Epoch: 0135 train_loss= 0.43066 time= 0.18275\n",
      "Epoch: 0136 train_loss= 0.43043 time= 0.18753\n",
      "Epoch: 0137 train_loss= 0.43019 time= 0.18965\n",
      "Epoch: 0138 train_loss= 0.42994 time= 0.18959\n",
      "Epoch: 0139 train_loss= 0.42969 time= 0.18691\n",
      "Epoch: 0140 train_loss= 0.42943 time= 0.19674\n",
      "Epoch: 0141 train_loss= 0.42916 time= 0.18988\n",
      "Epoch: 0142 train_loss= 0.42888 time= 0.18652\n",
      "Epoch: 0143 train_loss= 0.42859 time= 0.19016\n",
      "Epoch: 0144 train_loss= 0.42830 time= 0.18670\n",
      "Epoch: 0145 train_loss= 0.42800 time= 0.19063\n",
      "Epoch: 0146 train_loss= 0.42769 time= 0.17963\n",
      "Epoch: 0147 train_loss= 0.42737 time= 0.18924\n",
      "Epoch: 0148 train_loss= 0.42705 time= 0.18517\n",
      "Epoch: 0149 train_loss= 0.42673 time= 0.19011\n",
      "Epoch: 0150 train_loss= 0.42640 time= 0.18686\n",
      "Epoch: 0151 train_loss= 0.42607 time= 0.18833\n",
      "Epoch: 0152 train_loss= 0.42574 time= 0.19597\n",
      "Epoch: 0153 train_loss= 0.42541 time= 0.18642\n",
      "Epoch: 0154 train_loss= 0.42509 time= 0.19068\n",
      "Epoch: 0155 train_loss= 0.42478 time= 0.18824\n",
      "Epoch: 0156 train_loss= 0.42447 time= 0.18786\n",
      "Epoch: 0157 train_loss= 0.42417 time= 0.18517\n",
      "Epoch: 0158 train_loss= 0.42387 time= 0.18826\n",
      "Epoch: 0159 train_loss= 0.42359 time= 0.18969\n",
      "Epoch: 0160 train_loss= 0.42331 time= 0.18528\n",
      "Epoch: 0161 train_loss= 0.42304 time= 0.18605\n",
      "Epoch: 0162 train_loss= 0.42277 time= 0.19576\n",
      "Epoch: 0163 train_loss= 0.42251 time= 0.20585\n",
      "Epoch: 0164 train_loss= 0.42225 time= 0.19994\n",
      "Epoch: 0165 train_loss= 0.42200 time= 0.19252\n",
      "Epoch: 0166 train_loss= 0.42175 time= 0.19478\n",
      "Epoch: 0167 train_loss= 0.42150 time= 0.19451\n",
      "Epoch: 0168 train_loss= 0.42126 time= 0.19677\n",
      "Epoch: 0169 train_loss= 0.42101 time= 0.19453\n",
      "Epoch: 0170 train_loss= 0.42077 time= 0.19743\n",
      "Epoch: 0171 train_loss= 0.42053 time= 0.19178\n",
      "Epoch: 0172 train_loss= 0.42030 time= 0.19847\n",
      "Epoch: 0173 train_loss= 0.42006 time= 0.19713\n",
      "Epoch: 0174 train_loss= 0.41982 time= 0.19199\n",
      "Epoch: 0175 train_loss= 0.41959 time= 0.19275\n",
      "Epoch: 0176 train_loss= 0.41935 time= 0.19577\n",
      "Epoch: 0177 train_loss= 0.41911 time= 0.19352\n",
      "Epoch: 0178 train_loss= 0.41887 time= 0.18915\n",
      "Epoch: 0179 train_loss= 0.41863 time= 0.19457\n",
      "Epoch: 0180 train_loss= 0.41840 time= 0.19754\n",
      "Epoch: 0181 train_loss= 0.41816 time= 0.18975\n",
      "Epoch: 0182 train_loss= 0.41792 time= 0.19478\n",
      "Epoch: 0183 train_loss= 0.41769 time= 0.19119\n",
      "Epoch: 0184 train_loss= 0.41746 time= 0.20020\n",
      "Epoch: 0185 train_loss= 0.41723 time= 0.20616\n",
      "Epoch: 0186 train_loss= 0.41700 time= 0.19689\n",
      "Epoch: 0187 train_loss= 0.41678 time= 0.19424\n",
      "Epoch: 0188 train_loss= 0.41657 time= 0.19189\n",
      "Epoch: 0189 train_loss= 0.41635 time= 0.19406\n",
      "Epoch: 0190 train_loss= 0.41615 time= 0.19669\n",
      "Epoch: 0191 train_loss= 0.41595 time= 0.20586\n",
      "Epoch: 0192 train_loss= 0.41576 time= 0.19824\n",
      "Epoch: 0193 train_loss= 0.41557 time= 0.19388\n",
      "Epoch: 0194 train_loss= 0.41539 time= 0.20144\n",
      "Epoch: 0195 train_loss= 0.41522 time= 0.19947\n",
      "Epoch: 0196 train_loss= 0.41505 time= 0.19426\n",
      "Epoch: 0197 train_loss= 0.41488 time= 0.19430\n",
      "Epoch: 0198 train_loss= 0.41473 time= 0.18907\n",
      "Epoch: 0199 train_loss= 0.41457 time= 0.19715\n",
      "Epoch: 0200 train_loss= 0.41443 time= 0.20144\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79731 time= 0.25951\n",
      "Epoch: 0002 train_loss= 0.79645 time= 0.20619\n",
      "Epoch: 0003 train_loss= 0.79425 time= 0.20024\n",
      "Epoch: 0004 train_loss= 0.78987 time= 0.19521\n",
      "Epoch: 0005 train_loss= 0.78237 time= 0.19670\n",
      "Epoch: 0006 train_loss= 0.77102 time= 0.19272\n",
      "Epoch: 0007 train_loss= 0.75579 time= 0.19990\n",
      "Epoch: 0008 train_loss= 0.73837 time= 0.20195\n",
      "Epoch: 0009 train_loss= 0.72330 time= 0.20737\n",
      "Epoch: 0010 train_loss= 0.71796 time= 0.22020\n",
      "Epoch: 0011 train_loss= 0.72256 time= 0.19647\n",
      "Epoch: 0012 train_loss= 0.72252 time= 0.21520\n",
      "Epoch: 0013 train_loss= 0.71440 time= 0.20371\n",
      "Epoch: 0014 train_loss= 0.70360 time= 0.20854\n",
      "Epoch: 0015 train_loss= 0.69457 time= 0.19503\n",
      "Epoch: 0016 train_loss= 0.68853 time= 0.19181\n",
      "Epoch: 0017 train_loss= 0.68427 time= 0.19454\n",
      "Epoch: 0018 train_loss= 0.67998 time= 0.19559\n",
      "Epoch: 0019 train_loss= 0.67435 time= 0.19911\n",
      "Epoch: 0020 train_loss= 0.66690 time= 0.20599\n",
      "Epoch: 0021 train_loss= 0.65775 time= 0.20095\n",
      "Epoch: 0022 train_loss= 0.64743 time= 0.20406\n",
      "Epoch: 0023 train_loss= 0.63672 time= 0.20073\n",
      "Epoch: 0024 train_loss= 0.62636 time= 0.19946\n",
      "Epoch: 0025 train_loss= 0.61678 time= 0.20415\n",
      "Epoch: 0026 train_loss= 0.60796 time= 0.18459\n",
      "Epoch: 0027 train_loss= 0.59948 time= 0.20028\n",
      "Epoch: 0028 train_loss= 0.59093 time= 0.18538\n",
      "Epoch: 0029 train_loss= 0.58231 time= 0.19774\n",
      "Epoch: 0030 train_loss= 0.57400 time= 0.20019\n",
      "Epoch: 0031 train_loss= 0.56639 time= 0.20445\n",
      "Epoch: 0032 train_loss= 0.55957 time= 0.20154\n",
      "Epoch: 0033 train_loss= 0.55322 time= 0.19487\n",
      "Epoch: 0034 train_loss= 0.54690 time= 0.19878\n",
      "Epoch: 0035 train_loss= 0.54038 time= 0.19932\n",
      "Epoch: 0036 train_loss= 0.53378 time= 0.18703\n",
      "Epoch: 0037 train_loss= 0.52752 time= 0.20115\n",
      "Epoch: 0038 train_loss= 0.52204 time= 0.19429\n",
      "Epoch: 0039 train_loss= 0.51755 time= 0.19013\n",
      "Epoch: 0040 train_loss= 0.51396 time= 0.20394\n",
      "Epoch: 0041 train_loss= 0.51101 time= 0.19826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0042 train_loss= 0.50843 time= 0.19373\n",
      "Epoch: 0043 train_loss= 0.50602 time= 0.19463\n",
      "Epoch: 0044 train_loss= 0.50364 time= 0.19776\n",
      "Epoch: 0045 train_loss= 0.50119 time= 0.18943\n",
      "Epoch: 0046 train_loss= 0.49861 time= 0.19006\n",
      "Epoch: 0047 train_loss= 0.49589 time= 0.18635\n",
      "Epoch: 0048 train_loss= 0.49311 time= 0.18403\n",
      "Epoch: 0049 train_loss= 0.49035 time= 0.19459\n",
      "Epoch: 0050 train_loss= 0.48772 time= 0.18885\n",
      "Epoch: 0051 train_loss= 0.48526 time= 0.18560\n",
      "Epoch: 0052 train_loss= 0.48295 time= 0.19883\n",
      "Epoch: 0053 train_loss= 0.48072 time= 0.20020\n",
      "Epoch: 0054 train_loss= 0.47853 time= 0.19196\n",
      "Epoch: 0055 train_loss= 0.47635 time= 0.20053\n",
      "Epoch: 0056 train_loss= 0.47421 time= 0.18900\n",
      "Epoch: 0057 train_loss= 0.47216 time= 0.19046\n",
      "Epoch: 0058 train_loss= 0.47024 time= 0.19536\n",
      "Epoch: 0059 train_loss= 0.46848 time= 0.19847\n",
      "Epoch: 0060 train_loss= 0.46687 time= 0.19390\n",
      "Epoch: 0061 train_loss= 0.46538 time= 0.20665\n",
      "Epoch: 0062 train_loss= 0.46398 time= 0.19833\n",
      "Epoch: 0063 train_loss= 0.46266 time= 0.20617\n",
      "Epoch: 0064 train_loss= 0.46139 time= 0.19254\n",
      "Epoch: 0065 train_loss= 0.46016 time= 0.19448\n",
      "Epoch: 0066 train_loss= 0.45894 time= 0.18596\n",
      "Epoch: 0067 train_loss= 0.45771 time= 0.19619\n",
      "Epoch: 0068 train_loss= 0.45644 time= 0.20300\n",
      "Epoch: 0069 train_loss= 0.45516 time= 0.19758\n",
      "Epoch: 0070 train_loss= 0.45388 time= 0.18963\n",
      "Epoch: 0071 train_loss= 0.45265 time= 0.19152\n",
      "Epoch: 0072 train_loss= 0.45151 time= 0.19117\n",
      "Epoch: 0073 train_loss= 0.45048 time= 0.20131\n",
      "Epoch: 0074 train_loss= 0.44956 time= 0.19579\n",
      "Epoch: 0075 train_loss= 0.44873 time= 0.19694\n",
      "Epoch: 0076 train_loss= 0.44799 time= 0.18838\n",
      "Epoch: 0077 train_loss= 0.44731 time= 0.18745\n",
      "Epoch: 0078 train_loss= 0.44669 time= 0.18632\n",
      "Epoch: 0079 train_loss= 0.44611 time= 0.19227\n",
      "Epoch: 0080 train_loss= 0.44555 time= 0.19518\n",
      "Epoch: 0081 train_loss= 0.44500 time= 0.19290\n",
      "Epoch: 0082 train_loss= 0.44445 time= 0.19074\n",
      "Epoch: 0083 train_loss= 0.44389 time= 0.19293\n",
      "Epoch: 0084 train_loss= 0.44332 time= 0.19200\n",
      "Epoch: 0085 train_loss= 0.44276 time= 0.19053\n",
      "Epoch: 0086 train_loss= 0.44221 time= 0.19461\n",
      "Epoch: 0087 train_loss= 0.44168 time= 0.18676\n",
      "Epoch: 0088 train_loss= 0.44116 time= 0.18269\n",
      "Epoch: 0089 train_loss= 0.44068 time= 0.19059\n",
      "Epoch: 0090 train_loss= 0.44022 time= 0.19054\n",
      "Epoch: 0091 train_loss= 0.43979 time= 0.19576\n",
      "Epoch: 0092 train_loss= 0.43940 time= 0.18584\n",
      "Epoch: 0093 train_loss= 0.43904 time= 0.18907\n",
      "Epoch: 0094 train_loss= 0.43870 time= 0.18956\n",
      "Epoch: 0095 train_loss= 0.43839 time= 0.18933\n",
      "Epoch: 0096 train_loss= 0.43810 time= 0.19697\n",
      "Epoch: 0097 train_loss= 0.43781 time= 0.20082\n",
      "Epoch: 0098 train_loss= 0.43753 time= 0.19905\n",
      "Epoch: 0099 train_loss= 0.43726 time= 0.21564\n",
      "Epoch: 0100 train_loss= 0.43699 time= 0.19613\n",
      "Epoch: 0101 train_loss= 0.43670 time= 0.20066\n",
      "Epoch: 0102 train_loss= 0.43642 time= 0.19560\n",
      "Epoch: 0103 train_loss= 0.43612 time= 0.19908\n",
      "Epoch: 0104 train_loss= 0.43581 time= 0.19155\n",
      "Epoch: 0105 train_loss= 0.43550 time= 0.19266\n",
      "Epoch: 0106 train_loss= 0.43519 time= 0.18830\n",
      "Epoch: 0107 train_loss= 0.43487 time= 0.19369\n",
      "Epoch: 0108 train_loss= 0.43454 time= 0.18554\n",
      "Epoch: 0109 train_loss= 0.43421 time= 0.18807\n",
      "Epoch: 0110 train_loss= 0.43388 time= 0.19649\n",
      "Epoch: 0111 train_loss= 0.43354 time= 0.19090\n",
      "Epoch: 0112 train_loss= 0.43320 time= 0.19543\n",
      "Epoch: 0113 train_loss= 0.43285 time= 0.18690\n",
      "Epoch: 0114 train_loss= 0.43249 time= 0.19576\n",
      "Epoch: 0115 train_loss= 0.43213 time= 0.19632\n",
      "Epoch: 0116 train_loss= 0.43177 time= 0.19440\n",
      "Epoch: 0117 train_loss= 0.43140 time= 0.21703\n",
      "Epoch: 0118 train_loss= 0.43103 time= 0.19130\n",
      "Epoch: 0119 train_loss= 0.43067 time= 0.19731\n",
      "Epoch: 0120 train_loss= 0.43030 time= 0.19139\n",
      "Epoch: 0121 train_loss= 0.42994 time= 0.19306\n",
      "Epoch: 0122 train_loss= 0.42958 time= 0.19868\n",
      "Epoch: 0123 train_loss= 0.42923 time= 0.19876\n",
      "Epoch: 0124 train_loss= 0.42889 time= 0.19377\n",
      "Epoch: 0125 train_loss= 0.42856 time= 0.20098\n",
      "Epoch: 0126 train_loss= 0.42823 time= 0.19658\n",
      "Epoch: 0127 train_loss= 0.42791 time= 0.19555\n",
      "Epoch: 0128 train_loss= 0.42759 time= 0.20892\n",
      "Epoch: 0129 train_loss= 0.42727 time= 0.19206\n",
      "Epoch: 0130 train_loss= 0.42696 time= 0.20047\n",
      "Epoch: 0131 train_loss= 0.42663 time= 0.19896\n",
      "Epoch: 0132 train_loss= 0.42631 time= 0.20636\n",
      "Epoch: 0133 train_loss= 0.42599 time= 0.20357\n",
      "Epoch: 0134 train_loss= 0.42566 time= 0.20258\n",
      "Epoch: 0135 train_loss= 0.42534 time= 0.19532\n",
      "Epoch: 0136 train_loss= 0.42502 time= 0.19404\n",
      "Epoch: 0137 train_loss= 0.42471 time= 0.20576\n",
      "Epoch: 0138 train_loss= 0.42440 time= 0.19843\n",
      "Epoch: 0139 train_loss= 0.42410 time= 0.19648\n",
      "Epoch: 0140 train_loss= 0.42381 time= 0.19372\n",
      "Epoch: 0141 train_loss= 0.42352 time= 0.19576\n",
      "Epoch: 0142 train_loss= 0.42325 time= 0.19552\n",
      "Epoch: 0143 train_loss= 0.42298 time= 0.19989\n",
      "Epoch: 0144 train_loss= 0.42271 time= 0.19703\n",
      "Epoch: 0145 train_loss= 0.42245 time= 0.19486\n",
      "Epoch: 0146 train_loss= 0.42219 time= 0.18954\n",
      "Epoch: 0147 train_loss= 0.42193 time= 0.20183\n",
      "Epoch: 0148 train_loss= 0.42168 time= 0.19234\n",
      "Epoch: 0149 train_loss= 0.42143 time= 0.19028\n",
      "Epoch: 0150 train_loss= 0.42119 time= 0.19844\n",
      "Epoch: 0151 train_loss= 0.42095 time= 0.19993\n",
      "Epoch: 0152 train_loss= 0.42072 time= 0.19178\n",
      "Epoch: 0153 train_loss= 0.42050 time= 0.20044\n",
      "Epoch: 0154 train_loss= 0.42028 time= 0.20042\n",
      "Epoch: 0155 train_loss= 0.42007 time= 0.19691\n",
      "Epoch: 0156 train_loss= 0.41986 time= 0.19317\n",
      "Epoch: 0157 train_loss= 0.41965 time= 0.19374\n",
      "Epoch: 0158 train_loss= 0.41944 time= 0.20150\n",
      "Epoch: 0159 train_loss= 0.41923 time= 0.19719\n",
      "Epoch: 0160 train_loss= 0.41901 time= 0.19647\n",
      "Epoch: 0161 train_loss= 0.41880 time= 0.19352\n",
      "Epoch: 0162 train_loss= 0.41858 time= 0.18533\n",
      "Epoch: 0163 train_loss= 0.41836 time= 0.18550\n",
      "Epoch: 0164 train_loss= 0.41814 time= 0.20346\n",
      "Epoch: 0165 train_loss= 0.41792 time= 0.19747\n",
      "Epoch: 0166 train_loss= 0.41770 time= 0.19240\n",
      "Epoch: 0167 train_loss= 0.41748 time= 0.18872\n",
      "Epoch: 0168 train_loss= 0.41726 time= 0.18662\n",
      "Epoch: 0169 train_loss= 0.41704 time= 0.19208\n",
      "Epoch: 0170 train_loss= 0.41683 time= 0.18995\n",
      "Epoch: 0171 train_loss= 0.41663 time= 0.19717\n",
      "Epoch: 0172 train_loss= 0.41643 time= 0.19007\n",
      "Epoch: 0173 train_loss= 0.41623 time= 0.19327\n",
      "Epoch: 0174 train_loss= 0.41605 time= 0.20031\n",
      "Epoch: 0175 train_loss= 0.41587 time= 0.18001\n",
      "Epoch: 0176 train_loss= 0.41570 time= 0.19163\n",
      "Epoch: 0177 train_loss= 0.41553 time= 0.19038\n",
      "Epoch: 0178 train_loss= 0.41537 time= 0.18892\n",
      "Epoch: 0179 train_loss= 0.41521 time= 0.19514\n",
      "Epoch: 0180 train_loss= 0.41506 time= 0.18518\n",
      "Epoch: 0181 train_loss= 0.41491 time= 0.20246\n",
      "Epoch: 0182 train_loss= 0.41477 time= 0.19560\n",
      "Epoch: 0183 train_loss= 0.41463 time= 0.19767\n",
      "Epoch: 0184 train_loss= 0.41449 time= 0.20102\n",
      "Epoch: 0185 train_loss= 0.41435 time= 0.20233\n",
      "Epoch: 0186 train_loss= 0.41422 time= 0.20588\n",
      "Epoch: 0187 train_loss= 0.41408 time= 0.20472\n",
      "Epoch: 0188 train_loss= 0.41395 time= 0.20506\n",
      "Epoch: 0189 train_loss= 0.41381 time= 0.19903\n",
      "Epoch: 0190 train_loss= 0.41368 time= 0.19959\n",
      "Epoch: 0191 train_loss= 0.41355 time= 0.19948\n",
      "Epoch: 0192 train_loss= 0.41341 time= 0.19732\n",
      "Epoch: 0193 train_loss= 0.41328 time= 0.19324\n",
      "Epoch: 0194 train_loss= 0.41314 time= 0.20326\n",
      "Epoch: 0195 train_loss= 0.41300 time= 0.19030\n",
      "Epoch: 0196 train_loss= 0.41287 time= 0.19449\n",
      "Epoch: 0197 train_loss= 0.41273 time= 0.19097\n",
      "Epoch: 0198 train_loss= 0.41259 time= 0.19700\n",
      "Epoch: 0199 train_loss= 0.41244 time= 0.18614\n",
      "Epoch: 0200 train_loss= 0.41230 time= 0.19357\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79737 time= 0.25722\n",
      "Epoch: 0002 train_loss= 0.79682 time= 0.18567\n",
      "Epoch: 0003 train_loss= 0.79546 time= 0.20916\n",
      "Epoch: 0004 train_loss= 0.79279 time= 0.20020\n",
      "Epoch: 0005 train_loss= 0.78821 time= 0.20276\n",
      "Epoch: 0006 train_loss= 0.78102 time= 0.19852\n",
      "Epoch: 0007 train_loss= 0.77063 time= 0.20313\n",
      "Epoch: 0008 train_loss= 0.75696 time= 0.19597\n",
      "Epoch: 0009 train_loss= 0.74095 time= 0.20426\n",
      "Epoch: 0010 train_loss= 0.72544 time= 0.21433\n",
      "Epoch: 0011 train_loss= 0.71551 time= 0.20701\n",
      "Epoch: 0012 train_loss= 0.71453 time= 0.19530\n",
      "Epoch: 0013 train_loss= 0.71497 time= 0.19755\n",
      "Epoch: 0014 train_loss= 0.70878 time= 0.20030\n",
      "Epoch: 0015 train_loss= 0.69723 time= 0.19985\n",
      "Epoch: 0016 train_loss= 0.68436 time= 0.18940\n",
      "Epoch: 0017 train_loss= 0.67275 time= 0.19168\n",
      "Epoch: 0018 train_loss= 0.66280 time= 0.18080\n",
      "Epoch: 0019 train_loss= 0.65347 time= 0.18723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0020 train_loss= 0.64347 time= 0.20136\n",
      "Epoch: 0021 train_loss= 0.63205 time= 0.19752\n",
      "Epoch: 0022 train_loss= 0.61909 time= 0.20062\n",
      "Epoch: 0023 train_loss= 0.60496 time= 0.19454\n",
      "Epoch: 0024 train_loss= 0.59040 time= 0.19187\n",
      "Epoch: 0025 train_loss= 0.57631 time= 0.19692\n",
      "Epoch: 0026 train_loss= 0.56366 time= 0.19306\n",
      "Epoch: 0027 train_loss= 0.55314 time= 0.18750\n",
      "Epoch: 0028 train_loss= 0.54503 time= 0.19400\n",
      "Epoch: 0029 train_loss= 0.53906 time= 0.18895\n",
      "Epoch: 0030 train_loss= 0.53450 time= 0.19446\n",
      "Epoch: 0031 train_loss= 0.53058 time= 0.19153\n",
      "Epoch: 0032 train_loss= 0.52674 time= 0.19481\n",
      "Epoch: 0033 train_loss= 0.52284 time= 0.19160\n",
      "Epoch: 0034 train_loss= 0.51895 time= 0.19515\n",
      "Epoch: 0035 train_loss= 0.51512 time= 0.19866\n",
      "Epoch: 0036 train_loss= 0.51124 time= 0.19724\n",
      "Epoch: 0037 train_loss= 0.50718 time= 0.19998\n",
      "Epoch: 0038 train_loss= 0.50287 time= 0.18894\n",
      "Epoch: 0039 train_loss= 0.49840 time= 0.19471\n",
      "Epoch: 0040 train_loss= 0.49401 time= 0.19150\n",
      "Epoch: 0041 train_loss= 0.48999 time= 0.19146\n",
      "Epoch: 0042 train_loss= 0.48653 time= 0.19107\n",
      "Epoch: 0043 train_loss= 0.48370 time= 0.18721\n",
      "Epoch: 0044 train_loss= 0.48143 time= 0.20839\n",
      "Epoch: 0045 train_loss= 0.47958 time= 0.20701\n",
      "Epoch: 0046 train_loss= 0.47795 time= 0.20232\n",
      "Epoch: 0047 train_loss= 0.47643 time= 0.19398\n",
      "Epoch: 0048 train_loss= 0.47493 time= 0.19316\n",
      "Epoch: 0049 train_loss= 0.47340 time= 0.20410\n",
      "Epoch: 0050 train_loss= 0.47184 time= 0.19883\n",
      "Epoch: 0051 train_loss= 0.47022 time= 0.19468\n",
      "Epoch: 0052 train_loss= 0.46855 time= 0.19628\n",
      "Epoch: 0053 train_loss= 0.46683 time= 0.19383\n",
      "Epoch: 0054 train_loss= 0.46509 time= 0.18804\n",
      "Epoch: 0055 train_loss= 0.46342 time= 0.19569\n",
      "Epoch: 0056 train_loss= 0.46188 time= 0.18729\n",
      "Epoch: 0057 train_loss= 0.46052 time= 0.18850\n",
      "Epoch: 0058 train_loss= 0.45935 time= 0.19265\n",
      "Epoch: 0059 train_loss= 0.45836 time= 0.18969\n",
      "Epoch: 0060 train_loss= 0.45749 time= 0.18926\n",
      "Epoch: 0061 train_loss= 0.45669 time= 0.18773\n",
      "Epoch: 0062 train_loss= 0.45592 time= 0.19281\n",
      "Epoch: 0063 train_loss= 0.45515 time= 0.19292\n",
      "Epoch: 0064 train_loss= 0.45436 time= 0.19534\n",
      "Epoch: 0065 train_loss= 0.45356 time= 0.19149\n",
      "Epoch: 0066 train_loss= 0.45275 time= 0.20068\n",
      "Epoch: 0067 train_loss= 0.45193 time= 0.18996\n",
      "Epoch: 0068 train_loss= 0.45111 time= 0.18891\n",
      "Epoch: 0069 train_loss= 0.45030 time= 0.19387\n",
      "Epoch: 0070 train_loss= 0.44951 time= 0.19510\n",
      "Epoch: 0071 train_loss= 0.44876 time= 0.18589\n",
      "Epoch: 0072 train_loss= 0.44802 time= 0.18094\n",
      "Epoch: 0073 train_loss= 0.44729 time= 0.18233\n",
      "Epoch: 0074 train_loss= 0.44655 time= 0.18498\n",
      "Epoch: 0075 train_loss= 0.44581 time= 0.18267\n",
      "Epoch: 0076 train_loss= 0.44507 time= 0.18650\n",
      "Epoch: 0077 train_loss= 0.44433 time= 0.18768\n",
      "Epoch: 0078 train_loss= 0.44361 time= 0.18233\n",
      "Epoch: 0079 train_loss= 0.44290 time= 0.18664\n",
      "Epoch: 0080 train_loss= 0.44220 time= 0.18523\n",
      "Epoch: 0081 train_loss= 0.44151 time= 0.18586\n",
      "Epoch: 0082 train_loss= 0.44082 time= 0.19006\n",
      "Epoch: 0083 train_loss= 0.44014 time= 0.18838\n",
      "Epoch: 0084 train_loss= 0.43948 time= 0.19155\n",
      "Epoch: 0085 train_loss= 0.43883 time= 0.18787\n",
      "Epoch: 0086 train_loss= 0.43822 time= 0.18442\n",
      "Epoch: 0087 train_loss= 0.43763 time= 0.19149\n",
      "Epoch: 0088 train_loss= 0.43707 time= 0.19847\n",
      "Epoch: 0089 train_loss= 0.43654 time= 0.18376\n",
      "Epoch: 0090 train_loss= 0.43603 time= 0.19452\n",
      "Epoch: 0091 train_loss= 0.43555 time= 0.19304\n",
      "Epoch: 0092 train_loss= 0.43509 time= 0.19208\n",
      "Epoch: 0093 train_loss= 0.43464 time= 0.18411\n",
      "Epoch: 0094 train_loss= 0.43420 time= 0.19771\n",
      "Epoch: 0095 train_loss= 0.43376 time= 0.19304\n",
      "Epoch: 0096 train_loss= 0.43331 time= 0.18623\n",
      "Epoch: 0097 train_loss= 0.43287 time= 0.18331\n",
      "Epoch: 0098 train_loss= 0.43241 time= 0.18807\n",
      "Epoch: 0099 train_loss= 0.43196 time= 0.18443\n",
      "Epoch: 0100 train_loss= 0.43150 time= 0.18764\n",
      "Epoch: 0101 train_loss= 0.43104 time= 0.19074\n",
      "Epoch: 0102 train_loss= 0.43058 time= 0.19048\n",
      "Epoch: 0103 train_loss= 0.43013 time= 0.19173\n",
      "Epoch: 0104 train_loss= 0.42968 time= 0.19415\n",
      "Epoch: 0105 train_loss= 0.42924 time= 0.18796\n",
      "Epoch: 0106 train_loss= 0.42881 time= 0.19190\n",
      "Epoch: 0107 train_loss= 0.42839 time= 0.18600\n",
      "Epoch: 0108 train_loss= 0.42798 time= 0.19254\n",
      "Epoch: 0109 train_loss= 0.42759 time= 0.18794\n",
      "Epoch: 0110 train_loss= 0.42720 time= 0.18345\n",
      "Epoch: 0111 train_loss= 0.42683 time= 0.18300\n",
      "Epoch: 0112 train_loss= 0.42647 time= 0.18302\n",
      "Epoch: 0113 train_loss= 0.42612 time= 0.19135\n",
      "Epoch: 0114 train_loss= 0.42578 time= 0.18331\n",
      "Epoch: 0115 train_loss= 0.42546 time= 0.18418\n",
      "Epoch: 0116 train_loss= 0.42515 time= 0.18552\n",
      "Epoch: 0117 train_loss= 0.42485 time= 0.18919\n",
      "Epoch: 0118 train_loss= 0.42456 time= 0.18749\n",
      "Epoch: 0119 train_loss= 0.42428 time= 0.19301\n",
      "Epoch: 0120 train_loss= 0.42401 time= 0.19800\n",
      "Epoch: 0121 train_loss= 0.42374 time= 0.19285\n",
      "Epoch: 0122 train_loss= 0.42349 time= 0.19256\n",
      "Epoch: 0123 train_loss= 0.42324 time= 0.18663\n",
      "Epoch: 0124 train_loss= 0.42301 time= 0.18938\n",
      "Epoch: 0125 train_loss= 0.42278 time= 0.19577\n",
      "Epoch: 0126 train_loss= 0.42255 time= 0.19975\n",
      "Epoch: 0127 train_loss= 0.42233 time= 0.20568\n",
      "Epoch: 0128 train_loss= 0.42212 time= 0.19157\n",
      "Epoch: 0129 train_loss= 0.42191 time= 0.18704\n",
      "Epoch: 0130 train_loss= 0.42171 time= 0.18869\n",
      "Epoch: 0131 train_loss= 0.42150 time= 0.19617\n",
      "Epoch: 0132 train_loss= 0.42130 time= 0.18843\n",
      "Epoch: 0133 train_loss= 0.42111 time= 0.19302\n",
      "Epoch: 0134 train_loss= 0.42091 time= 0.18886\n",
      "Epoch: 0135 train_loss= 0.42072 time= 0.19038\n",
      "Epoch: 0136 train_loss= 0.42053 time= 0.19650\n",
      "Epoch: 0137 train_loss= 0.42035 time= 0.18889\n",
      "Epoch: 0138 train_loss= 0.42016 time= 0.19651\n",
      "Epoch: 0139 train_loss= 0.41999 time= 0.19994\n",
      "Epoch: 0140 train_loss= 0.41981 time= 0.19449\n",
      "Epoch: 0141 train_loss= 0.41963 time= 0.21635\n",
      "Epoch: 0142 train_loss= 0.41946 time= 0.20356\n",
      "Epoch: 0143 train_loss= 0.41928 time= 0.20342\n",
      "Epoch: 0144 train_loss= 0.41911 time= 0.20020\n",
      "Epoch: 0145 train_loss= 0.41893 time= 0.20923\n",
      "Epoch: 0146 train_loss= 0.41875 time= 0.20172\n",
      "Epoch: 0147 train_loss= 0.41857 time= 0.20344\n",
      "Epoch: 0148 train_loss= 0.41839 time= 0.20825\n",
      "Epoch: 0149 train_loss= 0.41821 time= 0.21386\n",
      "Epoch: 0150 train_loss= 0.41803 time= 0.22239\n",
      "Epoch: 0151 train_loss= 0.41784 time= 0.20604\n",
      "Epoch: 0152 train_loss= 0.41766 time= 0.20132\n",
      "Epoch: 0153 train_loss= 0.41747 time= 0.19464\n",
      "Epoch: 0154 train_loss= 0.41729 time= 0.20301\n",
      "Epoch: 0155 train_loss= 0.41711 time= 0.18720\n",
      "Epoch: 0156 train_loss= 0.41693 time= 0.18411\n",
      "Epoch: 0157 train_loss= 0.41675 time= 0.19151\n",
      "Epoch: 0158 train_loss= 0.41657 time= 0.19645\n",
      "Epoch: 0159 train_loss= 0.41639 time= 0.19301\n",
      "Epoch: 0160 train_loss= 0.41621 time= 0.18639\n",
      "Epoch: 0161 train_loss= 0.41603 time= 0.18486\n",
      "Epoch: 0162 train_loss= 0.41586 time= 0.19041\n",
      "Epoch: 0163 train_loss= 0.41568 time= 0.19562\n",
      "Epoch: 0164 train_loss= 0.41551 time= 0.18452\n",
      "Epoch: 0165 train_loss= 0.41534 time= 0.18344\n",
      "Epoch: 0166 train_loss= 0.41517 time= 0.18204\n",
      "Epoch: 0167 train_loss= 0.41500 time= 0.18396\n",
      "Epoch: 0168 train_loss= 0.41483 time= 0.18895\n",
      "Epoch: 0169 train_loss= 0.41467 time= 0.18703\n",
      "Epoch: 0170 train_loss= 0.41450 time= 0.19132\n",
      "Epoch: 0171 train_loss= 0.41434 time= 0.19037\n",
      "Epoch: 0172 train_loss= 0.41418 time= 0.18915\n",
      "Epoch: 0173 train_loss= 0.41402 time= 0.18399\n",
      "Epoch: 0174 train_loss= 0.41387 time= 0.18694\n",
      "Epoch: 0175 train_loss= 0.41372 time= 0.18508\n",
      "Epoch: 0176 train_loss= 0.41357 time= 0.18314\n",
      "Epoch: 0177 train_loss= 0.41342 time= 0.18690\n",
      "Epoch: 0178 train_loss= 0.41328 time= 0.18330\n",
      "Epoch: 0179 train_loss= 0.41313 time= 0.20231\n",
      "Epoch: 0180 train_loss= 0.41299 time= 0.18682\n",
      "Epoch: 0181 train_loss= 0.41285 time= 0.18601\n",
      "Epoch: 0182 train_loss= 0.41272 time= 0.18994\n",
      "Epoch: 0183 train_loss= 0.41258 time= 0.18164\n",
      "Epoch: 0184 train_loss= 0.41244 time= 0.17949\n",
      "Epoch: 0185 train_loss= 0.41230 time= 0.18750\n",
      "Epoch: 0186 train_loss= 0.41217 time= 0.18866\n",
      "Epoch: 0187 train_loss= 0.41203 time= 0.19455\n",
      "Epoch: 0188 train_loss= 0.41189 time= 0.18949\n",
      "Epoch: 0189 train_loss= 0.41175 time= 0.19149\n",
      "Epoch: 0190 train_loss= 0.41161 time= 0.18168\n",
      "Epoch: 0191 train_loss= 0.41147 time= 0.19301\n",
      "Epoch: 0192 train_loss= 0.41134 time= 0.18594\n",
      "Epoch: 0193 train_loss= 0.41120 time= 0.18729\n",
      "Epoch: 0194 train_loss= 0.41106 time= 0.18498\n",
      "Epoch: 0195 train_loss= 0.41092 time= 0.18191\n",
      "Epoch: 0196 train_loss= 0.41078 time= 0.19308\n",
      "Epoch: 0197 train_loss= 0.41065 time= 0.17707\n",
      "Epoch: 0198 train_loss= 0.41051 time= 0.19988\n",
      "Epoch: 0199 train_loss= 0.41038 time= 0.19106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0200 train_loss= 0.41025 time= 0.19396\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79733 time= 0.25130\n",
      "Epoch: 0002 train_loss= 0.79660 time= 0.19578\n",
      "Epoch: 0003 train_loss= 0.79475 time= 0.18655\n",
      "Epoch: 0004 train_loss= 0.79107 time= 0.18053\n",
      "Epoch: 0005 train_loss= 0.78474 time= 0.18522\n",
      "Epoch: 0006 train_loss= 0.77497 time= 0.18057\n",
      "Epoch: 0007 train_loss= 0.76146 time= 0.18436\n",
      "Epoch: 0008 train_loss= 0.74504 time= 0.18800\n",
      "Epoch: 0009 train_loss= 0.72870 time= 0.18425\n",
      "Epoch: 0010 train_loss= 0.71836 time= 0.18606\n",
      "Epoch: 0011 train_loss= 0.71938 time= 0.18543\n",
      "Epoch: 0012 train_loss= 0.72280 time= 0.18874\n",
      "Epoch: 0013 train_loss= 0.71824 time= 0.18357\n",
      "Epoch: 0014 train_loss= 0.70808 time= 0.18473\n",
      "Epoch: 0015 train_loss= 0.69751 time= 0.18705\n",
      "Epoch: 0016 train_loss= 0.68934 time= 0.19489\n",
      "Epoch: 0017 train_loss= 0.68358 time= 0.19116\n",
      "Epoch: 0018 train_loss= 0.67868 time= 0.18078\n",
      "Epoch: 0019 train_loss= 0.67302 time= 0.18844\n",
      "Epoch: 0020 train_loss= 0.66568 time= 0.19099\n",
      "Epoch: 0021 train_loss= 0.65643 time= 0.18999\n",
      "Epoch: 0022 train_loss= 0.64552 time= 0.18862\n",
      "Epoch: 0023 train_loss= 0.63350 time= 0.18230\n",
      "Epoch: 0024 train_loss= 0.62106 time= 0.19473\n",
      "Epoch: 0025 train_loss= 0.60883 time= 0.18834\n",
      "Epoch: 0026 train_loss= 0.59717 time= 0.18856\n",
      "Epoch: 0027 train_loss= 0.58611 time= 0.18167\n",
      "Epoch: 0028 train_loss= 0.57552 time= 0.18616\n",
      "Epoch: 0029 train_loss= 0.56540 time= 0.19302\n",
      "Epoch: 0030 train_loss= 0.55605 time= 0.19441\n",
      "Epoch: 0031 train_loss= 0.54804 time= 0.18896\n",
      "Epoch: 0032 train_loss= 0.54186 time= 0.18759\n",
      "Epoch: 0033 train_loss= 0.53760 time= 0.19955\n",
      "Epoch: 0034 train_loss= 0.53479 time= 0.19606\n",
      "Epoch: 0035 train_loss= 0.53258 time= 0.18581\n",
      "Epoch: 0036 train_loss= 0.53013 time= 0.18435\n",
      "Epoch: 0037 train_loss= 0.52698 time= 0.18553\n",
      "Epoch: 0038 train_loss= 0.52305 time= 0.18158\n",
      "Epoch: 0039 train_loss= 0.51854 time= 0.19789\n",
      "Epoch: 0040 train_loss= 0.51365 time= 0.18785\n",
      "Epoch: 0041 train_loss= 0.50861 time= 0.18830\n",
      "Epoch: 0042 train_loss= 0.50359 time= 0.19786\n",
      "Epoch: 0043 train_loss= 0.49878 time= 0.19062\n",
      "Epoch: 0044 train_loss= 0.49441 time= 0.19016\n",
      "Epoch: 0045 train_loss= 0.49063 time= 0.18993\n",
      "Epoch: 0046 train_loss= 0.48754 time= 0.18623\n",
      "Epoch: 0047 train_loss= 0.48508 time= 0.18115\n",
      "Epoch: 0048 train_loss= 0.48315 time= 0.19063\n",
      "Epoch: 0049 train_loss= 0.48158 time= 0.18959\n",
      "Epoch: 0050 train_loss= 0.48025 time= 0.18699\n",
      "Epoch: 0051 train_loss= 0.47905 time= 0.19551\n",
      "Epoch: 0052 train_loss= 0.47791 time= 0.19137\n",
      "Epoch: 0053 train_loss= 0.47680 time= 0.19112\n",
      "Epoch: 0054 train_loss= 0.47567 time= 0.19162\n",
      "Epoch: 0055 train_loss= 0.47447 time= 0.19436\n",
      "Epoch: 0056 train_loss= 0.47320 time= 0.18831\n",
      "Epoch: 0057 train_loss= 0.47182 time= 0.18790\n",
      "Epoch: 0058 train_loss= 0.47035 time= 0.19092\n",
      "Epoch: 0059 train_loss= 0.46883 time= 0.18375\n",
      "Epoch: 0060 train_loss= 0.46731 time= 0.18336\n",
      "Epoch: 0061 train_loss= 0.46587 time= 0.19923\n",
      "Epoch: 0062 train_loss= 0.46452 time= 0.19455\n",
      "Epoch: 0063 train_loss= 0.46329 time= 0.19319\n",
      "Epoch: 0064 train_loss= 0.46216 time= 0.18764\n",
      "Epoch: 0065 train_loss= 0.46114 time= 0.19018\n",
      "Epoch: 0066 train_loss= 0.46021 time= 0.18772\n",
      "Epoch: 0067 train_loss= 0.45937 time= 0.18602\n",
      "Epoch: 0068 train_loss= 0.45860 time= 0.18386\n",
      "Epoch: 0069 train_loss= 0.45789 time= 0.18038\n",
      "Epoch: 0070 train_loss= 0.45719 time= 0.18249\n",
      "Epoch: 0071 train_loss= 0.45649 time= 0.17975\n",
      "Epoch: 0072 train_loss= 0.45578 time= 0.18844\n",
      "Epoch: 0073 train_loss= 0.45508 time= 0.18634\n",
      "Epoch: 0074 train_loss= 0.45439 time= 0.18534\n",
      "Epoch: 0075 train_loss= 0.45373 time= 0.18833\n",
      "Epoch: 0076 train_loss= 0.45309 time= 0.18934\n",
      "Epoch: 0077 train_loss= 0.45246 time= 0.18657\n",
      "Epoch: 0078 train_loss= 0.45184 time= 0.18364\n",
      "Epoch: 0079 train_loss= 0.45121 time= 0.18540\n",
      "Epoch: 0080 train_loss= 0.45057 time= 0.18872\n",
      "Epoch: 0081 train_loss= 0.44992 time= 0.18611\n",
      "Epoch: 0082 train_loss= 0.44926 time= 0.20087\n",
      "Epoch: 0083 train_loss= 0.44858 time= 0.18650\n",
      "Epoch: 0084 train_loss= 0.44791 time= 0.19810\n",
      "Epoch: 0085 train_loss= 0.44725 time= 0.19226\n",
      "Epoch: 0086 train_loss= 0.44659 time= 0.19172\n",
      "Epoch: 0087 train_loss= 0.44595 time= 0.19207\n",
      "Epoch: 0088 train_loss= 0.44532 time= 0.19440\n",
      "Epoch: 0089 train_loss= 0.44470 time= 0.20399\n",
      "Epoch: 0090 train_loss= 0.44410 time= 0.19743\n",
      "Epoch: 0091 train_loss= 0.44350 time= 0.18615\n",
      "Epoch: 0092 train_loss= 0.44292 time= 0.18102\n",
      "Epoch: 0093 train_loss= 0.44235 time= 0.18218\n",
      "Epoch: 0094 train_loss= 0.44181 time= 0.18180\n",
      "Epoch: 0095 train_loss= 0.44127 time= 0.18027\n",
      "Epoch: 0096 train_loss= 0.44074 time= 0.19012\n",
      "Epoch: 0097 train_loss= 0.44023 time= 0.18700\n",
      "Epoch: 0098 train_loss= 0.43971 time= 0.18647\n",
      "Epoch: 0099 train_loss= 0.43921 time= 0.18499\n",
      "Epoch: 0100 train_loss= 0.43870 time= 0.19107\n",
      "Epoch: 0101 train_loss= 0.43820 time= 0.18058\n",
      "Epoch: 0102 train_loss= 0.43769 time= 0.18194\n",
      "Epoch: 0103 train_loss= 0.43720 time= 0.18741\n",
      "Epoch: 0104 train_loss= 0.43670 time= 0.18684\n",
      "Epoch: 0105 train_loss= 0.43622 time= 0.18820\n",
      "Epoch: 0106 train_loss= 0.43575 time= 0.19132\n",
      "Epoch: 0107 train_loss= 0.43530 time= 0.18085\n",
      "Epoch: 0108 train_loss= 0.43487 time= 0.17796\n",
      "Epoch: 0109 train_loss= 0.43445 time= 0.18232\n",
      "Epoch: 0110 train_loss= 0.43405 time= 0.18015\n",
      "Epoch: 0111 train_loss= 0.43366 time= 0.18428\n",
      "Epoch: 0112 train_loss= 0.43330 time= 0.18892\n",
      "Epoch: 0113 train_loss= 0.43294 time= 0.18182\n",
      "Epoch: 0114 train_loss= 0.43259 time= 0.18783\n",
      "Epoch: 0115 train_loss= 0.43226 time= 0.17988\n",
      "Epoch: 0116 train_loss= 0.43194 time= 0.18254\n",
      "Epoch: 0117 train_loss= 0.43163 time= 0.18469\n",
      "Epoch: 0118 train_loss= 0.43132 time= 0.18330\n",
      "Epoch: 0119 train_loss= 0.43103 time= 0.18380\n",
      "Epoch: 0120 train_loss= 0.43074 time= 0.19760\n",
      "Epoch: 0121 train_loss= 0.43045 time= 0.18512\n",
      "Epoch: 0122 train_loss= 0.43016 time= 0.18477\n",
      "Epoch: 0123 train_loss= 0.42987 time= 0.18655\n",
      "Epoch: 0124 train_loss= 0.42958 time= 0.18168\n",
      "Epoch: 0125 train_loss= 0.42928 time= 0.19308\n",
      "Epoch: 0126 train_loss= 0.42898 time= 0.18378\n",
      "Epoch: 0127 train_loss= 0.42867 time= 0.19995\n",
      "Epoch: 0128 train_loss= 0.42835 time= 0.18781\n",
      "Epoch: 0129 train_loss= 0.42803 time= 0.18234\n",
      "Epoch: 0130 train_loss= 0.42771 time= 0.18612\n",
      "Epoch: 0131 train_loss= 0.42738 time= 0.18173\n",
      "Epoch: 0132 train_loss= 0.42705 time= 0.18426\n",
      "Epoch: 0133 train_loss= 0.42672 time= 0.18804\n",
      "Epoch: 0134 train_loss= 0.42639 time= 0.18745\n",
      "Epoch: 0135 train_loss= 0.42606 time= 0.19219\n",
      "Epoch: 0136 train_loss= 0.42572 time= 0.18904\n",
      "Epoch: 0137 train_loss= 0.42539 time= 0.18815\n",
      "Epoch: 0138 train_loss= 0.42506 time= 0.19199\n",
      "Epoch: 0139 train_loss= 0.42474 time= 0.19386\n",
      "Epoch: 0140 train_loss= 0.42442 time= 0.18889\n",
      "Epoch: 0141 train_loss= 0.42410 time= 0.19002\n",
      "Epoch: 0142 train_loss= 0.42378 time= 0.19608\n",
      "Epoch: 0143 train_loss= 0.42347 time= 0.19112\n",
      "Epoch: 0144 train_loss= 0.42317 time= 0.18401\n",
      "Epoch: 0145 train_loss= 0.42286 time= 0.18350\n",
      "Epoch: 0146 train_loss= 0.42257 time= 0.18716\n",
      "Epoch: 0147 train_loss= 0.42227 time= 0.18400\n",
      "Epoch: 0148 train_loss= 0.42198 time= 0.18191\n",
      "Epoch: 0149 train_loss= 0.42170 time= 0.18342\n",
      "Epoch: 0150 train_loss= 0.42142 time= 0.18691\n",
      "Epoch: 0151 train_loss= 0.42114 time= 0.18704\n",
      "Epoch: 0152 train_loss= 0.42087 time= 0.18599\n",
      "Epoch: 0153 train_loss= 0.42060 time= 0.18520\n",
      "Epoch: 0154 train_loss= 0.42034 time= 0.18574\n",
      "Epoch: 0155 train_loss= 0.42008 time= 0.18715\n",
      "Epoch: 0156 train_loss= 0.41983 time= 0.18200\n",
      "Epoch: 0157 train_loss= 0.41958 time= 0.18505\n",
      "Epoch: 0158 train_loss= 0.41934 time= 0.18397\n",
      "Epoch: 0159 train_loss= 0.41909 time= 0.18771\n",
      "Epoch: 0160 train_loss= 0.41885 time= 0.18744\n",
      "Epoch: 0161 train_loss= 0.41862 time= 0.18616\n",
      "Epoch: 0162 train_loss= 0.41838 time= 0.18610\n",
      "Epoch: 0163 train_loss= 0.41815 time= 0.17872\n",
      "Epoch: 0164 train_loss= 0.41792 time= 0.17963\n",
      "Epoch: 0165 train_loss= 0.41770 time= 0.19109\n",
      "Epoch: 0166 train_loss= 0.41748 time= 0.18955\n",
      "Epoch: 0167 train_loss= 0.41727 time= 0.18473\n",
      "Epoch: 0168 train_loss= 0.41706 time= 0.19683\n",
      "Epoch: 0169 train_loss= 0.41686 time= 0.18616\n",
      "Epoch: 0170 train_loss= 0.41667 time= 0.20063\n",
      "Epoch: 0171 train_loss= 0.41648 time= 0.18278\n",
      "Epoch: 0172 train_loss= 0.41631 time= 0.18133\n",
      "Epoch: 0173 train_loss= 0.41614 time= 0.18535\n",
      "Epoch: 0174 train_loss= 0.41598 time= 0.18448\n",
      "Epoch: 0175 train_loss= 0.41584 time= 0.18026\n",
      "Epoch: 0176 train_loss= 0.41570 time= 0.18062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0177 train_loss= 0.41556 time= 0.18150\n",
      "Epoch: 0178 train_loss= 0.41544 time= 0.18636\n",
      "Epoch: 0179 train_loss= 0.41532 time= 0.18346\n",
      "Epoch: 0180 train_loss= 0.41521 time= 0.18126\n",
      "Epoch: 0181 train_loss= 0.41510 time= 0.18989\n",
      "Epoch: 0182 train_loss= 0.41499 time= 0.18426\n",
      "Epoch: 0183 train_loss= 0.41489 time= 0.18228\n",
      "Epoch: 0184 train_loss= 0.41479 time= 0.19220\n",
      "Epoch: 0185 train_loss= 0.41469 time= 0.18595\n",
      "Epoch: 0186 train_loss= 0.41459 time= 0.18352\n",
      "Epoch: 0187 train_loss= 0.41449 time= 0.17904\n",
      "Epoch: 0188 train_loss= 0.41439 time= 0.18793\n",
      "Epoch: 0189 train_loss= 0.41428 time= 0.18226\n",
      "Epoch: 0190 train_loss= 0.41418 time= 0.18699\n",
      "Epoch: 0191 train_loss= 0.41407 time= 0.18467\n",
      "Epoch: 0192 train_loss= 0.41397 time= 0.19883\n",
      "Epoch: 0193 train_loss= 0.41386 time= 0.18820\n",
      "Epoch: 0194 train_loss= 0.41375 time= 0.18868\n",
      "Epoch: 0195 train_loss= 0.41364 time= 0.18652\n",
      "Epoch: 0196 train_loss= 0.41352 time= 0.18922\n",
      "Epoch: 0197 train_loss= 0.41341 time= 0.18670\n",
      "Epoch: 0198 train_loss= 0.41329 time= 0.18886\n",
      "Epoch: 0199 train_loss= 0.41317 time= 0.19135\n",
      "Epoch: 0200 train_loss= 0.41306 time= 0.19614\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79736 time= 0.26275\n",
      "Epoch: 0002 train_loss= 0.79675 time= 0.18945\n",
      "Epoch: 0003 train_loss= 0.79523 time= 0.18766\n",
      "Epoch: 0004 train_loss= 0.79224 time= 0.18752\n",
      "Epoch: 0005 train_loss= 0.78708 time= 0.19327\n",
      "Epoch: 0006 train_loss= 0.77904 time= 0.18050\n",
      "Epoch: 0007 train_loss= 0.76758 time= 0.18104\n",
      "Epoch: 0008 train_loss= 0.75279 time= 0.18405\n",
      "Epoch: 0009 train_loss= 0.73614 time= 0.18260\n",
      "Epoch: 0010 train_loss= 0.72137 time= 0.19130\n",
      "Epoch: 0011 train_loss= 0.71450 time= 0.18622\n",
      "Epoch: 0012 train_loss= 0.71687 time= 0.19034\n",
      "Epoch: 0013 train_loss= 0.71724 time= 0.19002\n",
      "Epoch: 0014 train_loss= 0.71018 time= 0.18596\n",
      "Epoch: 0015 train_loss= 0.69893 time= 0.19036\n",
      "Epoch: 0016 train_loss= 0.68755 time= 0.18516\n",
      "Epoch: 0017 train_loss= 0.67794 time= 0.18885\n",
      "Epoch: 0018 train_loss= 0.66986 time= 0.20430\n",
      "Epoch: 0019 train_loss= 0.66207 time= 0.19874\n",
      "Epoch: 0020 train_loss= 0.65336 time= 0.19591\n",
      "Epoch: 0021 train_loss= 0.64306 time= 0.19263\n",
      "Epoch: 0022 train_loss= 0.63107 time= 0.19705\n",
      "Epoch: 0023 train_loss= 0.61774 time= 0.19368\n",
      "Epoch: 0024 train_loss= 0.60377 time= 0.19638\n",
      "Epoch: 0025 train_loss= 0.58997 time= 0.18872\n",
      "Epoch: 0026 train_loss= 0.57717 time= 0.18096\n",
      "Epoch: 0027 train_loss= 0.56604 time= 0.18424\n",
      "Epoch: 0028 train_loss= 0.55688 time= 0.18137\n",
      "Epoch: 0029 train_loss= 0.54951 time= 0.18462\n",
      "Epoch: 0030 train_loss= 0.54328 time= 0.18831\n",
      "Epoch: 0031 train_loss= 0.53731 time= 0.18840\n",
      "Epoch: 0032 train_loss= 0.53095 time= 0.19595\n",
      "Epoch: 0033 train_loss= 0.52405 time= 0.18251\n",
      "Epoch: 0034 train_loss= 0.51690 time= 0.18827\n",
      "Epoch: 0035 train_loss= 0.51012 time= 0.18864\n",
      "Epoch: 0036 train_loss= 0.50425 time= 0.19368\n",
      "Epoch: 0037 train_loss= 0.49966 time= 0.19210\n",
      "Epoch: 0038 train_loss= 0.49636 time= 0.19271\n",
      "Epoch: 0039 train_loss= 0.49412 time= 0.19705\n",
      "Epoch: 0040 train_loss= 0.49251 time= 0.19281\n",
      "Epoch: 0041 train_loss= 0.49110 time= 0.19047\n",
      "Epoch: 0042 train_loss= 0.48954 time= 0.18513\n",
      "Epoch: 0043 train_loss= 0.48764 time= 0.18533\n",
      "Epoch: 0044 train_loss= 0.48536 time= 0.18547\n",
      "Epoch: 0045 train_loss= 0.48276 time= 0.18718\n",
      "Epoch: 0046 train_loss= 0.47999 time= 0.18618\n",
      "Epoch: 0047 train_loss= 0.47719 time= 0.19148\n",
      "Epoch: 0048 train_loss= 0.47449 time= 0.18616\n",
      "Epoch: 0049 train_loss= 0.47201 time= 0.19184\n",
      "Epoch: 0050 train_loss= 0.46979 time= 0.18686\n",
      "Epoch: 0051 train_loss= 0.46784 time= 0.18336\n",
      "Epoch: 0052 train_loss= 0.46610 time= 0.18707\n",
      "Epoch: 0053 train_loss= 0.46455 time= 0.18566\n",
      "Epoch: 0054 train_loss= 0.46312 time= 0.19135\n",
      "Epoch: 0055 train_loss= 0.46179 time= 0.19178\n",
      "Epoch: 0056 train_loss= 0.46054 time= 0.18631\n",
      "Epoch: 0057 train_loss= 0.45935 time= 0.19387\n",
      "Epoch: 0058 train_loss= 0.45823 time= 0.20267\n",
      "Epoch: 0059 train_loss= 0.45716 time= 0.19709\n",
      "Epoch: 0060 train_loss= 0.45613 time= 0.20105\n",
      "Epoch: 0061 train_loss= 0.45515 time= 0.18964\n",
      "Epoch: 0062 train_loss= 0.45420 time= 0.18974\n",
      "Epoch: 0063 train_loss= 0.45329 time= 0.18973\n",
      "Epoch: 0064 train_loss= 0.45242 time= 0.19424\n",
      "Epoch: 0065 train_loss= 0.45157 time= 0.19464\n",
      "Epoch: 0066 train_loss= 0.45075 time= 0.19035\n",
      "Epoch: 0067 train_loss= 0.44995 time= 0.19180\n",
      "Epoch: 0068 train_loss= 0.44917 time= 0.19076\n",
      "Epoch: 0069 train_loss= 0.44842 time= 0.18891\n",
      "Epoch: 0070 train_loss= 0.44771 time= 0.19095\n",
      "Epoch: 0071 train_loss= 0.44705 time= 0.19460\n",
      "Epoch: 0072 train_loss= 0.44646 time= 0.19020\n",
      "Epoch: 0073 train_loss= 0.44593 time= 0.18530\n",
      "Epoch: 0074 train_loss= 0.44544 time= 0.18836\n",
      "Epoch: 0075 train_loss= 0.44497 time= 0.19631\n",
      "Epoch: 0076 train_loss= 0.44452 time= 0.19023\n",
      "Epoch: 0077 train_loss= 0.44406 time= 0.18920\n",
      "Epoch: 0078 train_loss= 0.44360 time= 0.18334\n",
      "Epoch: 0079 train_loss= 0.44313 time= 0.17952\n",
      "Epoch: 0080 train_loss= 0.44267 time= 0.18009\n",
      "Epoch: 0081 train_loss= 0.44221 time= 0.18113\n",
      "Epoch: 0082 train_loss= 0.44176 time= 0.18593\n",
      "Epoch: 0083 train_loss= 0.44133 time= 0.19001\n",
      "Epoch: 0084 train_loss= 0.44091 time= 0.19808\n",
      "Epoch: 0085 train_loss= 0.44052 time= 0.18520\n",
      "Epoch: 0086 train_loss= 0.44014 time= 0.19095\n",
      "Epoch: 0087 train_loss= 0.43977 time= 0.19082\n",
      "Epoch: 0088 train_loss= 0.43943 time= 0.18504\n",
      "Epoch: 0089 train_loss= 0.43911 time= 0.18119\n",
      "Epoch: 0090 train_loss= 0.43882 time= 0.19530\n",
      "Epoch: 0091 train_loss= 0.43854 time= 0.20676\n",
      "Epoch: 0092 train_loss= 0.43829 time= 0.19781\n",
      "Epoch: 0093 train_loss= 0.43804 time= 0.21228\n",
      "Epoch: 0094 train_loss= 0.43781 time= 0.19630\n",
      "Epoch: 0095 train_loss= 0.43758 time= 0.19668\n",
      "Epoch: 0096 train_loss= 0.43735 time= 0.19289\n",
      "Epoch: 0097 train_loss= 0.43713 time= 0.18391\n",
      "Epoch: 0098 train_loss= 0.43690 time= 0.18459\n",
      "Epoch: 0099 train_loss= 0.43668 time= 0.18804\n",
      "Epoch: 0100 train_loss= 0.43645 time= 0.19631\n",
      "Epoch: 0101 train_loss= 0.43622 time= 0.18589\n",
      "Epoch: 0102 train_loss= 0.43599 time= 0.19478\n",
      "Epoch: 0103 train_loss= 0.43575 time= 0.19892\n",
      "Epoch: 0104 train_loss= 0.43551 time= 0.19727\n",
      "Epoch: 0105 train_loss= 0.43526 time= 0.19194\n",
      "Epoch: 0106 train_loss= 0.43501 time= 0.19152\n",
      "Epoch: 0107 train_loss= 0.43475 time= 0.18925\n",
      "Epoch: 0108 train_loss= 0.43448 time= 0.18909\n",
      "Epoch: 0109 train_loss= 0.43421 time= 0.18941\n",
      "Epoch: 0110 train_loss= 0.43393 time= 0.19107\n",
      "Epoch: 0111 train_loss= 0.43364 time= 0.19396\n",
      "Epoch: 0112 train_loss= 0.43334 time= 0.19492\n",
      "Epoch: 0113 train_loss= 0.43302 time= 0.18743\n",
      "Epoch: 0114 train_loss= 0.43270 time= 0.20004\n",
      "Epoch: 0115 train_loss= 0.43236 time= 0.19562\n",
      "Epoch: 0116 train_loss= 0.43200 time= 0.18942\n",
      "Epoch: 0117 train_loss= 0.43164 time= 0.19389\n",
      "Epoch: 0118 train_loss= 0.43127 time= 0.18457\n",
      "Epoch: 0119 train_loss= 0.43088 time= 0.18651\n",
      "Epoch: 0120 train_loss= 0.43049 time= 0.18350\n",
      "Epoch: 0121 train_loss= 0.43009 time= 0.18643\n",
      "Epoch: 0122 train_loss= 0.42969 time= 0.18710\n",
      "Epoch: 0123 train_loss= 0.42928 time= 0.18642\n",
      "Epoch: 0124 train_loss= 0.42887 time= 0.18938\n",
      "Epoch: 0125 train_loss= 0.42846 time= 0.19060\n",
      "Epoch: 0126 train_loss= 0.42806 time= 0.18569\n",
      "Epoch: 0127 train_loss= 0.42766 time= 0.21526\n",
      "Epoch: 0128 train_loss= 0.42726 time= 0.19572\n",
      "Epoch: 0129 train_loss= 0.42687 time= 0.19674\n",
      "Epoch: 0130 train_loss= 0.42650 time= 0.18184\n",
      "Epoch: 0131 train_loss= 0.42612 time= 0.19141\n",
      "Epoch: 0132 train_loss= 0.42576 time= 0.18447\n",
      "Epoch: 0133 train_loss= 0.42540 time= 0.18546\n",
      "Epoch: 0134 train_loss= 0.42504 time= 0.18347\n",
      "Epoch: 0135 train_loss= 0.42469 time= 0.18512\n",
      "Epoch: 0136 train_loss= 0.42434 time= 0.18335\n",
      "Epoch: 0137 train_loss= 0.42399 time= 0.19307\n",
      "Epoch: 0138 train_loss= 0.42364 time= 0.18384\n",
      "Epoch: 0139 train_loss= 0.42329 time= 0.20407\n",
      "Epoch: 0140 train_loss= 0.42295 time= 0.20435\n",
      "Epoch: 0141 train_loss= 0.42261 time= 0.19947\n",
      "Epoch: 0142 train_loss= 0.42228 time= 0.19254\n",
      "Epoch: 0143 train_loss= 0.42195 time= 0.19813\n",
      "Epoch: 0144 train_loss= 0.42163 time= 0.19740\n",
      "Epoch: 0145 train_loss= 0.42131 time= 0.19144\n",
      "Epoch: 0146 train_loss= 0.42100 time= 0.18505\n",
      "Epoch: 0147 train_loss= 0.42069 time= 0.18653\n",
      "Epoch: 0148 train_loss= 0.42039 time= 0.19525\n",
      "Epoch: 0149 train_loss= 0.42009 time= 0.20803\n",
      "Epoch: 0150 train_loss= 0.41979 time= 0.20695\n",
      "Epoch: 0151 train_loss= 0.41950 time= 0.19585\n",
      "Epoch: 0152 train_loss= 0.41921 time= 0.19629\n",
      "Epoch: 0153 train_loss= 0.41893 time= 0.19589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0154 train_loss= 0.41865 time= 0.20402\n",
      "Epoch: 0155 train_loss= 0.41837 time= 0.19659\n",
      "Epoch: 0156 train_loss= 0.41809 time= 0.19181\n",
      "Epoch: 0157 train_loss= 0.41781 time= 0.20140\n",
      "Epoch: 0158 train_loss= 0.41754 time= 0.19507\n",
      "Epoch: 0159 train_loss= 0.41726 time= 0.19595\n",
      "Epoch: 0160 train_loss= 0.41699 time= 0.19475\n",
      "Epoch: 0161 train_loss= 0.41671 time= 0.19579\n",
      "Epoch: 0162 train_loss= 0.41643 time= 0.19266\n",
      "Epoch: 0163 train_loss= 0.41616 time= 0.18913\n",
      "Epoch: 0164 train_loss= 0.41588 time= 0.19538\n",
      "Epoch: 0165 train_loss= 0.41561 time= 0.21558\n",
      "Epoch: 0166 train_loss= 0.41534 time= 0.19880\n",
      "Epoch: 0167 train_loss= 0.41507 time= 0.20108\n",
      "Epoch: 0168 train_loss= 0.41480 time= 0.19335\n",
      "Epoch: 0169 train_loss= 0.41454 time= 0.20393\n",
      "Epoch: 0170 train_loss= 0.41428 time= 0.19873\n",
      "Epoch: 0171 train_loss= 0.41403 time= 0.19047\n",
      "Epoch: 0172 train_loss= 0.41378 time= 0.19851\n",
      "Epoch: 0173 train_loss= 0.41354 time= 0.19477\n",
      "Epoch: 0174 train_loss= 0.41331 time= 0.19367\n",
      "Epoch: 0175 train_loss= 0.41308 time= 0.20255\n",
      "Epoch: 0176 train_loss= 0.41286 time= 0.20038\n",
      "Epoch: 0177 train_loss= 0.41265 time= 0.20311\n",
      "Epoch: 0178 train_loss= 0.41245 time= 0.19903\n",
      "Epoch: 0179 train_loss= 0.41225 time= 0.19797\n",
      "Epoch: 0180 train_loss= 0.41206 time= 0.19285\n",
      "Epoch: 0181 train_loss= 0.41188 time= 0.19011\n",
      "Epoch: 0182 train_loss= 0.41170 time= 0.20016\n",
      "Epoch: 0183 train_loss= 0.41153 time= 0.19276\n",
      "Epoch: 0184 train_loss= 0.41136 time= 0.19380\n",
      "Epoch: 0185 train_loss= 0.41120 time= 0.19959\n",
      "Epoch: 0186 train_loss= 0.41105 time= 0.19750\n",
      "Epoch: 0187 train_loss= 0.41090 time= 0.19722\n",
      "Epoch: 0188 train_loss= 0.41076 time= 0.20128\n",
      "Epoch: 0189 train_loss= 0.41062 time= 0.19538\n",
      "Epoch: 0190 train_loss= 0.41049 time= 0.19872\n",
      "Epoch: 0191 train_loss= 0.41037 time= 0.18849\n",
      "Epoch: 0192 train_loss= 0.41025 time= 0.20054\n",
      "Epoch: 0193 train_loss= 0.41014 time= 0.19264\n",
      "Epoch: 0194 train_loss= 0.41003 time= 0.19500\n",
      "Epoch: 0195 train_loss= 0.40993 time= 0.20269\n",
      "Epoch: 0196 train_loss= 0.40983 time= 0.19937\n",
      "Epoch: 0197 train_loss= 0.40973 time= 0.19859\n",
      "Epoch: 0198 train_loss= 0.40964 time= 0.19849\n",
      "Epoch: 0199 train_loss= 0.40955 time= 0.20067\n",
      "Epoch: 0200 train_loss= 0.40947 time= 0.19551\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79739 time= 0.27028\n",
      "Epoch: 0002 train_loss= 0.79685 time= 0.19129\n",
      "Epoch: 0003 train_loss= 0.79545 time= 0.19367\n",
      "Epoch: 0004 train_loss= 0.79267 time= 0.19109\n",
      "Epoch: 0005 train_loss= 0.78786 time= 0.20274\n",
      "Epoch: 0006 train_loss= 0.78032 time= 0.19581\n",
      "Epoch: 0007 train_loss= 0.76947 time= 0.19547\n",
      "Epoch: 0008 train_loss= 0.75525 time= 0.19296\n",
      "Epoch: 0009 train_loss= 0.73866 time= 0.18908\n",
      "Epoch: 0010 train_loss= 0.72255 time= 0.20539\n",
      "Epoch: 0011 train_loss= 0.71203 time= 0.18854\n",
      "Epoch: 0012 train_loss= 0.71080 time= 0.18837\n",
      "Epoch: 0013 train_loss= 0.71173 time= 0.19000\n",
      "Epoch: 0014 train_loss= 0.70601 time= 0.19235\n",
      "Epoch: 0015 train_loss= 0.69438 time= 0.18838\n",
      "Epoch: 0016 train_loss= 0.68107 time= 0.19569\n",
      "Epoch: 0017 train_loss= 0.66907 time= 0.19216\n",
      "Epoch: 0018 train_loss= 0.65904 time= 0.18697\n",
      "Epoch: 0019 train_loss= 0.65000 time= 0.18401\n",
      "Epoch: 0020 train_loss= 0.64076 time= 0.18911\n",
      "Epoch: 0021 train_loss= 0.63062 time= 0.19953\n",
      "Epoch: 0022 train_loss= 0.61963 time= 0.19447\n",
      "Epoch: 0023 train_loss= 0.60839 time= 0.19479\n",
      "Epoch: 0024 train_loss= 0.59782 time= 0.19158\n",
      "Epoch: 0025 train_loss= 0.58885 time= 0.19467\n",
      "Epoch: 0026 train_loss= 0.58204 time= 0.19156\n",
      "Epoch: 0027 train_loss= 0.57728 time= 0.19498\n",
      "Epoch: 0028 train_loss= 0.57359 time= 0.19218\n",
      "Epoch: 0029 train_loss= 0.56948 time= 0.18799\n",
      "Epoch: 0030 train_loss= 0.56384 time= 0.18987\n",
      "Epoch: 0031 train_loss= 0.55659 time= 0.18919\n",
      "Epoch: 0032 train_loss= 0.54845 time= 0.19890\n",
      "Epoch: 0033 train_loss= 0.54049 time= 0.19373\n",
      "Epoch: 0034 train_loss= 0.53357 time= 0.19127\n",
      "Epoch: 0035 train_loss= 0.52809 time= 0.19219\n",
      "Epoch: 0036 train_loss= 0.52394 time= 0.19987\n",
      "Epoch: 0037 train_loss= 0.52070 time= 0.20077\n",
      "Epoch: 0038 train_loss= 0.51786 time= 0.19619\n",
      "Epoch: 0039 train_loss= 0.51501 time= 0.19155\n",
      "Epoch: 0040 train_loss= 0.51189 time= 0.19250\n",
      "Epoch: 0041 train_loss= 0.50845 time= 0.18792\n",
      "Epoch: 0042 train_loss= 0.50473 time= 0.18851\n",
      "Epoch: 0043 train_loss= 0.50086 time= 0.19193\n",
      "Epoch: 0044 train_loss= 0.49702 time= 0.19164\n",
      "Epoch: 0045 train_loss= 0.49340 time= 0.18928\n",
      "Epoch: 0046 train_loss= 0.49016 time= 0.20152\n",
      "Epoch: 0047 train_loss= 0.48742 time= 0.18970\n",
      "Epoch: 0048 train_loss= 0.48525 time= 0.19184\n",
      "Epoch: 0049 train_loss= 0.48361 time= 0.19388\n",
      "Epoch: 0050 train_loss= 0.48241 time= 0.18479\n",
      "Epoch: 0051 train_loss= 0.48146 time= 0.18992\n",
      "Epoch: 0052 train_loss= 0.48058 time= 0.20111\n",
      "Epoch: 0053 train_loss= 0.47960 time= 0.19047\n",
      "Epoch: 0054 train_loss= 0.47846 time= 0.19291\n",
      "Epoch: 0055 train_loss= 0.47714 time= 0.19373\n",
      "Epoch: 0056 train_loss= 0.47572 time= 0.19273\n",
      "Epoch: 0057 train_loss= 0.47427 time= 0.18827\n",
      "Epoch: 0058 train_loss= 0.47287 time= 0.19541\n",
      "Epoch: 0059 train_loss= 0.47157 time= 0.18968\n",
      "Epoch: 0060 train_loss= 0.47038 time= 0.18583\n",
      "Epoch: 0061 train_loss= 0.46931 time= 0.18486\n",
      "Epoch: 0062 train_loss= 0.46835 time= 0.19222\n",
      "Epoch: 0063 train_loss= 0.46748 time= 0.18837\n",
      "Epoch: 0064 train_loss= 0.46669 time= 0.19297\n",
      "Epoch: 0065 train_loss= 0.46594 time= 0.19357\n",
      "Epoch: 0066 train_loss= 0.46520 time= 0.19091\n",
      "Epoch: 0067 train_loss= 0.46444 time= 0.18736\n",
      "Epoch: 0068 train_loss= 0.46364 time= 0.18739\n",
      "Epoch: 0069 train_loss= 0.46280 time= 0.19022\n",
      "Epoch: 0070 train_loss= 0.46194 time= 0.19058\n",
      "Epoch: 0071 train_loss= 0.46107 time= 0.18594\n",
      "Epoch: 0072 train_loss= 0.46020 time= 0.19158\n",
      "Epoch: 0073 train_loss= 0.45933 time= 0.19049\n",
      "Epoch: 0074 train_loss= 0.45847 time= 0.19286\n",
      "Epoch: 0075 train_loss= 0.45762 time= 0.19156\n",
      "Epoch: 0076 train_loss= 0.45677 time= 0.19346\n",
      "Epoch: 0077 train_loss= 0.45594 time= 0.18889\n",
      "Epoch: 0078 train_loss= 0.45512 time= 0.19150\n",
      "Epoch: 0079 train_loss= 0.45432 time= 0.18857\n",
      "Epoch: 0080 train_loss= 0.45352 time= 0.19404\n",
      "Epoch: 0081 train_loss= 0.45273 time= 0.18608\n",
      "Epoch: 0082 train_loss= 0.45196 time= 0.19053\n",
      "Epoch: 0083 train_loss= 0.45120 time= 0.19069\n",
      "Epoch: 0084 train_loss= 0.45047 time= 0.18675\n",
      "Epoch: 0085 train_loss= 0.44977 time= 0.19702\n",
      "Epoch: 0086 train_loss= 0.44910 time= 0.19173\n",
      "Epoch: 0087 train_loss= 0.44846 time= 0.18637\n",
      "Epoch: 0088 train_loss= 0.44786 time= 0.19611\n",
      "Epoch: 0089 train_loss= 0.44729 time= 0.19430\n",
      "Epoch: 0090 train_loss= 0.44675 time= 0.19155\n",
      "Epoch: 0091 train_loss= 0.44623 time= 0.19207\n",
      "Epoch: 0092 train_loss= 0.44574 time= 0.18353\n",
      "Epoch: 0093 train_loss= 0.44527 time= 0.19384\n",
      "Epoch: 0094 train_loss= 0.44481 time= 0.18850\n",
      "Epoch: 0095 train_loss= 0.44435 time= 0.20155\n",
      "Epoch: 0096 train_loss= 0.44389 time= 0.18868\n",
      "Epoch: 0097 train_loss= 0.44343 time= 0.19275\n",
      "Epoch: 0098 train_loss= 0.44296 time= 0.18824\n",
      "Epoch: 0099 train_loss= 0.44248 time= 0.19526\n",
      "Epoch: 0100 train_loss= 0.44199 time= 0.19457\n",
      "Epoch: 0101 train_loss= 0.44149 time= 0.18688\n",
      "Epoch: 0102 train_loss= 0.44099 time= 0.19221\n",
      "Epoch: 0103 train_loss= 0.44047 time= 0.18844\n",
      "Epoch: 0104 train_loss= 0.43994 time= 0.19503\n",
      "Epoch: 0105 train_loss= 0.43941 time= 0.19060\n",
      "Epoch: 0106 train_loss= 0.43887 time= 0.20103\n",
      "Epoch: 0107 train_loss= 0.43831 time= 0.19161\n",
      "Epoch: 0108 train_loss= 0.43775 time= 0.18467\n",
      "Epoch: 0109 train_loss= 0.43717 time= 0.19972\n",
      "Epoch: 0110 train_loss= 0.43658 time= 0.18924\n",
      "Epoch: 0111 train_loss= 0.43599 time= 0.18941\n",
      "Epoch: 0112 train_loss= 0.43539 time= 0.19234\n",
      "Epoch: 0113 train_loss= 0.43478 time= 0.19262\n",
      "Epoch: 0114 train_loss= 0.43417 time= 0.19215\n",
      "Epoch: 0115 train_loss= 0.43357 time= 0.19271\n",
      "Epoch: 0116 train_loss= 0.43297 time= 0.19126\n",
      "Epoch: 0117 train_loss= 0.43239 time= 0.19019\n",
      "Epoch: 0118 train_loss= 0.43182 time= 0.19694\n",
      "Epoch: 0119 train_loss= 0.43128 time= 0.19244\n",
      "Epoch: 0120 train_loss= 0.43076 time= 0.19470\n",
      "Epoch: 0121 train_loss= 0.43027 time= 0.19192\n",
      "Epoch: 0122 train_loss= 0.42980 time= 0.19649\n",
      "Epoch: 0123 train_loss= 0.42936 time= 0.18416\n",
      "Epoch: 0124 train_loss= 0.42893 time= 0.19624\n",
      "Epoch: 0125 train_loss= 0.42853 time= 0.19185\n",
      "Epoch: 0126 train_loss= 0.42813 time= 0.19255\n",
      "Epoch: 0127 train_loss= 0.42775 time= 0.18947\n",
      "Epoch: 0128 train_loss= 0.42737 time= 0.19768\n",
      "Epoch: 0129 train_loss= 0.42700 time= 0.19353\n",
      "Epoch: 0130 train_loss= 0.42663 time= 0.19259\n",
      "Epoch: 0131 train_loss= 0.42627 time= 0.19496\n",
      "Epoch: 0132 train_loss= 0.42592 time= 0.19751\n",
      "Epoch: 0133 train_loss= 0.42557 time= 0.19160\n",
      "Epoch: 0134 train_loss= 0.42524 time= 0.18847\n",
      "Epoch: 0135 train_loss= 0.42492 time= 0.19848\n",
      "Epoch: 0136 train_loss= 0.42461 time= 0.19442\n",
      "Epoch: 0137 train_loss= 0.42432 time= 0.19951\n",
      "Epoch: 0138 train_loss= 0.42404 time= 0.19321\n",
      "Epoch: 0139 train_loss= 0.42376 time= 0.19739\n",
      "Epoch: 0140 train_loss= 0.42349 time= 0.19886\n",
      "Epoch: 0141 train_loss= 0.42323 time= 0.19304\n",
      "Epoch: 0142 train_loss= 0.42297 time= 0.19992\n",
      "Epoch: 0143 train_loss= 0.42270 time= 0.19575\n",
      "Epoch: 0144 train_loss= 0.42244 time= 0.19212\n",
      "Epoch: 0145 train_loss= 0.42218 time= 0.19103\n",
      "Epoch: 0146 train_loss= 0.42191 time= 0.19867\n",
      "Epoch: 0147 train_loss= 0.42165 time= 0.19248\n",
      "Epoch: 0148 train_loss= 0.42139 time= 0.19421\n",
      "Epoch: 0149 train_loss= 0.42114 time= 0.18807\n",
      "Epoch: 0150 train_loss= 0.42088 time= 0.19882\n",
      "Epoch: 0151 train_loss= 0.42064 time= 0.21089\n",
      "Epoch: 0152 train_loss= 0.42039 time= 0.19800\n",
      "Epoch: 0153 train_loss= 0.42015 time= 0.19244\n",
      "Epoch: 0154 train_loss= 0.41991 time= 0.19248\n",
      "Epoch: 0155 train_loss= 0.41968 time= 0.19709\n",
      "Epoch: 0156 train_loss= 0.41945 time= 0.19063\n",
      "Epoch: 0157 train_loss= 0.41922 time= 0.19649\n",
      "Epoch: 0158 train_loss= 0.41899 time= 0.19168\n",
      "Epoch: 0159 train_loss= 0.41876 time= 0.19090\n",
      "Epoch: 0160 train_loss= 0.41853 time= 0.20388\n",
      "Epoch: 0161 train_loss= 0.41831 time= 0.19748\n",
      "Epoch: 0162 train_loss= 0.41808 time= 0.19477\n",
      "Epoch: 0163 train_loss= 0.41786 time= 0.18957\n",
      "Epoch: 0164 train_loss= 0.41763 time= 0.18877\n",
      "Epoch: 0165 train_loss= 0.41741 time= 0.19180\n",
      "Epoch: 0166 train_loss= 0.41719 time= 0.19428\n",
      "Epoch: 0167 train_loss= 0.41697 time= 0.19487\n",
      "Epoch: 0168 train_loss= 0.41676 time= 0.19263\n",
      "Epoch: 0169 train_loss= 0.41654 time= 0.18923\n",
      "Epoch: 0170 train_loss= 0.41633 time= 0.19286\n",
      "Epoch: 0171 train_loss= 0.41612 time= 0.19104\n",
      "Epoch: 0172 train_loss= 0.41591 time= 0.19488\n",
      "Epoch: 0173 train_loss= 0.41571 time= 0.19229\n",
      "Epoch: 0174 train_loss= 0.41551 time= 0.18224\n",
      "Epoch: 0175 train_loss= 0.41531 time= 0.19118\n",
      "Epoch: 0176 train_loss= 0.41512 time= 0.19461\n",
      "Epoch: 0177 train_loss= 0.41492 time= 0.18868\n",
      "Epoch: 0178 train_loss= 0.41474 time= 0.19451\n",
      "Epoch: 0179 train_loss= 0.41455 time= 0.19203\n",
      "Epoch: 0180 train_loss= 0.41437 time= 0.19148\n",
      "Epoch: 0181 train_loss= 0.41420 time= 0.19152\n",
      "Epoch: 0182 train_loss= 0.41402 time= 0.19402\n",
      "Epoch: 0183 train_loss= 0.41385 time= 0.19284\n",
      "Epoch: 0184 train_loss= 0.41369 time= 0.19182\n",
      "Epoch: 0185 train_loss= 0.41352 time= 0.18774\n",
      "Epoch: 0186 train_loss= 0.41336 time= 0.19746\n",
      "Epoch: 0187 train_loss= 0.41320 time= 0.18849\n",
      "Epoch: 0188 train_loss= 0.41304 time= 0.19866\n",
      "Epoch: 0189 train_loss= 0.41289 time= 0.19191\n",
      "Epoch: 0190 train_loss= 0.41273 time= 0.20024\n",
      "Epoch: 0191 train_loss= 0.41258 time= 0.19483\n",
      "Epoch: 0192 train_loss= 0.41243 time= 0.19783\n",
      "Epoch: 0193 train_loss= 0.41228 time= 0.19646\n",
      "Epoch: 0194 train_loss= 0.41213 time= 0.19508\n",
      "Epoch: 0195 train_loss= 0.41198 time= 0.19276\n",
      "Epoch: 0196 train_loss= 0.41184 time= 0.19103\n",
      "Epoch: 0197 train_loss= 0.41169 time= 0.19832\n",
      "Epoch: 0198 train_loss= 0.41155 time= 0.19032\n",
      "Epoch: 0199 train_loss= 0.41142 time= 0.18911\n",
      "Epoch: 0200 train_loss= 0.41128 time= 0.19249\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79734 time= 0.27354\n",
      "Epoch: 0002 train_loss= 0.79671 time= 0.19380\n",
      "Epoch: 0003 train_loss= 0.79516 time= 0.19173\n",
      "Epoch: 0004 train_loss= 0.79209 time= 0.19350\n",
      "Epoch: 0005 train_loss= 0.78680 time= 0.18945\n",
      "Epoch: 0006 train_loss= 0.77854 time= 0.19211\n",
      "Epoch: 0007 train_loss= 0.76680 time= 0.19051\n",
      "Epoch: 0008 train_loss= 0.75188 time= 0.19725\n",
      "Epoch: 0009 train_loss= 0.73568 time= 0.19700\n",
      "Epoch: 0010 train_loss= 0.72279 time= 0.19335\n",
      "Epoch: 0011 train_loss= 0.71977 time= 0.19096\n",
      "Epoch: 0012 train_loss= 0.72426 time= 0.19829\n",
      "Epoch: 0013 train_loss= 0.72314 time= 0.18869\n",
      "Epoch: 0014 train_loss= 0.71481 time= 0.19080\n",
      "Epoch: 0015 train_loss= 0.70425 time= 0.19248\n",
      "Epoch: 0016 train_loss= 0.69527 time= 0.19773\n",
      "Epoch: 0017 train_loss= 0.68877 time= 0.19495\n",
      "Epoch: 0018 train_loss= 0.68362 time= 0.19381\n",
      "Epoch: 0019 train_loss= 0.67824 time= 0.20318\n",
      "Epoch: 0020 train_loss= 0.67152 time= 0.19385\n",
      "Epoch: 0021 train_loss= 0.66299 time= 0.19855\n",
      "Epoch: 0022 train_loss= 0.65268 time= 0.20490\n",
      "Epoch: 0023 train_loss= 0.64096 time= 0.19367\n",
      "Epoch: 0024 train_loss= 0.62837 time= 0.19857\n",
      "Epoch: 0025 train_loss= 0.61550 time= 0.19557\n",
      "Epoch: 0026 train_loss= 0.60281 time= 0.19475\n",
      "Epoch: 0027 train_loss= 0.59061 time= 0.19890\n",
      "Epoch: 0028 train_loss= 0.57904 time= 0.20276\n",
      "Epoch: 0029 train_loss= 0.56823 time= 0.19728\n",
      "Epoch: 0030 train_loss= 0.55836 time= 0.19636\n",
      "Epoch: 0031 train_loss= 0.54956 time= 0.19624\n",
      "Epoch: 0032 train_loss= 0.54179 time= 0.19957\n",
      "Epoch: 0033 train_loss= 0.53484 time= 0.20457\n",
      "Epoch: 0034 train_loss= 0.52838 time= 0.20560\n",
      "Epoch: 0035 train_loss= 0.52219 time= 0.20559\n",
      "Epoch: 0036 train_loss= 0.51623 time= 0.19791\n",
      "Epoch: 0037 train_loss= 0.51062 time= 0.20160\n",
      "Epoch: 0038 train_loss= 0.50558 time= 0.19448\n",
      "Epoch: 0039 train_loss= 0.50125 time= 0.19049\n",
      "Epoch: 0040 train_loss= 0.49767 time= 0.20190\n",
      "Epoch: 0041 train_loss= 0.49471 time= 0.20191\n",
      "Epoch: 0042 train_loss= 0.49215 time= 0.18990\n",
      "Epoch: 0043 train_loss= 0.48980 time= 0.19306\n",
      "Epoch: 0044 train_loss= 0.48757 time= 0.19224\n",
      "Epoch: 0045 train_loss= 0.48548 time= 0.19854\n",
      "Epoch: 0046 train_loss= 0.48356 time= 0.19724\n",
      "Epoch: 0047 train_loss= 0.48185 time= 0.19911\n",
      "Epoch: 0048 train_loss= 0.48032 time= 0.19990\n",
      "Epoch: 0049 train_loss= 0.47889 time= 0.20024\n",
      "Epoch: 0050 train_loss= 0.47751 time= 0.20034\n",
      "Epoch: 0051 train_loss= 0.47613 time= 0.19951\n",
      "Epoch: 0052 train_loss= 0.47475 time= 0.18900\n",
      "Epoch: 0053 train_loss= 0.47337 time= 0.19049\n",
      "Epoch: 0054 train_loss= 0.47201 time= 0.18481\n",
      "Epoch: 0055 train_loss= 0.47068 time= 0.19256\n",
      "Epoch: 0056 train_loss= 0.46939 time= 0.18785\n",
      "Epoch: 0057 train_loss= 0.46817 time= 0.20200\n",
      "Epoch: 0058 train_loss= 0.46702 time= 0.19984\n",
      "Epoch: 0059 train_loss= 0.46594 time= 0.19621\n",
      "Epoch: 0060 train_loss= 0.46494 time= 0.19566\n",
      "Epoch: 0061 train_loss= 0.46400 time= 0.19723\n",
      "Epoch: 0062 train_loss= 0.46308 time= 0.20330\n",
      "Epoch: 0063 train_loss= 0.46218 time= 0.18836\n",
      "Epoch: 0064 train_loss= 0.46126 time= 0.19643\n",
      "Epoch: 0065 train_loss= 0.46032 time= 0.19467\n",
      "Epoch: 0066 train_loss= 0.45935 time= 0.19498\n",
      "Epoch: 0067 train_loss= 0.45839 time= 0.19227\n",
      "Epoch: 0068 train_loss= 0.45743 time= 0.19541\n",
      "Epoch: 0069 train_loss= 0.45652 time= 0.19435\n",
      "Epoch: 0070 train_loss= 0.45567 time= 0.18865\n",
      "Epoch: 0071 train_loss= 0.45489 time= 0.18638\n",
      "Epoch: 0072 train_loss= 0.45417 time= 0.19798\n",
      "Epoch: 0073 train_loss= 0.45352 time= 0.19142\n",
      "Epoch: 0074 train_loss= 0.45290 time= 0.18804\n",
      "Epoch: 0075 train_loss= 0.45232 time= 0.18686\n",
      "Epoch: 0076 train_loss= 0.45173 time= 0.19048\n",
      "Epoch: 0077 train_loss= 0.45115 time= 0.19014\n",
      "Epoch: 0078 train_loss= 0.45055 time= 0.18743\n",
      "Epoch: 0079 train_loss= 0.44995 time= 0.19084\n",
      "Epoch: 0080 train_loss= 0.44934 time= 0.18898\n",
      "Epoch: 0081 train_loss= 0.44875 time= 0.19482\n",
      "Epoch: 0082 train_loss= 0.44815 time= 0.18703\n",
      "Epoch: 0083 train_loss= 0.44756 time= 0.19307\n",
      "Epoch: 0084 train_loss= 0.44697 time= 0.18736\n",
      "Epoch: 0085 train_loss= 0.44639 time= 0.18951\n",
      "Epoch: 0086 train_loss= 0.44581 time= 0.19180\n",
      "Epoch: 0087 train_loss= 0.44524 time= 0.19259\n",
      "Epoch: 0088 train_loss= 0.44468 time= 0.18411\n",
      "Epoch: 0089 train_loss= 0.44413 time= 0.19126\n",
      "Epoch: 0090 train_loss= 0.44360 time= 0.18753\n",
      "Epoch: 0091 train_loss= 0.44307 time= 0.18853\n",
      "Epoch: 0092 train_loss= 0.44255 time= 0.19665\n",
      "Epoch: 0093 train_loss= 0.44203 time= 0.19202\n",
      "Epoch: 0094 train_loss= 0.44152 time= 0.19346\n",
      "Epoch: 0095 train_loss= 0.44100 time= 0.18836\n",
      "Epoch: 0096 train_loss= 0.44048 time= 0.18601\n",
      "Epoch: 0097 train_loss= 0.43995 time= 0.18864\n",
      "Epoch: 0098 train_loss= 0.43942 time= 0.20382\n",
      "Epoch: 0099 train_loss= 0.43887 time= 0.18740\n",
      "Epoch: 0100 train_loss= 0.43831 time= 0.19171\n",
      "Epoch: 0101 train_loss= 0.43775 time= 0.19221\n",
      "Epoch: 0102 train_loss= 0.43718 time= 0.19107\n",
      "Epoch: 0103 train_loss= 0.43662 time= 0.19162\n",
      "Epoch: 0104 train_loss= 0.43606 time= 0.18967\n",
      "Epoch: 0105 train_loss= 0.43550 time= 0.19100\n",
      "Epoch: 0106 train_loss= 0.43495 time= 0.18357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0107 train_loss= 0.43442 time= 0.18872\n",
      "Epoch: 0108 train_loss= 0.43390 time= 0.19248\n",
      "Epoch: 0109 train_loss= 0.43339 time= 0.18754\n",
      "Epoch: 0110 train_loss= 0.43289 time= 0.19986\n",
      "Epoch: 0111 train_loss= 0.43241 time= 0.19506\n",
      "Epoch: 0112 train_loss= 0.43194 time= 0.19653\n",
      "Epoch: 0113 train_loss= 0.43148 time= 0.19323\n",
      "Epoch: 0114 train_loss= 0.43104 time= 0.19618\n",
      "Epoch: 0115 train_loss= 0.43060 time= 0.19486\n",
      "Epoch: 0116 train_loss= 0.43017 time= 0.18028\n",
      "Epoch: 0117 train_loss= 0.42975 time= 0.18449\n",
      "Epoch: 0118 train_loss= 0.42933 time= 0.18759\n",
      "Epoch: 0119 train_loss= 0.42892 time= 0.18558\n",
      "Epoch: 0120 train_loss= 0.42852 time= 0.18910\n",
      "Epoch: 0121 train_loss= 0.42813 time= 0.18880\n",
      "Epoch: 0122 train_loss= 0.42775 time= 0.19178\n",
      "Epoch: 0123 train_loss= 0.42737 time= 0.18945\n",
      "Epoch: 0124 train_loss= 0.42701 time= 0.18985\n",
      "Epoch: 0125 train_loss= 0.42667 time= 0.19112\n",
      "Epoch: 0126 train_loss= 0.42633 time= 0.18444\n",
      "Epoch: 0127 train_loss= 0.42600 time= 0.18454\n",
      "Epoch: 0128 train_loss= 0.42569 time= 0.18983\n",
      "Epoch: 0129 train_loss= 0.42538 time= 0.18850\n",
      "Epoch: 0130 train_loss= 0.42508 time= 0.19160\n",
      "Epoch: 0131 train_loss= 0.42479 time= 0.18687\n",
      "Epoch: 0132 train_loss= 0.42450 time= 0.18671\n",
      "Epoch: 0133 train_loss= 0.42422 time= 0.18735\n",
      "Epoch: 0134 train_loss= 0.42394 time= 0.19078\n",
      "Epoch: 0135 train_loss= 0.42365 time= 0.18551\n",
      "Epoch: 0136 train_loss= 0.42337 time= 0.19351\n",
      "Epoch: 0137 train_loss= 0.42309 time= 0.18462\n",
      "Epoch: 0138 train_loss= 0.42281 time= 0.18495\n",
      "Epoch: 0139 train_loss= 0.42252 time= 0.18877\n",
      "Epoch: 0140 train_loss= 0.42224 time= 0.19321\n",
      "Epoch: 0141 train_loss= 0.42196 time= 0.18827\n",
      "Epoch: 0142 train_loss= 0.42168 time= 0.18871\n",
      "Epoch: 0143 train_loss= 0.42140 time= 0.19184\n",
      "Epoch: 0144 train_loss= 0.42112 time= 0.18932\n",
      "Epoch: 0145 train_loss= 0.42085 time= 0.19288\n",
      "Epoch: 0146 train_loss= 0.42059 time= 0.18576\n",
      "Epoch: 0147 train_loss= 0.42033 time= 0.18849\n",
      "Epoch: 0148 train_loss= 0.42008 time= 0.18320\n",
      "Epoch: 0149 train_loss= 0.41984 time= 0.18765\n",
      "Epoch: 0150 train_loss= 0.41960 time= 0.18989\n",
      "Epoch: 0151 train_loss= 0.41936 time= 0.18659\n",
      "Epoch: 0152 train_loss= 0.41913 time= 0.19418\n",
      "Epoch: 0153 train_loss= 0.41890 time= 0.19014\n",
      "Epoch: 0154 train_loss= 0.41867 time= 0.18847\n",
      "Epoch: 0155 train_loss= 0.41844 time= 0.18898\n",
      "Epoch: 0156 train_loss= 0.41821 time= 0.18702\n",
      "Epoch: 0157 train_loss= 0.41797 time= 0.19006\n",
      "Epoch: 0158 train_loss= 0.41774 time= 0.18627\n",
      "Epoch: 0159 train_loss= 0.41750 time= 0.18433\n",
      "Epoch: 0160 train_loss= 0.41725 time= 0.19386\n",
      "Epoch: 0161 train_loss= 0.41701 time= 0.19306\n",
      "Epoch: 0162 train_loss= 0.41676 time= 0.18890\n",
      "Epoch: 0163 train_loss= 0.41652 time= 0.18740\n",
      "Epoch: 0164 train_loss= 0.41627 time= 0.19906\n",
      "Epoch: 0165 train_loss= 0.41603 time= 0.18438\n",
      "Epoch: 0166 train_loss= 0.41579 time= 0.18972\n",
      "Epoch: 0167 train_loss= 0.41555 time= 0.19026\n",
      "Epoch: 0168 train_loss= 0.41531 time= 0.19045\n",
      "Epoch: 0169 train_loss= 0.41508 time= 0.18386\n",
      "Epoch: 0170 train_loss= 0.41485 time= 0.19004\n",
      "Epoch: 0171 train_loss= 0.41462 time= 0.19443\n",
      "Epoch: 0172 train_loss= 0.41440 time= 0.18641\n",
      "Epoch: 0173 train_loss= 0.41419 time= 0.19357\n",
      "Epoch: 0174 train_loss= 0.41398 time= 0.18745\n",
      "Epoch: 0175 train_loss= 0.41378 time= 0.19815\n",
      "Epoch: 0176 train_loss= 0.41358 time= 0.19055\n",
      "Epoch: 0177 train_loss= 0.41339 time= 0.18877\n",
      "Epoch: 0178 train_loss= 0.41321 time= 0.18931\n",
      "Epoch: 0179 train_loss= 0.41303 time= 0.18867\n",
      "Epoch: 0180 train_loss= 0.41286 time= 0.18328\n",
      "Epoch: 0181 train_loss= 0.41270 time= 0.18952\n",
      "Epoch: 0182 train_loss= 0.41254 time= 0.19090\n",
      "Epoch: 0183 train_loss= 0.41239 time= 0.18901\n",
      "Epoch: 0184 train_loss= 0.41224 time= 0.19031\n",
      "Epoch: 0185 train_loss= 0.41209 time= 0.19062\n",
      "Epoch: 0186 train_loss= 0.41196 time= 0.19486\n",
      "Epoch: 0187 train_loss= 0.41182 time= 0.19219\n",
      "Epoch: 0188 train_loss= 0.41169 time= 0.18456\n",
      "Epoch: 0189 train_loss= 0.41156 time= 0.19022\n",
      "Epoch: 0190 train_loss= 0.41144 time= 0.18690\n",
      "Epoch: 0191 train_loss= 0.41132 time= 0.18982\n",
      "Epoch: 0192 train_loss= 0.41120 time= 0.18650\n",
      "Epoch: 0193 train_loss= 0.41109 time= 0.19023\n",
      "Epoch: 0194 train_loss= 0.41097 time= 0.19134\n",
      "Epoch: 0195 train_loss= 0.41086 time= 0.18664\n",
      "Epoch: 0196 train_loss= 0.41076 time= 0.18651\n",
      "Epoch: 0197 train_loss= 0.41065 time= 0.19349\n",
      "Epoch: 0198 train_loss= 0.41055 time= 0.19194\n",
      "Epoch: 0199 train_loss= 0.41044 time= 0.18942\n",
      "Epoch: 0200 train_loss= 0.41034 time= 0.19113\n",
      "Testing model...\n"
     ]
    }
   ],
   "source": [
    "# The entire training+test process is repeated FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "\n",
    "    if FLAGS.task == 'link_prediction' :\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking test edges...\")\n",
    "        # Edge Masking for Link Prediction: compute Train/Validation/Test set\n",
    "        adj, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "        mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "\n",
    "    # Start computation of running times\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Degeneracy Framework / K-Core Decomposition\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Starting k-core decomposition of the graph\")\n",
    "        # Save adjacency matrix of un-decomposed graph\n",
    "        # (needed to embed nodes that are not in k-core, after GAE training)\n",
    "        adj_orig = adj\n",
    "        # Get the (smaller) adjacency matrix of the k-core subgraph,\n",
    "        # and the corresponding nodes\n",
    "        adj, nodes_kcore = compute_kcore(adj, FLAGS.k)\n",
    "        # Get the (smaller) feature matrix of the nb_core graph\n",
    "        if FLAGS.features:\n",
    "            features = features_init[nodes_kcore,:]\n",
    "        # Flag to compute k-core decomposition's running time\n",
    "        t_core = time.time()\n",
    "    elif FLAGS.features:\n",
    "        features = features_init\n",
    "\n",
    "    # Preprocessing and initialization\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing and Initializing...\")\n",
    "    # Compute number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "    # If features are not used, replace feature matrix by identity matrix\n",
    "    if not FLAGS.features:\n",
    "        features = sp.identity(adj.shape[0])\n",
    "    # Preprocessing on node features\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ())\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = None\n",
    "    if FLAGS.model == 'gcn_ae':\n",
    "        # Standard Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # Standard Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                            features_nonzero)\n",
    "    elif FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes,\n",
    "                               features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_ae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = DeepGCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Variational Autoencoder\n",
    "        model = DeepGCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                                features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "    # Optimizer\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0]\n",
    "                                                - adj.sum()) * 2)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # Optimizer for Non-Variational Autoencoders\n",
    "        if FLAGS.model in ('gcn_ae', 'linear_ae', 'deep_gcn_ae'):\n",
    "            opt = OptimizerAE(preds = model.reconstructions,\n",
    "                              labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                            validate_indices = False), [-1]),\n",
    "                              pos_weight = pos_weight,\n",
    "                              norm = norm)\n",
    "        # Optimizer for Variational Autoencoders\n",
    "        elif FLAGS.model in ('gcn_vae', 'linear_vae', 'deep_gcn_vae'):\n",
    "            opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                               labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                             validate_indices = False), [-1]),\n",
    "                               model = model,\n",
    "                               num_nodes = num_nodes,\n",
    "                               pos_weight = pos_weight,\n",
    "                               norm = norm)\n",
    "\n",
    "    # Normalization and preprocessing on adjacency matrix\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = sparse_to_tuple(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Initialize TF session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Training...\")\n",
    "\n",
    "    for epoch in range(FLAGS.epochs):\n",
    "        # Flag to compute running time for each epoch\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_label, features,\n",
    "                                        placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.accuracy],\n",
    "                        feed_dict = feed_dict)\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display epoch information\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "            # Validation, for Link Prediction\n",
    "            if not FLAGS.kcore and FLAGS.validation and FLAGS.task == 'link_prediction':\n",
    "                feed_dict.update({placeholders['dropout']: 0})\n",
    "                emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "                feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "                val_roc, val_ap = get_roc_score(val_edges, val_edges_false, emb)\n",
    "                print(\"val_roc=\", \"{:.5f}\".format(val_roc), \"val_ap=\", \"{:.5f}\".format(val_ap))\n",
    "\n",
    "    # Flag to compute Graph AE/VAE training time\n",
    "    t_model = time.time()\n",
    "\n",
    "    # Compute embedding\n",
    "\n",
    "    # Get embedding from model\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "\n",
    "    # If k-core is used, only part of the nodes from the original\n",
    "    # graph are embedded. The remaining ones are projected in the\n",
    "    # latent space via the expand_embedding heuristic\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Propagation to remaining nodes...\")\n",
    "        # Project remaining nodes in latent space\n",
    "        emb = expand_embedding(adj_orig, emb, nodes_kcore, FLAGS.nb_iterations)\n",
    "        # Compute mean running times for K-Core, GAE Train and Propagation steps\n",
    "        mean_time_expand.append(time.time() - t_model)\n",
    "        mean_time_train.append(t_model - t_core)\n",
    "        mean_time_kcore.append(t_core - t_start)\n",
    "        # Compute mean size of K-Core graph\n",
    "        # Note: size is fixed if task is node clustering, but will vary if\n",
    "        # task is link prediction due to edge masking\n",
    "        mean_core_size.append(len(nodes_kcore))\n",
    "\n",
    "    # Compute mean total running time\n",
    "    mean_time.append(time.time() - t_start)\n",
    "\n",
    "    # Test model\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing model...\")\n",
    "    # Link Prediction: classification edges/non-edges\n",
    "    if FLAGS.task == 'link_prediction':\n",
    "        # Get ROC and AP scores\n",
    "        ap_score, roc_score, acc_score, f1_score = get_roc_score(test_edges, test_edges_false, emb)\n",
    "        # Report scores\n",
    "        mean_ap.append(ap_score)\n",
    "        mean_roc.append(roc_score)\n",
    "        mean_acc.append(acc_score)\n",
    "        mean_f1.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c0c9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:44:40.742944Z",
     "start_time": "2022-05-02T12:44:40.719097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results for gcn_ae model on cora on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AP scores\n",
      " [0.8986684401177076, 0.8769288132694488, 0.8700742895945683, 0.8826454669888419, 0.8718322324755028, 0.8925877426924322, 0.8993076930518, 0.8704754522517385, 0.8719461553621801, 0.8904244308913802]\n",
      "Mean AP score:  0.88248907166956 \n",
      "Std of AP scores:  0.011241473103280043 \n",
      " \n",
      "\n",
      "AUC scores\n",
      " [0.8626502813894119, 0.8306226573386286, 0.826973416531943, 0.8472251727403332, 0.8296630888384, 0.8627150927702905, 0.8621587950844167, 0.8260444534060182, 0.8261542726902844, 0.8550925542525267]\n",
      "Mean AUC score:  0.8429299785042254 \n",
      "Std of AUC scores:  0.01570064670926057 \n",
      " \n",
      "\n",
      "ACC scores\n",
      " [0.7409867172675522, 0.6755218216318786, 0.6755218216318786, 0.6935483870967742, 0.7049335863377609, 0.722011385199241, 0.715370018975332, 0.6859582542694497, 0.6650853889943074, 0.698292220113852]\n",
      "Mean ACC score:  0.6977229601518026 \n",
      "Std of ACC scores:  0.022432662931509147 \n",
      " \n",
      "\n",
      "F1 scores\n",
      " [0.7676595744680851, 0.7150000000000001, 0.7076923076923075, 0.7328370554177005, 0.7379949452401011, 0.7564422277639234, 0.7483221476510068, 0.7202028740490278, 0.7137064071370642, 0.739770867430442]\n",
      "Mean F1 score:  0.7339628406849659 \n",
      "Std of F1 scores:  0.018835155572474394 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Report Final Results ######\n",
    "\n",
    "# Report final results\n",
    "print(\"\\nTest results for\", FLAGS.model,\n",
    "      \"model on\", FLAGS.dataset, \"on\", FLAGS.task, \"\\n\",\n",
    "      \"___________________________________________________\\n\")\n",
    "\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    print(\"AP scores\\n\", mean_ap)\n",
    "    print(\"Mean AP score: \", np.mean(mean_ap),\n",
    "          \"\\nStd of AP scores: \", np.std(mean_ap), \"\\n \\n\")\n",
    "    \n",
    "    print(\"AUC scores\\n\", mean_roc)\n",
    "    print(\"Mean AUC score: \", np.mean(mean_roc),\n",
    "          \"\\nStd of AUC scores: \", np.std(mean_roc), \"\\n \\n\")\n",
    "    \n",
    "    print(\"ACC scores\\n\", mean_acc)\n",
    "    print(\"Mean ACC score: \", np.mean(mean_acc),\n",
    "          \"\\nStd of ACC scores: \", np.std(mean_acc), \"\\n \\n\")\n",
    "    \n",
    "    print(\"F1 scores\\n\", mean_f1)\n",
    "    print(\"Mean F1 score: \", np.mean(mean_f1),\n",
    "          \"\\nStd of F1 scores: \", np.std(mean_f1), \"\\n \\n\")\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    print(\"Details on degeneracy framework, with k =\", FLAGS.k, \": \\n \\n\")\n",
    "\n",
    "    print(\"Running times for k-core decomposition\\n\", mean_time_kcore)\n",
    "    print(\"Mean: \", np.mean(mean_time_kcore),\n",
    "          \"\\nStd: \", np.std(mean_time_kcore), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for autoencoder training\\n\", mean_time_train)\n",
    "    print(\"Mean: \", np.mean(mean_time_train),\n",
    "          \"\\nStd: \", np.std(mean_time_train), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for propagation\\n\", mean_time_expand)\n",
    "    print(\"Mean: \", np.mean(mean_time_expand),\n",
    "          \"\\nStd: \", np.std(mean_time_expand), \"\\n \\n\")\n",
    "\n",
    "    print(\"Sizes of k-core subgraph\\n\", mean_core_size)\n",
    "    print(\"Mean: \", np.mean(mean_core_size),\n",
    "          \"\\nStd: \", np.std(mean_core_size), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70400324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
