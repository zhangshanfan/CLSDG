{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c935237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:45:44.918694Z",
     "start_time": "2022-05-08T13:45:41.243914Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from linear_gae.evaluation import get_roc_score, clustering_latent_space\n",
    "from linear_gae.kcore import compute_kcore, expand_embedding\n",
    "from linear_gae.model import *\n",
    "from linear_gae.optimizer import OptimizerAE, OptimizerVAE\n",
    "from linear_gae.preprocessing import *\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384cb05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:45:46.594280Z",
     "start_time": "2022-05-08T13:45:46.573335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x2892d27db70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')   # 添加的，不报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19800c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:45:47.994078Z",
     "start_time": "2022-05-08T13:45:47.982113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Available Models:\\n\\n- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\\n          GCN encoder and inner product decoder\\n\\n- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\\n           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\\n           product decoder\\n\\n- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\\n             workshop paper, with linear encoder, and inner product decoder\\n\\n- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\\n              of NeurIPS workshop paper, with Gaussian priors, linear encoders\\n              for mu and sigma, and inner product decoder\\n \\n- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\\n               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\\n               product decoder\\n \\n- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\\n                in section 4 of NeurIPS workshop paper, with Gaussian priors,\\n                3-layer GCN encoders for mu and sigma, and inner product\\n                decoder\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select graph dataset\n",
    "flags.DEFINE_string('dataset', 'citeseer', 'Name of the graph dataset')\n",
    "\n",
    "# Select machine learning task to perform on graph\n",
    "flags.DEFINE_string('task', 'link_prediction', 'Name of the learning task')\n",
    "\n",
    "# Model\n",
    "flags.DEFINE_string('model', 'gcn_ae', 'Name of the model')\n",
    "''' Available Models:\n",
    "\n",
    "- gcn_ae: Graph Autoencoder from Kipf and Welling (2016), with 2-layer\n",
    "          GCN encoder and inner product decoder\n",
    "\n",
    "- gcn_vae: Graph Variational Autoencoder from Kipf and Welling (2016), with\n",
    "           Gaussian priors, 2-layer GCN encoders for mu and sigma, and inner\n",
    "           product decoder\n",
    "\n",
    "- linear_ae: Linear Graph Autoencoder, as introduced in section 3 of NeurIPS\n",
    "             workshop paper, with linear encoder, and inner product decoder\n",
    "\n",
    "- linear_vae: Linear Graph Variational Autoencoder, as introduced in section 3\n",
    "              of NeurIPS workshop paper, with Gaussian priors, linear encoders\n",
    "              for mu and sigma, and inner product decoder\n",
    " \n",
    "- deep_gcn_ae: Deeper version of Graph Autoencoder, as introduced in section 4\n",
    "               of NeurIPS workshop paper, with 3-layer GCN encoder, and inner\n",
    "               product decoder\n",
    " \n",
    "- deep_gcn_vae: Deeper version of Graph Variational Autoencoder, as introduced\n",
    "                in section 4 of NeurIPS workshop paper, with Gaussian priors,\n",
    "                3-layer GCN encoders for mu and sigma, and inner product\n",
    "                decoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eeddc89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:45:49.102566Z",
     "start_time": "2022-05-08T13:45:49.083612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "flags.DEFINE_float('dropout', 0., 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs in training.')\n",
    "flags.DEFINE_boolean('features', False, 'Include node features or not in encoder')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate (with Adam)')\n",
    "flags.DEFINE_integer('hidden', 32, 'Number of units in GCN hidden layer(s).')\n",
    "flags.DEFINE_integer('dimension', 16, 'Dimension of encoder output, i.e. \\\n",
    "                                       embedding dimension')\n",
    "\n",
    "# Experimental setup parameters\n",
    "flags.DEFINE_integer('nb_run', 10, 'Number of model run + test')\n",
    "flags.DEFINE_float('prop_val', 5., 'Proportion of edges in validation set \\\n",
    "                                   (for Link Prediction task)')\n",
    "flags.DEFINE_float('prop_test', 10., 'Proportion of edges in test set \\\n",
    "                                      (for Link Prediction task)')\n",
    "flags.DEFINE_boolean('validation', False, 'Whether to report validation \\\n",
    "                                           results at each epoch (for \\\n",
    "                                           Link Prediction task)')\n",
    "flags.DEFINE_boolean('verbose', True, 'Whether to print comments details.')\n",
    "\n",
    "# Parameters related to the \"degeneracy framework\" from IJCAI 2019 paper,\n",
    "# aiming at scaling-up Graph AE/VAE by training the model only on the k-core\n",
    "# (smaller) version of the graph, then expanding embedding to remaining nodes\n",
    "# via simpler and faster heuristics\n",
    "flags.DEFINE_boolean('kcore', False, 'Whether to run k-core decomposition \\\n",
    "                                      and use the framework. False = model \\\n",
    "                                      will be trained on the entire graph')\n",
    "flags.DEFINE_integer('k', 2, 'Which k-core to use. Higher k => smaller graphs\\\n",
    "                              and faster (but maybe less accurate) training')\n",
    "flags.DEFINE_integer('nb_iterations', 10, 'Number of fix point iterations in \\\n",
    "                                           algorithm 2 of IJCAI paper. See \\\n",
    "                                           kcore.py file for details')\n",
    "\n",
    "# Lists to collect average results\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    mean_roc = []\n",
    "    mean_ap = []\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    mean_time_kcore = []\n",
    "    mean_time_train = []\n",
    "    mean_time_expand = []\n",
    "    mean_core_size = []\n",
    "mean_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8eb1b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:45:50.292208Z",
     "start_time": "2022-05-08T13:45:50.286110Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filename_adj, nodes_numbers):\n",
    "    raw_edges = pd.read_csv(filename_adj, header=None)\n",
    "    \n",
    "    drop_self_loop = raw_edges[raw_edges[0] != raw_edges[1]]\n",
    "    \n",
    "    graph_np = np.zeros((nodes_numbers, nodes_numbers))\n",
    "    \n",
    "    for i in range(drop_self_loop.shape[0]):\n",
    "        graph_np[drop_self_loop.iloc[i,0], drop_self_loop.iloc[i,1]]=1\n",
    "        graph_np[drop_self_loop.iloc[i,1], drop_self_loop.iloc[i,0]]=1\n",
    "        \n",
    "    adj = nx.adjacency_matrix(nx.from_numpy_matrix(graph_np))\n",
    "    \n",
    "    features = sp.lil_matrix(np.eye(nodes_numbers))\n",
    "    \n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa41c5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:45:53.187109Z",
     "start_time": "2022-05-08T13:45:51.741450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load graph dataset\n",
    "if FLAGS.verbose:\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "adj_name = 'datasets/citeseer-edges.txt'\n",
    "nodes_number = 3327\n",
    "adj_init, features_init = load_data(adj_name, nodes_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386820fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:56:15.050911Z",
     "start_time": "2022-05-08T13:45:53.532509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\linear_graph_autoencoders-master\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84164 time= 0.41252\n",
      "Epoch: 0002 train_loss= 0.84051 time= 0.30361\n",
      "Epoch: 0003 train_loss= 0.83776 time= 0.28184\n",
      "Epoch: 0004 train_loss= 0.83254 time= 0.30246\n",
      "Epoch: 0005 train_loss= 0.82391 time= 0.29300\n",
      "Epoch: 0006 train_loss= 0.81105 time= 0.31743\n",
      "Epoch: 0007 train_loss= 0.79359 time= 0.31597\n",
      "Epoch: 0008 train_loss= 0.77234 time= 0.32023\n",
      "Epoch: 0009 train_loss= 0.75013 time= 0.29885\n",
      "Epoch: 0010 train_loss= 0.73273 time= 0.32687\n",
      "Epoch: 0011 train_loss= 0.72711 time= 0.30498\n",
      "Epoch: 0012 train_loss= 0.73065 time= 0.30410\n",
      "Epoch: 0013 train_loss= 0.73008 time= 0.29562\n",
      "Epoch: 0014 train_loss= 0.72152 time= 0.30520\n",
      "Epoch: 0015 train_loss= 0.70872 time= 0.30882\n",
      "Epoch: 0016 train_loss= 0.69594 time= 0.31665\n",
      "Epoch: 0017 train_loss= 0.68532 time= 0.29923\n",
      "Epoch: 0018 train_loss= 0.67685 time= 0.33009\n",
      "Epoch: 0019 train_loss= 0.66932 time= 0.30641\n",
      "Epoch: 0020 train_loss= 0.66136 time= 0.32682\n",
      "Epoch: 0021 train_loss= 0.65206 time= 0.32323\n",
      "Epoch: 0022 train_loss= 0.64112 time= 0.31757\n",
      "Epoch: 0023 train_loss= 0.62874 time= 0.30863\n",
      "Epoch: 0024 train_loss= 0.61551 time= 0.31140\n",
      "Epoch: 0025 train_loss= 0.60223 time= 0.30098\n",
      "Epoch: 0026 train_loss= 0.58977 time= 0.32713\n",
      "Epoch: 0027 train_loss= 0.57884 time= 0.30321\n",
      "Epoch: 0028 train_loss= 0.56981 time= 0.30440\n",
      "Epoch: 0029 train_loss= 0.56266 time= 0.30300\n",
      "Epoch: 0030 train_loss= 0.55694 time= 0.30079\n",
      "Epoch: 0031 train_loss= 0.55200 time= 0.30090\n",
      "Epoch: 0032 train_loss= 0.54723 time= 0.30313\n",
      "Epoch: 0033 train_loss= 0.54231 time= 0.32313\n",
      "Epoch: 0034 train_loss= 0.53718 time= 0.29935\n",
      "Epoch: 0035 train_loss= 0.53200 time= 0.30882\n",
      "Epoch: 0036 train_loss= 0.52703 time= 0.29428\n",
      "Epoch: 0037 train_loss= 0.52243 time= 0.31821\n",
      "Epoch: 0038 train_loss= 0.51830 time= 0.31901\n",
      "Epoch: 0039 train_loss= 0.51463 time= 0.31758\n",
      "Epoch: 0040 train_loss= 0.51133 time= 0.31187\n",
      "Epoch: 0041 train_loss= 0.50830 time= 0.31180\n",
      "Epoch: 0042 train_loss= 0.50546 time= 0.32246\n",
      "Epoch: 0043 train_loss= 0.50274 time= 0.29966\n",
      "Epoch: 0044 train_loss= 0.50013 time= 0.30990\n",
      "Epoch: 0045 train_loss= 0.49762 time= 0.30564\n",
      "Epoch: 0046 train_loss= 0.49522 time= 0.30595\n",
      "Epoch: 0047 train_loss= 0.49292 time= 0.32241\n",
      "Epoch: 0048 train_loss= 0.49073 time= 0.30589\n",
      "Epoch: 0049 train_loss= 0.48863 time= 0.31489\n",
      "Epoch: 0050 train_loss= 0.48663 time= 0.30138\n",
      "Epoch: 0051 train_loss= 0.48472 time= 0.30875\n",
      "Epoch: 0052 train_loss= 0.48291 time= 0.30111\n",
      "Epoch: 0053 train_loss= 0.48122 time= 0.31855\n",
      "Epoch: 0054 train_loss= 0.47964 time= 0.31972\n",
      "Epoch: 0055 train_loss= 0.47815 time= 0.32231\n",
      "Epoch: 0056 train_loss= 0.47674 time= 0.32307\n",
      "Epoch: 0057 train_loss= 0.47537 time= 0.28835\n",
      "Epoch: 0058 train_loss= 0.47401 time= 0.31392\n",
      "Epoch: 0059 train_loss= 0.47265 time= 0.32437\n",
      "Epoch: 0060 train_loss= 0.47131 time= 0.30304\n",
      "Epoch: 0061 train_loss= 0.46997 time= 0.30164\n",
      "Epoch: 0062 train_loss= 0.46866 time= 0.32821\n",
      "Epoch: 0063 train_loss= 0.46738 time= 0.31105\n",
      "Epoch: 0064 train_loss= 0.46614 time= 0.31919\n",
      "Epoch: 0065 train_loss= 0.46492 time= 0.31969\n",
      "Epoch: 0066 train_loss= 0.46375 time= 0.31940\n",
      "Epoch: 0067 train_loss= 0.46263 time= 0.32217\n",
      "Epoch: 0068 train_loss= 0.46155 time= 0.30797\n",
      "Epoch: 0069 train_loss= 0.46052 time= 0.31519\n",
      "Epoch: 0070 train_loss= 0.45953 time= 0.30797\n",
      "Epoch: 0071 train_loss= 0.45860 time= 0.33359\n",
      "Epoch: 0072 train_loss= 0.45771 time= 0.31559\n",
      "Epoch: 0073 train_loss= 0.45686 time= 0.32245\n",
      "Epoch: 0074 train_loss= 0.45606 time= 0.31805\n",
      "Epoch: 0075 train_loss= 0.45529 time= 0.31728\n",
      "Epoch: 0076 train_loss= 0.45456 time= 0.31108\n",
      "Epoch: 0077 train_loss= 0.45386 time= 0.31424\n",
      "Epoch: 0078 train_loss= 0.45319 time= 0.32201\n",
      "Epoch: 0079 train_loss= 0.45255 time= 0.31324\n",
      "Epoch: 0080 train_loss= 0.45193 time= 0.30286\n",
      "Epoch: 0081 train_loss= 0.45135 time= 0.31528\n",
      "Epoch: 0082 train_loss= 0.45078 time= 0.31065\n",
      "Epoch: 0083 train_loss= 0.45022 time= 0.31877\n",
      "Epoch: 0084 train_loss= 0.44968 time= 0.29940\n",
      "Epoch: 0085 train_loss= 0.44914 time= 0.32624\n",
      "Epoch: 0086 train_loss= 0.44861 time= 0.32463\n",
      "Epoch: 0087 train_loss= 0.44809 time= 0.32136\n",
      "Epoch: 0088 train_loss= 0.44757 time= 0.30782\n",
      "Epoch: 0089 train_loss= 0.44707 time= 0.31457\n",
      "Epoch: 0090 train_loss= 0.44659 time= 0.32164\n",
      "Epoch: 0091 train_loss= 0.44611 time= 0.33055\n",
      "Epoch: 0092 train_loss= 0.44565 time= 0.32347\n",
      "Epoch: 0093 train_loss= 0.44520 time= 0.32859\n",
      "Epoch: 0094 train_loss= 0.44476 time= 0.32152\n",
      "Epoch: 0095 train_loss= 0.44432 time= 0.32194\n",
      "Epoch: 0096 train_loss= 0.44390 time= 0.32239\n",
      "Epoch: 0097 train_loss= 0.44349 time= 0.32442\n",
      "Epoch: 0098 train_loss= 0.44308 time= 0.30459\n",
      "Epoch: 0099 train_loss= 0.44267 time= 0.31963\n",
      "Epoch: 0100 train_loss= 0.44227 time= 0.33468\n",
      "Epoch: 0101 train_loss= 0.44188 time= 0.29600\n",
      "Epoch: 0102 train_loss= 0.44148 time= 0.30519\n",
      "Epoch: 0103 train_loss= 0.44110 time= 0.34892\n",
      "Epoch: 0104 train_loss= 0.44071 time= 0.30785\n",
      "Epoch: 0105 train_loss= 0.44034 time= 0.32365\n",
      "Epoch: 0106 train_loss= 0.43996 time= 0.31374\n",
      "Epoch: 0107 train_loss= 0.43959 time= 0.31240\n",
      "Epoch: 0108 train_loss= 0.43922 time= 0.30841\n",
      "Epoch: 0109 train_loss= 0.43885 time= 0.29120\n",
      "Epoch: 0110 train_loss= 0.43848 time= 0.30767\n",
      "Epoch: 0111 train_loss= 0.43811 time= 0.31643\n",
      "Epoch: 0112 train_loss= 0.43774 time= 0.31347\n",
      "Epoch: 0113 train_loss= 0.43738 time= 0.30677\n",
      "Epoch: 0114 train_loss= 0.43702 time= 0.31858\n",
      "Epoch: 0115 train_loss= 0.43666 time= 0.30009\n",
      "Epoch: 0116 train_loss= 0.43630 time= 0.31180\n",
      "Epoch: 0117 train_loss= 0.43595 time= 0.31176\n",
      "Epoch: 0118 train_loss= 0.43561 time= 0.33814\n",
      "Epoch: 0119 train_loss= 0.43527 time= 0.30837\n",
      "Epoch: 0120 train_loss= 0.43494 time= 0.32872\n",
      "Epoch: 0121 train_loss= 0.43462 time= 0.34122\n",
      "Epoch: 0122 train_loss= 0.43431 time= 0.30796\n",
      "Epoch: 0123 train_loss= 0.43400 time= 0.32490\n",
      "Epoch: 0124 train_loss= 0.43370 time= 0.32293\n",
      "Epoch: 0125 train_loss= 0.43341 time= 0.30991\n",
      "Epoch: 0126 train_loss= 0.43312 time= 0.31430\n",
      "Epoch: 0127 train_loss= 0.43283 time= 0.31091\n",
      "Epoch: 0128 train_loss= 0.43254 time= 0.31771\n",
      "Epoch: 0129 train_loss= 0.43226 time= 0.32077\n",
      "Epoch: 0130 train_loss= 0.43198 time= 0.33498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0131 train_loss= 0.43170 time= 0.29955\n",
      "Epoch: 0132 train_loss= 0.43142 time= 0.32137\n",
      "Epoch: 0133 train_loss= 0.43115 time= 0.30522\n",
      "Epoch: 0134 train_loss= 0.43087 time= 0.30319\n",
      "Epoch: 0135 train_loss= 0.43060 time= 0.31147\n",
      "Epoch: 0136 train_loss= 0.43034 time= 0.29732\n",
      "Epoch: 0137 train_loss= 0.43008 time= 0.32929\n",
      "Epoch: 0138 train_loss= 0.42982 time= 0.29515\n",
      "Epoch: 0139 train_loss= 0.42956 time= 0.30342\n",
      "Epoch: 0140 train_loss= 0.42931 time= 0.30918\n",
      "Epoch: 0141 train_loss= 0.42907 time= 0.29889\n",
      "Epoch: 0142 train_loss= 0.42882 time= 0.31925\n",
      "Epoch: 0143 train_loss= 0.42858 time= 0.31414\n",
      "Epoch: 0144 train_loss= 0.42834 time= 0.31001\n",
      "Epoch: 0145 train_loss= 0.42810 time= 0.30149\n",
      "Epoch: 0146 train_loss= 0.42786 time= 0.29698\n",
      "Epoch: 0147 train_loss= 0.42762 time= 0.30541\n",
      "Epoch: 0148 train_loss= 0.42738 time= 0.30245\n",
      "Epoch: 0149 train_loss= 0.42714 time= 0.31264\n",
      "Epoch: 0150 train_loss= 0.42690 time= 0.33064\n",
      "Epoch: 0151 train_loss= 0.42666 time= 0.31077\n",
      "Epoch: 0152 train_loss= 0.42642 time= 0.31641\n",
      "Epoch: 0153 train_loss= 0.42618 time= 0.29609\n",
      "Epoch: 0154 train_loss= 0.42594 time= 0.30555\n",
      "Epoch: 0155 train_loss= 0.42569 time= 0.31001\n",
      "Epoch: 0156 train_loss= 0.42545 time= 0.31158\n",
      "Epoch: 0157 train_loss= 0.42521 time= 0.31043\n",
      "Epoch: 0158 train_loss= 0.42498 time= 0.30115\n",
      "Epoch: 0159 train_loss= 0.42474 time= 0.28236\n",
      "Epoch: 0160 train_loss= 0.42450 time= 0.32287\n",
      "Epoch: 0161 train_loss= 0.42427 time= 0.30885\n",
      "Epoch: 0162 train_loss= 0.42403 time= 0.29888\n",
      "Epoch: 0163 train_loss= 0.42379 time= 0.32752\n",
      "Epoch: 0164 train_loss= 0.42356 time= 0.30501\n",
      "Epoch: 0165 train_loss= 0.42332 time= 0.30790\n",
      "Epoch: 0166 train_loss= 0.42309 time= 0.30990\n",
      "Epoch: 0167 train_loss= 0.42286 time= 0.30539\n",
      "Epoch: 0168 train_loss= 0.42262 time= 0.31410\n",
      "Epoch: 0169 train_loss= 0.42239 time= 0.30062\n",
      "Epoch: 0170 train_loss= 0.42215 time= 0.31005\n",
      "Epoch: 0171 train_loss= 0.42192 time= 0.31075\n",
      "Epoch: 0172 train_loss= 0.42169 time= 0.29547\n",
      "Epoch: 0173 train_loss= 0.42146 time= 0.29614\n",
      "Epoch: 0174 train_loss= 0.42123 time= 0.29799\n",
      "Epoch: 0175 train_loss= 0.42100 time= 0.32747\n",
      "Epoch: 0176 train_loss= 0.42077 time= 0.29409\n",
      "Epoch: 0177 train_loss= 0.42055 time= 0.31399\n",
      "Epoch: 0178 train_loss= 0.42033 time= 0.29515\n",
      "Epoch: 0179 train_loss= 0.42011 time= 0.30606\n",
      "Epoch: 0180 train_loss= 0.41989 time= 0.30906\n",
      "Epoch: 0181 train_loss= 0.41968 time= 0.28807\n",
      "Epoch: 0182 train_loss= 0.41947 time= 0.31000\n",
      "Epoch: 0183 train_loss= 0.41926 time= 0.30737\n",
      "Epoch: 0184 train_loss= 0.41906 time= 0.29945\n",
      "Epoch: 0185 train_loss= 0.41886 time= 0.30716\n",
      "Epoch: 0186 train_loss= 0.41866 time= 0.29889\n",
      "Epoch: 0187 train_loss= 0.41847 time= 0.32384\n",
      "Epoch: 0188 train_loss= 0.41828 time= 0.30050\n",
      "Epoch: 0189 train_loss= 0.41809 time= 0.30847\n",
      "Epoch: 0190 train_loss= 0.41791 time= 0.31650\n",
      "Epoch: 0191 train_loss= 0.41773 time= 0.30499\n",
      "Epoch: 0192 train_loss= 0.41755 time= 0.31349\n",
      "Epoch: 0193 train_loss= 0.41738 time= 0.30578\n",
      "Epoch: 0194 train_loss= 0.41720 time= 0.31380\n",
      "Epoch: 0195 train_loss= 0.41703 time= 0.32524\n",
      "Epoch: 0196 train_loss= 0.41687 time= 0.31952\n",
      "Epoch: 0197 train_loss= 0.41670 time= 0.31472\n",
      "Epoch: 0198 train_loss= 0.41654 time= 0.31066\n",
      "Epoch: 0199 train_loss= 0.41639 time= 0.30452\n",
      "Epoch: 0200 train_loss= 0.41623 time= 0.30145\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84170 time= 0.40509\n",
      "Epoch: 0002 train_loss= 0.84087 time= 0.28661\n",
      "Epoch: 0003 train_loss= 0.83897 time= 0.29122\n",
      "Epoch: 0004 train_loss= 0.83543 time= 0.28895\n",
      "Epoch: 0005 train_loss= 0.82956 time= 0.30346\n",
      "Epoch: 0006 train_loss= 0.82063 time= 0.31795\n",
      "Epoch: 0007 train_loss= 0.80799 time= 0.31616\n",
      "Epoch: 0008 train_loss= 0.79129 time= 0.30830\n",
      "Epoch: 0009 train_loss= 0.77100 time= 0.30726\n",
      "Epoch: 0010 train_loss= 0.74895 time= 0.31015\n",
      "Epoch: 0011 train_loss= 0.72907 time= 0.31261\n",
      "Epoch: 0012 train_loss= 0.71690 time= 0.31617\n",
      "Epoch: 0013 train_loss= 0.71434 time= 0.31617\n",
      "Epoch: 0014 train_loss= 0.71312 time= 0.31058\n",
      "Epoch: 0015 train_loss= 0.70551 time= 0.30018\n",
      "Epoch: 0016 train_loss= 0.69165 time= 0.30608\n",
      "Epoch: 0017 train_loss= 0.67499 time= 0.28816\n",
      "Epoch: 0018 train_loss= 0.65854 time= 0.31148\n",
      "Epoch: 0019 train_loss= 0.64374 time= 0.29631\n",
      "Epoch: 0020 train_loss= 0.63054 time= 0.30267\n",
      "Epoch: 0021 train_loss= 0.61817 time= 0.29655\n",
      "Epoch: 0022 train_loss= 0.60589 time= 0.29710\n",
      "Epoch: 0023 train_loss= 0.59344 time= 0.30579\n",
      "Epoch: 0024 train_loss= 0.58105 time= 0.30080\n",
      "Epoch: 0025 train_loss= 0.56928 time= 0.30940\n",
      "Epoch: 0026 train_loss= 0.55878 time= 0.29094\n",
      "Epoch: 0027 train_loss= 0.55005 time= 0.30244\n",
      "Epoch: 0028 train_loss= 0.54320 time= 0.29942\n",
      "Epoch: 0029 train_loss= 0.53792 time= 0.30806\n",
      "Epoch: 0030 train_loss= 0.53342 time= 0.29772\n",
      "Epoch: 0031 train_loss= 0.52881 time= 0.29439\n",
      "Epoch: 0032 train_loss= 0.52351 time= 0.30590\n",
      "Epoch: 0033 train_loss= 0.51749 time= 0.30061\n",
      "Epoch: 0034 train_loss= 0.51118 time= 0.29792\n",
      "Epoch: 0035 train_loss= 0.50519 time= 0.30894\n",
      "Epoch: 0036 train_loss= 0.50008 time= 0.29920\n",
      "Epoch: 0037 train_loss= 0.49610 time= 0.30285\n",
      "Epoch: 0038 train_loss= 0.49326 time= 0.30287\n",
      "Epoch: 0039 train_loss= 0.49132 time= 0.29417\n",
      "Epoch: 0040 train_loss= 0.48996 time= 0.30520\n",
      "Epoch: 0041 train_loss= 0.48890 time= 0.30121\n",
      "Epoch: 0042 train_loss= 0.48791 time= 0.29407\n",
      "Epoch: 0043 train_loss= 0.48687 time= 0.30419\n",
      "Epoch: 0044 train_loss= 0.48570 time= 0.30461\n",
      "Epoch: 0045 train_loss= 0.48435 time= 0.30939\n",
      "Epoch: 0046 train_loss= 0.48281 time= 0.30642\n",
      "Epoch: 0047 train_loss= 0.48109 time= 0.30315\n",
      "Epoch: 0048 train_loss= 0.47924 time= 0.31270\n",
      "Epoch: 0049 train_loss= 0.47732 time= 0.30707\n",
      "Epoch: 0050 train_loss= 0.47542 time= 0.29664\n",
      "Epoch: 0051 train_loss= 0.47359 time= 0.30816\n",
      "Epoch: 0052 train_loss= 0.47190 time= 0.31331\n",
      "Epoch: 0053 train_loss= 0.47035 time= 0.30479\n",
      "Epoch: 0054 train_loss= 0.46896 time= 0.31929\n",
      "Epoch: 0055 train_loss= 0.46770 time= 0.30049\n",
      "Epoch: 0056 train_loss= 0.46657 time= 0.30849\n",
      "Epoch: 0057 train_loss= 0.46555 time= 0.31284\n",
      "Epoch: 0058 train_loss= 0.46462 time= 0.31093\n",
      "Epoch: 0059 train_loss= 0.46376 time= 0.31263\n",
      "Epoch: 0060 train_loss= 0.46294 time= 0.30991\n",
      "Epoch: 0061 train_loss= 0.46213 time= 0.29899\n",
      "Epoch: 0062 train_loss= 0.46130 time= 0.31295\n",
      "Epoch: 0063 train_loss= 0.46044 time= 0.32527\n",
      "Epoch: 0064 train_loss= 0.45953 time= 0.30613\n",
      "Epoch: 0065 train_loss= 0.45858 time= 0.30253\n",
      "Epoch: 0066 train_loss= 0.45759 time= 0.31094\n",
      "Epoch: 0067 train_loss= 0.45660 time= 0.30897\n",
      "Epoch: 0068 train_loss= 0.45560 time= 0.30008\n",
      "Epoch: 0069 train_loss= 0.45461 time= 0.31181\n",
      "Epoch: 0070 train_loss= 0.45365 time= 0.31291\n",
      "Epoch: 0071 train_loss= 0.45270 time= 0.30068\n",
      "Epoch: 0072 train_loss= 0.45177 time= 0.31576\n",
      "Epoch: 0073 train_loss= 0.45087 time= 0.30775\n",
      "Epoch: 0074 train_loss= 0.44998 time= 0.30355\n",
      "Epoch: 0075 train_loss= 0.44912 time= 0.30529\n",
      "Epoch: 0076 train_loss= 0.44828 time= 0.30821\n",
      "Epoch: 0077 train_loss= 0.44746 time= 0.31415\n",
      "Epoch: 0078 train_loss= 0.44667 time= 0.31037\n",
      "Epoch: 0079 train_loss= 0.44589 time= 0.30550\n",
      "Epoch: 0080 train_loss= 0.44514 time= 0.30895\n",
      "Epoch: 0081 train_loss= 0.44440 time= 0.30328\n",
      "Epoch: 0082 train_loss= 0.44367 time= 0.30964\n",
      "Epoch: 0083 train_loss= 0.44297 time= 0.31584\n",
      "Epoch: 0084 train_loss= 0.44228 time= 0.31053\n",
      "Epoch: 0085 train_loss= 0.44161 time= 0.31158\n",
      "Epoch: 0086 train_loss= 0.44096 time= 0.29946\n",
      "Epoch: 0087 train_loss= 0.44032 time= 0.30613\n",
      "Epoch: 0088 train_loss= 0.43970 time= 0.30746\n",
      "Epoch: 0089 train_loss= 0.43910 time= 0.30334\n",
      "Epoch: 0090 train_loss= 0.43851 time= 0.30901\n",
      "Epoch: 0091 train_loss= 0.43795 time= 0.30639\n",
      "Epoch: 0092 train_loss= 0.43741 time= 0.30785\n",
      "Epoch: 0093 train_loss= 0.43689 time= 0.29803\n",
      "Epoch: 0094 train_loss= 0.43639 time= 0.31640\n",
      "Epoch: 0095 train_loss= 0.43592 time= 0.31744\n",
      "Epoch: 0096 train_loss= 0.43547 time= 0.31882\n",
      "Epoch: 0097 train_loss= 0.43505 time= 0.31415\n",
      "Epoch: 0098 train_loss= 0.43464 time= 0.31319\n",
      "Epoch: 0099 train_loss= 0.43426 time= 0.30578\n",
      "Epoch: 0100 train_loss= 0.43390 time= 0.31185\n",
      "Epoch: 0101 train_loss= 0.43356 time= 0.30171\n",
      "Epoch: 0102 train_loss= 0.43323 time= 0.31839\n",
      "Epoch: 0103 train_loss= 0.43291 time= 0.29677\n",
      "Epoch: 0104 train_loss= 0.43260 time= 0.30195\n",
      "Epoch: 0105 train_loss= 0.43231 time= 0.29217\n",
      "Epoch: 0106 train_loss= 0.43201 time= 0.30084\n",
      "Epoch: 0107 train_loss= 0.43172 time= 0.30831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0108 train_loss= 0.43143 time= 0.30664\n",
      "Epoch: 0109 train_loss= 0.43115 time= 0.29714\n",
      "Epoch: 0110 train_loss= 0.43087 time= 0.30167\n",
      "Epoch: 0111 train_loss= 0.43059 time= 0.31553\n",
      "Epoch: 0112 train_loss= 0.43032 time= 0.29938\n",
      "Epoch: 0113 train_loss= 0.43004 time= 0.30002\n",
      "Epoch: 0114 train_loss= 0.42977 time= 0.31414\n",
      "Epoch: 0115 train_loss= 0.42951 time= 0.30020\n",
      "Epoch: 0116 train_loss= 0.42924 time= 0.29163\n",
      "Epoch: 0117 train_loss= 0.42898 time= 0.31836\n",
      "Epoch: 0118 train_loss= 0.42871 time= 0.29427\n",
      "Epoch: 0119 train_loss= 0.42845 time= 0.29726\n",
      "Epoch: 0120 train_loss= 0.42819 time= 0.30446\n",
      "Epoch: 0121 train_loss= 0.42793 time= 0.30019\n",
      "Epoch: 0122 train_loss= 0.42767 time= 0.30745\n",
      "Epoch: 0123 train_loss= 0.42741 time= 0.31356\n",
      "Epoch: 0124 train_loss= 0.42714 time= 0.30373\n",
      "Epoch: 0125 train_loss= 0.42688 time= 0.29699\n",
      "Epoch: 0126 train_loss= 0.42662 time= 0.31226\n",
      "Epoch: 0127 train_loss= 0.42635 time= 0.29421\n",
      "Epoch: 0128 train_loss= 0.42608 time= 0.30319\n",
      "Epoch: 0129 train_loss= 0.42581 time= 0.30254\n",
      "Epoch: 0130 train_loss= 0.42554 time= 0.30946\n",
      "Epoch: 0131 train_loss= 0.42526 time= 0.30407\n",
      "Epoch: 0132 train_loss= 0.42498 time= 0.30190\n",
      "Epoch: 0133 train_loss= 0.42469 time= 0.29997\n",
      "Epoch: 0134 train_loss= 0.42440 time= 0.30262\n",
      "Epoch: 0135 train_loss= 0.42411 time= 0.31003\n",
      "Epoch: 0136 train_loss= 0.42382 time= 0.30021\n",
      "Epoch: 0137 train_loss= 0.42352 time= 0.30190\n",
      "Epoch: 0138 train_loss= 0.42322 time= 0.29252\n",
      "Epoch: 0139 train_loss= 0.42292 time= 0.31932\n",
      "Epoch: 0140 train_loss= 0.42262 time= 0.30717\n",
      "Epoch: 0141 train_loss= 0.42232 time= 0.31017\n",
      "Epoch: 0142 train_loss= 0.42202 time= 0.30710\n",
      "Epoch: 0143 train_loss= 0.42172 time= 0.30225\n",
      "Epoch: 0144 train_loss= 0.42142 time= 0.30482\n",
      "Epoch: 0145 train_loss= 0.42112 time= 0.28812\n",
      "Epoch: 0146 train_loss= 0.42083 time= 0.30119\n",
      "Epoch: 0147 train_loss= 0.42054 time= 0.30917\n",
      "Epoch: 0148 train_loss= 0.42025 time= 0.29521\n",
      "Epoch: 0149 train_loss= 0.41996 time= 0.29753\n",
      "Epoch: 0150 train_loss= 0.41968 time= 0.31097\n",
      "Epoch: 0151 train_loss= 0.41941 time= 0.29642\n",
      "Epoch: 0152 train_loss= 0.41913 time= 0.31062\n",
      "Epoch: 0153 train_loss= 0.41886 time= 0.29480\n",
      "Epoch: 0154 train_loss= 0.41859 time= 0.30917\n",
      "Epoch: 0155 train_loss= 0.41833 time= 0.30419\n",
      "Epoch: 0156 train_loss= 0.41807 time= 0.29471\n",
      "Epoch: 0157 train_loss= 0.41781 time= 0.30685\n",
      "Epoch: 0158 train_loss= 0.41756 time= 0.29663\n",
      "Epoch: 0159 train_loss= 0.41732 time= 0.30103\n",
      "Epoch: 0160 train_loss= 0.41708 time= 0.30020\n",
      "Epoch: 0161 train_loss= 0.41685 time= 0.31416\n",
      "Epoch: 0162 train_loss= 0.41663 time= 0.29261\n",
      "Epoch: 0163 train_loss= 0.41641 time= 0.30330\n",
      "Epoch: 0164 train_loss= 0.41621 time= 0.30687\n",
      "Epoch: 0165 train_loss= 0.41601 time= 0.29799\n",
      "Epoch: 0166 train_loss= 0.41582 time= 0.30552\n",
      "Epoch: 0167 train_loss= 0.41564 time= 0.29739\n",
      "Epoch: 0168 train_loss= 0.41547 time= 0.29720\n",
      "Epoch: 0169 train_loss= 0.41530 time= 0.30470\n",
      "Epoch: 0170 train_loss= 0.41514 time= 0.30892\n",
      "Epoch: 0171 train_loss= 0.41499 time= 0.29653\n",
      "Epoch: 0172 train_loss= 0.41484 time= 0.30470\n",
      "Epoch: 0173 train_loss= 0.41470 time= 0.29820\n",
      "Epoch: 0174 train_loss= 0.41456 time= 0.30201\n",
      "Epoch: 0175 train_loss= 0.41442 time= 0.29621\n",
      "Epoch: 0176 train_loss= 0.41429 time= 0.31092\n",
      "Epoch: 0177 train_loss= 0.41416 time= 0.30243\n",
      "Epoch: 0178 train_loss= 0.41403 time= 0.30242\n",
      "Epoch: 0179 train_loss= 0.41391 time= 0.32432\n",
      "Epoch: 0180 train_loss= 0.41379 time= 0.30637\n",
      "Epoch: 0181 train_loss= 0.41367 time= 0.31032\n",
      "Epoch: 0182 train_loss= 0.41356 time= 0.30501\n",
      "Epoch: 0183 train_loss= 0.41345 time= 0.31267\n",
      "Epoch: 0184 train_loss= 0.41334 time= 0.29644\n",
      "Epoch: 0185 train_loss= 0.41323 time= 0.30961\n",
      "Epoch: 0186 train_loss= 0.41312 time= 0.30319\n",
      "Epoch: 0187 train_loss= 0.41301 time= 0.30818\n",
      "Epoch: 0188 train_loss= 0.41291 time= 0.30718\n",
      "Epoch: 0189 train_loss= 0.41281 time= 0.30806\n",
      "Epoch: 0190 train_loss= 0.41270 time= 0.31037\n",
      "Epoch: 0191 train_loss= 0.41260 time= 0.30278\n",
      "Epoch: 0192 train_loss= 0.41249 time= 0.30522\n",
      "Epoch: 0193 train_loss= 0.41239 time= 0.29023\n",
      "Epoch: 0194 train_loss= 0.41229 time= 0.30917\n",
      "Epoch: 0195 train_loss= 0.41218 time= 0.31366\n",
      "Epoch: 0196 train_loss= 0.41207 time= 0.30023\n",
      "Epoch: 0197 train_loss= 0.41197 time= 0.29975\n",
      "Epoch: 0198 train_loss= 0.41186 time= 0.29588\n",
      "Epoch: 0199 train_loss= 0.41175 time= 0.29922\n",
      "Epoch: 0200 train_loss= 0.41163 time= 0.30618\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84170 time= 0.41628\n",
      "Epoch: 0002 train_loss= 0.84091 time= 0.28580\n",
      "Epoch: 0003 train_loss= 0.83909 time= 0.30957\n",
      "Epoch: 0004 train_loss= 0.83572 time= 0.33047\n",
      "Epoch: 0005 train_loss= 0.83012 time= 0.30344\n",
      "Epoch: 0006 train_loss= 0.82154 time= 0.32925\n",
      "Epoch: 0007 train_loss= 0.80923 time= 0.32666\n",
      "Epoch: 0008 train_loss= 0.79276 time= 0.32830\n",
      "Epoch: 0009 train_loss= 0.77250 time= 0.31923\n",
      "Epoch: 0010 train_loss= 0.75036 time= 0.31408\n",
      "Epoch: 0011 train_loss= 0.73060 time= 0.32513\n",
      "Epoch: 0012 train_loss= 0.71949 time= 0.31666\n",
      "Epoch: 0013 train_loss= 0.71897 time= 0.32521\n",
      "Epoch: 0014 train_loss= 0.71911 time= 0.30975\n",
      "Epoch: 0015 train_loss= 0.71192 time= 0.29523\n",
      "Epoch: 0016 train_loss= 0.69854 time= 0.31216\n",
      "Epoch: 0017 train_loss= 0.68305 time= 0.30419\n",
      "Epoch: 0018 train_loss= 0.66857 time= 0.27626\n",
      "Epoch: 0019 train_loss= 0.65615 time= 0.30096\n",
      "Epoch: 0020 train_loss= 0.64518 time= 0.30589\n",
      "Epoch: 0021 train_loss= 0.63441 time= 0.28260\n",
      "Epoch: 0022 train_loss= 0.62279 time= 0.28260\n",
      "Epoch: 0023 train_loss= 0.60981 time= 0.28051\n",
      "Epoch: 0024 train_loss= 0.59550 time= 0.27899\n",
      "Epoch: 0025 train_loss= 0.58035 time= 0.28929\n",
      "Epoch: 0026 train_loss= 0.56510 time= 0.27276\n",
      "Epoch: 0027 train_loss= 0.55062 time= 0.27889\n",
      "Epoch: 0028 train_loss= 0.53772 time= 0.28422\n",
      "Epoch: 0029 train_loss= 0.52699 time= 0.28433\n",
      "Epoch: 0030 train_loss= 0.51865 time= 0.28723\n",
      "Epoch: 0031 train_loss= 0.51254 time= 0.28523\n",
      "Epoch: 0032 train_loss= 0.50821 time= 0.27036\n",
      "Epoch: 0033 train_loss= 0.50509 time= 0.27197\n",
      "Epoch: 0034 train_loss= 0.50273 time= 0.27420\n",
      "Epoch: 0035 train_loss= 0.50084 time= 0.27953\n",
      "Epoch: 0036 train_loss= 0.49922 time= 0.28340\n",
      "Epoch: 0037 train_loss= 0.49774 time= 0.28723\n",
      "Epoch: 0038 train_loss= 0.49621 time= 0.29521\n",
      "Epoch: 0039 train_loss= 0.49448 time= 0.29621\n",
      "Epoch: 0040 train_loss= 0.49242 time= 0.30688\n",
      "Epoch: 0041 train_loss= 0.49002 time= 0.28383\n",
      "Epoch: 0042 train_loss= 0.48738 time= 0.27456\n",
      "Epoch: 0043 train_loss= 0.48469 time= 0.29324\n",
      "Epoch: 0044 train_loss= 0.48212 time= 0.29421\n",
      "Epoch: 0045 train_loss= 0.47984 time= 0.28224\n",
      "Epoch: 0046 train_loss= 0.47789 time= 0.28245\n",
      "Epoch: 0047 train_loss= 0.47630 time= 0.29938\n",
      "Epoch: 0048 train_loss= 0.47500 time= 0.28347\n",
      "Epoch: 0049 train_loss= 0.47394 time= 0.27622\n",
      "Epoch: 0050 train_loss= 0.47303 time= 0.28653\n",
      "Epoch: 0051 train_loss= 0.47221 time= 0.28434\n",
      "Epoch: 0052 train_loss= 0.47140 time= 0.28129\n",
      "Epoch: 0053 train_loss= 0.47057 time= 0.27356\n",
      "Epoch: 0054 train_loss= 0.46968 time= 0.28490\n",
      "Epoch: 0055 train_loss= 0.46871 time= 0.29084\n",
      "Epoch: 0056 train_loss= 0.46766 time= 0.28581\n",
      "Epoch: 0057 train_loss= 0.46655 time= 0.28821\n",
      "Epoch: 0058 train_loss= 0.46541 time= 0.29588\n",
      "Epoch: 0059 train_loss= 0.46429 time= 0.28009\n",
      "Epoch: 0060 train_loss= 0.46323 time= 0.28186\n",
      "Epoch: 0061 train_loss= 0.46224 time= 0.29591\n",
      "Epoch: 0062 train_loss= 0.46135 time= 0.28438\n",
      "Epoch: 0063 train_loss= 0.46055 time= 0.27376\n",
      "Epoch: 0064 train_loss= 0.45981 time= 0.27370\n",
      "Epoch: 0065 train_loss= 0.45912 time= 0.28482\n",
      "Epoch: 0066 train_loss= 0.45844 time= 0.30715\n",
      "Epoch: 0067 train_loss= 0.45777 time= 0.29874\n",
      "Epoch: 0068 train_loss= 0.45710 time= 0.29295\n",
      "Epoch: 0069 train_loss= 0.45640 time= 0.28709\n",
      "Epoch: 0070 train_loss= 0.45569 time= 0.28662\n",
      "Epoch: 0071 train_loss= 0.45497 time= 0.30061\n",
      "Epoch: 0072 train_loss= 0.45424 time= 0.28068\n",
      "Epoch: 0073 train_loss= 0.45350 time= 0.29248\n",
      "Epoch: 0074 train_loss= 0.45276 time= 0.29642\n",
      "Epoch: 0075 train_loss= 0.45203 time= 0.30285\n",
      "Epoch: 0076 train_loss= 0.45131 time= 0.28712\n",
      "Epoch: 0077 train_loss= 0.45061 time= 0.28422\n",
      "Epoch: 0078 train_loss= 0.44992 time= 0.28658\n",
      "Epoch: 0079 train_loss= 0.44925 time= 0.28731\n",
      "Epoch: 0080 train_loss= 0.44858 time= 0.28251\n",
      "Epoch: 0081 train_loss= 0.44791 time= 0.28702\n",
      "Epoch: 0082 train_loss= 0.44725 time= 0.28209\n",
      "Epoch: 0083 train_loss= 0.44659 time= 0.28403\n",
      "Epoch: 0084 train_loss= 0.44594 time= 0.26470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0085 train_loss= 0.44528 time= 0.27390\n",
      "Epoch: 0086 train_loss= 0.44463 time= 0.27567\n",
      "Epoch: 0087 train_loss= 0.44399 time= 0.27738\n",
      "Epoch: 0088 train_loss= 0.44336 time= 0.28096\n",
      "Epoch: 0089 train_loss= 0.44275 time= 0.27305\n",
      "Epoch: 0090 train_loss= 0.44215 time= 0.29113\n",
      "Epoch: 0091 train_loss= 0.44159 time= 0.26868\n",
      "Epoch: 0092 train_loss= 0.44104 time= 0.30523\n",
      "Epoch: 0093 train_loss= 0.44053 time= 0.29944\n",
      "Epoch: 0094 train_loss= 0.44003 time= 0.30230\n",
      "Epoch: 0095 train_loss= 0.43955 time= 0.27794\n",
      "Epoch: 0096 train_loss= 0.43909 time= 0.28488\n",
      "Epoch: 0097 train_loss= 0.43865 time= 0.27534\n",
      "Epoch: 0098 train_loss= 0.43821 time= 0.29656\n",
      "Epoch: 0099 train_loss= 0.43778 time= 0.29326\n",
      "Epoch: 0100 train_loss= 0.43736 time= 0.27825\n",
      "Epoch: 0101 train_loss= 0.43694 time= 0.28991\n",
      "Epoch: 0102 train_loss= 0.43652 time= 0.27562\n",
      "Epoch: 0103 train_loss= 0.43611 time= 0.27589\n",
      "Epoch: 0104 train_loss= 0.43570 time= 0.29829\n",
      "Epoch: 0105 train_loss= 0.43530 time= 0.29613\n",
      "Epoch: 0106 train_loss= 0.43490 time= 0.27726\n",
      "Epoch: 0107 train_loss= 0.43451 time= 0.28424\n",
      "Epoch: 0108 train_loss= 0.43413 time= 0.28940\n",
      "Epoch: 0109 train_loss= 0.43376 time= 0.27613\n",
      "Epoch: 0110 train_loss= 0.43339 time= 0.28324\n",
      "Epoch: 0111 train_loss= 0.43304 time= 0.28093\n",
      "Epoch: 0112 train_loss= 0.43270 time= 0.28138\n",
      "Epoch: 0113 train_loss= 0.43236 time= 0.28723\n",
      "Epoch: 0114 train_loss= 0.43204 time= 0.26729\n",
      "Epoch: 0115 train_loss= 0.43172 time= 0.26900\n",
      "Epoch: 0116 train_loss= 0.43141 time= 0.28754\n",
      "Epoch: 0117 train_loss= 0.43110 time= 0.30224\n",
      "Epoch: 0118 train_loss= 0.43080 time= 0.28590\n",
      "Epoch: 0119 train_loss= 0.43050 time= 0.28658\n",
      "Epoch: 0120 train_loss= 0.43021 time= 0.28571\n",
      "Epoch: 0121 train_loss= 0.42993 time= 0.28324\n",
      "Epoch: 0122 train_loss= 0.42965 time= 0.28913\n",
      "Epoch: 0123 train_loss= 0.42937 time= 0.28493\n",
      "Epoch: 0124 train_loss= 0.42910 time= 0.28463\n",
      "Epoch: 0125 train_loss= 0.42883 time= 0.28497\n",
      "Epoch: 0126 train_loss= 0.42857 time= 0.27563\n",
      "Epoch: 0127 train_loss= 0.42831 time= 0.28564\n",
      "Epoch: 0128 train_loss= 0.42805 time= 0.27834\n",
      "Epoch: 0129 train_loss= 0.42780 time= 0.29597\n",
      "Epoch: 0130 train_loss= 0.42756 time= 0.29353\n",
      "Epoch: 0131 train_loss= 0.42731 time= 0.29974\n",
      "Epoch: 0132 train_loss= 0.42707 time= 0.32533\n",
      "Epoch: 0133 train_loss= 0.42683 time= 0.31209\n",
      "Epoch: 0134 train_loss= 0.42659 time= 0.30031\n",
      "Epoch: 0135 train_loss= 0.42635 time= 0.30064\n",
      "Epoch: 0136 train_loss= 0.42610 time= 0.32006\n",
      "Epoch: 0137 train_loss= 0.42586 time= 0.31740\n",
      "Epoch: 0138 train_loss= 0.42562 time= 0.29164\n",
      "Epoch: 0139 train_loss= 0.42538 time= 0.30892\n",
      "Epoch: 0140 train_loss= 0.42514 time= 0.30023\n",
      "Epoch: 0141 train_loss= 0.42489 time= 0.31108\n",
      "Epoch: 0142 train_loss= 0.42465 time= 0.32128\n",
      "Epoch: 0143 train_loss= 0.42441 time= 0.31728\n",
      "Epoch: 0144 train_loss= 0.42417 time= 0.30326\n",
      "Epoch: 0145 train_loss= 0.42393 time= 0.29993\n",
      "Epoch: 0146 train_loss= 0.42369 time= 0.30988\n",
      "Epoch: 0147 train_loss= 0.42345 time= 0.30109\n",
      "Epoch: 0148 train_loss= 0.42321 time= 0.33318\n",
      "Epoch: 0149 train_loss= 0.42298 time= 0.30770\n",
      "Epoch: 0150 train_loss= 0.42275 time= 0.32764\n",
      "Epoch: 0151 train_loss= 0.42252 time= 0.29271\n",
      "Epoch: 0152 train_loss= 0.42229 time= 0.31772\n",
      "Epoch: 0153 train_loss= 0.42207 time= 0.29965\n",
      "Epoch: 0154 train_loss= 0.42185 time= 0.31720\n",
      "Epoch: 0155 train_loss= 0.42163 time= 0.29812\n",
      "Epoch: 0156 train_loss= 0.42141 time= 0.31160\n",
      "Epoch: 0157 train_loss= 0.42120 time= 0.31674\n",
      "Epoch: 0158 train_loss= 0.42099 time= 0.30053\n",
      "Epoch: 0159 train_loss= 0.42079 time= 0.30915\n",
      "Epoch: 0160 train_loss= 0.42059 time= 0.30818\n",
      "Epoch: 0161 train_loss= 0.42039 time= 0.30129\n",
      "Epoch: 0162 train_loss= 0.42020 time= 0.32234\n",
      "Epoch: 0163 train_loss= 0.42002 time= 0.31269\n",
      "Epoch: 0164 train_loss= 0.41983 time= 0.32107\n",
      "Epoch: 0165 train_loss= 0.41966 time= 0.31705\n",
      "Epoch: 0166 train_loss= 0.41949 time= 0.28900\n",
      "Epoch: 0167 train_loss= 0.41932 time= 0.29577\n",
      "Epoch: 0168 train_loss= 0.41916 time= 0.31841\n",
      "Epoch: 0169 train_loss= 0.41901 time= 0.29781\n",
      "Epoch: 0170 train_loss= 0.41886 time= 0.32308\n",
      "Epoch: 0171 train_loss= 0.41872 time= 0.30812\n",
      "Epoch: 0172 train_loss= 0.41859 time= 0.31043\n",
      "Epoch: 0173 train_loss= 0.41845 time= 0.29131\n",
      "Epoch: 0174 train_loss= 0.41833 time= 0.30519\n",
      "Epoch: 0175 train_loss= 0.41821 time= 0.32301\n",
      "Epoch: 0176 train_loss= 0.41809 time= 0.33053\n",
      "Epoch: 0177 train_loss= 0.41798 time= 0.30774\n",
      "Epoch: 0178 train_loss= 0.41787 time= 0.29972\n",
      "Epoch: 0179 train_loss= 0.41776 time= 0.32634\n",
      "Epoch: 0180 train_loss= 0.41766 time= 0.29936\n",
      "Epoch: 0181 train_loss= 0.41756 time= 0.31789\n",
      "Epoch: 0182 train_loss= 0.41747 time= 0.29724\n",
      "Epoch: 0183 train_loss= 0.41737 time= 0.30454\n",
      "Epoch: 0184 train_loss= 0.41728 time= 0.30037\n",
      "Epoch: 0185 train_loss= 0.41719 time= 0.31539\n",
      "Epoch: 0186 train_loss= 0.41710 time= 0.30818\n",
      "Epoch: 0187 train_loss= 0.41700 time= 0.31410\n",
      "Epoch: 0188 train_loss= 0.41691 time= 0.31732\n",
      "Epoch: 0189 train_loss= 0.41682 time= 0.32457\n",
      "Epoch: 0190 train_loss= 0.41673 time= 0.31042\n",
      "Epoch: 0191 train_loss= 0.41664 time= 0.31754\n",
      "Epoch: 0192 train_loss= 0.41654 time= 0.30253\n",
      "Epoch: 0193 train_loss= 0.41645 time= 0.31320\n",
      "Epoch: 0194 train_loss= 0.41635 time= 0.31745\n",
      "Epoch: 0195 train_loss= 0.41625 time= 0.30813\n",
      "Epoch: 0196 train_loss= 0.41615 time= 0.31599\n",
      "Epoch: 0197 train_loss= 0.41605 time= 0.30159\n",
      "Epoch: 0198 train_loss= 0.41594 time= 0.31290\n",
      "Epoch: 0199 train_loss= 0.41583 time= 0.30919\n",
      "Epoch: 0200 train_loss= 0.41572 time= 0.29952\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84170 time= 0.43583\n",
      "Epoch: 0002 train_loss= 0.84088 time= 0.29001\n",
      "Epoch: 0003 train_loss= 0.83902 time= 0.30720\n",
      "Epoch: 0004 train_loss= 0.83559 time= 0.31441\n",
      "Epoch: 0005 train_loss= 0.82992 time= 0.30514\n",
      "Epoch: 0006 train_loss= 0.82130 time= 0.30356\n",
      "Epoch: 0007 train_loss= 0.80906 time= 0.30215\n",
      "Epoch: 0008 train_loss= 0.79279 time= 0.30173\n",
      "Epoch: 0009 train_loss= 0.77277 time= 0.31176\n",
      "Epoch: 0010 train_loss= 0.75045 time= 0.30152\n",
      "Epoch: 0011 train_loss= 0.72910 time= 0.32041\n",
      "Epoch: 0012 train_loss= 0.71375 time= 0.29841\n",
      "Epoch: 0013 train_loss= 0.70786 time= 0.29486\n",
      "Epoch: 0014 train_loss= 0.70633 time= 0.29896\n",
      "Epoch: 0015 train_loss= 0.70021 time= 0.30895\n",
      "Epoch: 0016 train_loss= 0.68736 time= 0.28830\n",
      "Epoch: 0017 train_loss= 0.67050 time= 0.30040\n",
      "Epoch: 0018 train_loss= 0.65288 time= 0.29596\n",
      "Epoch: 0019 train_loss= 0.63652 time= 0.29608\n",
      "Epoch: 0020 train_loss= 0.62181 time= 0.30410\n",
      "Epoch: 0021 train_loss= 0.60816 time= 0.30266\n",
      "Epoch: 0022 train_loss= 0.59475 time= 0.30984\n",
      "Epoch: 0023 train_loss= 0.58111 time= 0.28978\n",
      "Epoch: 0024 train_loss= 0.56729 time= 0.31026\n",
      "Epoch: 0025 train_loss= 0.55373 time= 0.29794\n",
      "Epoch: 0026 train_loss= 0.54108 time= 0.30804\n",
      "Epoch: 0027 train_loss= 0.53007 time= 0.28794\n",
      "Epoch: 0028 train_loss= 0.52130 time= 0.29599\n",
      "Epoch: 0029 train_loss= 0.51510 time= 0.30503\n",
      "Epoch: 0030 train_loss= 0.51139 time= 0.29849\n",
      "Epoch: 0031 train_loss= 0.50956 time= 0.29535\n",
      "Epoch: 0032 train_loss= 0.50870 time= 0.30572\n",
      "Epoch: 0033 train_loss= 0.50780 time= 0.29744\n",
      "Epoch: 0034 train_loss= 0.50614 time= 0.29995\n",
      "Epoch: 0035 train_loss= 0.50345 time= 0.29264\n",
      "Epoch: 0036 train_loss= 0.49987 time= 0.30440\n",
      "Epoch: 0037 train_loss= 0.49575 time= 0.30677\n",
      "Epoch: 0038 train_loss= 0.49151 time= 0.30453\n",
      "Epoch: 0039 train_loss= 0.48748 time= 0.29318\n",
      "Epoch: 0040 train_loss= 0.48385 time= 0.33041\n",
      "Epoch: 0041 train_loss= 0.48071 time= 0.30086\n",
      "Epoch: 0042 train_loss= 0.47808 time= 0.29496\n",
      "Epoch: 0043 train_loss= 0.47592 time= 0.30618\n",
      "Epoch: 0044 train_loss= 0.47417 time= 0.31056\n",
      "Epoch: 0045 train_loss= 0.47275 time= 0.31411\n",
      "Epoch: 0046 train_loss= 0.47157 time= 0.29436\n",
      "Epoch: 0047 train_loss= 0.47052 time= 0.29601\n",
      "Epoch: 0048 train_loss= 0.46952 time= 0.30356\n",
      "Epoch: 0049 train_loss= 0.46849 time= 0.30094\n",
      "Epoch: 0050 train_loss= 0.46738 time= 0.30435\n",
      "Epoch: 0051 train_loss= 0.46618 time= 0.30200\n",
      "Epoch: 0052 train_loss= 0.46488 time= 0.30224\n",
      "Epoch: 0053 train_loss= 0.46353 time= 0.29454\n",
      "Epoch: 0054 train_loss= 0.46215 time= 0.32662\n",
      "Epoch: 0055 train_loss= 0.46079 time= 0.29815\n",
      "Epoch: 0056 train_loss= 0.45949 time= 0.30119\n",
      "Epoch: 0057 train_loss= 0.45825 time= 0.31254\n",
      "Epoch: 0058 train_loss= 0.45710 time= 0.30723\n",
      "Epoch: 0059 train_loss= 0.45602 time= 0.29809\n",
      "Epoch: 0060 train_loss= 0.45502 time= 0.29574\n",
      "Epoch: 0061 train_loss= 0.45408 time= 0.31303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0062 train_loss= 0.45319 time= 0.31918\n",
      "Epoch: 0063 train_loss= 0.45234 time= 0.29464\n",
      "Epoch: 0064 train_loss= 0.45151 time= 0.31003\n",
      "Epoch: 0065 train_loss= 0.45070 time= 0.30959\n",
      "Epoch: 0066 train_loss= 0.44990 time= 0.31719\n",
      "Epoch: 0067 train_loss= 0.44911 time= 0.30437\n",
      "Epoch: 0068 train_loss= 0.44832 time= 0.29846\n",
      "Epoch: 0069 train_loss= 0.44755 time= 0.29812\n",
      "Epoch: 0070 train_loss= 0.44680 time= 0.31219\n",
      "Epoch: 0071 train_loss= 0.44609 time= 0.29834\n",
      "Epoch: 0072 train_loss= 0.44542 time= 0.30493\n",
      "Epoch: 0073 train_loss= 0.44479 time= 0.30100\n",
      "Epoch: 0074 train_loss= 0.44418 time= 0.30298\n",
      "Epoch: 0075 train_loss= 0.44361 time= 0.31017\n",
      "Epoch: 0076 train_loss= 0.44306 time= 0.29325\n",
      "Epoch: 0077 train_loss= 0.44253 time= 0.31268\n",
      "Epoch: 0078 train_loss= 0.44202 time= 0.29871\n",
      "Epoch: 0079 train_loss= 0.44153 time= 0.31394\n",
      "Epoch: 0080 train_loss= 0.44106 time= 0.29442\n",
      "Epoch: 0081 train_loss= 0.44061 time= 0.30285\n",
      "Epoch: 0082 train_loss= 0.44018 time= 0.29721\n",
      "Epoch: 0083 train_loss= 0.43977 time= 0.30038\n",
      "Epoch: 0084 train_loss= 0.43936 time= 0.30296\n",
      "Epoch: 0085 train_loss= 0.43896 time= 0.30958\n",
      "Epoch: 0086 train_loss= 0.43858 time= 0.29581\n",
      "Epoch: 0087 train_loss= 0.43821 time= 0.29532\n",
      "Epoch: 0088 train_loss= 0.43785 time= 0.30681\n",
      "Epoch: 0089 train_loss= 0.43750 time= 0.30020\n",
      "Epoch: 0090 train_loss= 0.43715 time= 0.29869\n",
      "Epoch: 0091 train_loss= 0.43681 time= 0.30738\n",
      "Epoch: 0092 train_loss= 0.43647 time= 0.30569\n",
      "Epoch: 0093 train_loss= 0.43612 time= 0.29966\n",
      "Epoch: 0094 train_loss= 0.43578 time= 0.29851\n",
      "Epoch: 0095 train_loss= 0.43544 time= 0.31416\n",
      "Epoch: 0096 train_loss= 0.43509 time= 0.29452\n",
      "Epoch: 0097 train_loss= 0.43475 time= 0.30794\n",
      "Epoch: 0098 train_loss= 0.43440 time= 0.29564\n",
      "Epoch: 0099 train_loss= 0.43406 time= 0.30645\n",
      "Epoch: 0100 train_loss= 0.43373 time= 0.30088\n",
      "Epoch: 0101 train_loss= 0.43340 time= 0.30547\n",
      "Epoch: 0102 train_loss= 0.43308 time= 0.30518\n",
      "Epoch: 0103 train_loss= 0.43276 time= 0.30062\n",
      "Epoch: 0104 train_loss= 0.43245 time= 0.29636\n",
      "Epoch: 0105 train_loss= 0.43214 time= 0.30132\n",
      "Epoch: 0106 train_loss= 0.43184 time= 0.31130\n",
      "Epoch: 0107 train_loss= 0.43155 time= 0.30933\n",
      "Epoch: 0108 train_loss= 0.43126 time= 0.30690\n",
      "Epoch: 0109 train_loss= 0.43098 time= 0.31154\n",
      "Epoch: 0110 train_loss= 0.43070 time= 0.29596\n",
      "Epoch: 0111 train_loss= 0.43042 time= 0.30976\n",
      "Epoch: 0112 train_loss= 0.43015 time= 0.30723\n",
      "Epoch: 0113 train_loss= 0.42987 time= 0.29757\n",
      "Epoch: 0114 train_loss= 0.42959 time= 0.31121\n",
      "Epoch: 0115 train_loss= 0.42932 time= 0.30506\n",
      "Epoch: 0116 train_loss= 0.42903 time= 0.30565\n",
      "Epoch: 0117 train_loss= 0.42875 time= 0.31161\n",
      "Epoch: 0118 train_loss= 0.42846 time= 0.30603\n",
      "Epoch: 0119 train_loss= 0.42818 time= 0.30913\n",
      "Epoch: 0120 train_loss= 0.42789 time= 0.29304\n",
      "Epoch: 0121 train_loss= 0.42760 time= 0.30818\n",
      "Epoch: 0122 train_loss= 0.42731 time= 0.29933\n",
      "Epoch: 0123 train_loss= 0.42702 time= 0.30758\n",
      "Epoch: 0124 train_loss= 0.42673 time= 0.30253\n",
      "Epoch: 0125 train_loss= 0.42645 time= 0.30041\n",
      "Epoch: 0126 train_loss= 0.42616 time= 0.30624\n",
      "Epoch: 0127 train_loss= 0.42588 time= 0.29421\n",
      "Epoch: 0128 train_loss= 0.42560 time= 0.30319\n",
      "Epoch: 0129 train_loss= 0.42532 time= 0.30926\n",
      "Epoch: 0130 train_loss= 0.42504 time= 0.30593\n",
      "Epoch: 0131 train_loss= 0.42477 time= 0.31203\n",
      "Epoch: 0132 train_loss= 0.42450 time= 0.29338\n",
      "Epoch: 0133 train_loss= 0.42423 time= 0.30846\n",
      "Epoch: 0134 train_loss= 0.42397 time= 0.30803\n",
      "Epoch: 0135 train_loss= 0.42371 time= 0.30634\n",
      "Epoch: 0136 train_loss= 0.42345 time= 0.30686\n",
      "Epoch: 0137 train_loss= 0.42320 time= 0.30576\n",
      "Epoch: 0138 train_loss= 0.42295 time= 0.30151\n",
      "Epoch: 0139 train_loss= 0.42270 time= 0.31178\n",
      "Epoch: 0140 train_loss= 0.42246 time= 0.29217\n",
      "Epoch: 0141 train_loss= 0.42222 time= 0.30125\n",
      "Epoch: 0142 train_loss= 0.42198 time= 0.30326\n",
      "Epoch: 0143 train_loss= 0.42174 time= 0.30374\n",
      "Epoch: 0144 train_loss= 0.42150 time= 0.30557\n",
      "Epoch: 0145 train_loss= 0.42127 time= 0.31030\n",
      "Epoch: 0146 train_loss= 0.42104 time= 0.30477\n",
      "Epoch: 0147 train_loss= 0.42080 time= 0.29923\n",
      "Epoch: 0148 train_loss= 0.42057 time= 0.29956\n",
      "Epoch: 0149 train_loss= 0.42034 time= 0.30162\n",
      "Epoch: 0150 train_loss= 0.42011 time= 0.30524\n",
      "Epoch: 0151 train_loss= 0.41988 time= 0.31439\n",
      "Epoch: 0152 train_loss= 0.41965 time= 0.31245\n",
      "Epoch: 0153 train_loss= 0.41942 time= 0.29644\n",
      "Epoch: 0154 train_loss= 0.41919 time= 0.30879\n",
      "Epoch: 0155 train_loss= 0.41896 time= 0.30949\n",
      "Epoch: 0156 train_loss= 0.41873 time= 0.29968\n",
      "Epoch: 0157 train_loss= 0.41850 time= 0.31005\n",
      "Epoch: 0158 train_loss= 0.41828 time= 0.31571\n",
      "Epoch: 0159 train_loss= 0.41805 time= 0.29531\n",
      "Epoch: 0160 train_loss= 0.41783 time= 0.29737\n",
      "Epoch: 0161 train_loss= 0.41761 time= 0.30917\n",
      "Epoch: 0162 train_loss= 0.41739 time= 0.29771\n",
      "Epoch: 0163 train_loss= 0.41718 time= 0.30297\n",
      "Epoch: 0164 train_loss= 0.41696 time= 0.30920\n",
      "Epoch: 0165 train_loss= 0.41676 time= 0.30495\n",
      "Epoch: 0166 train_loss= 0.41655 time= 0.30555\n",
      "Epoch: 0167 train_loss= 0.41635 time= 0.30690\n",
      "Epoch: 0168 train_loss= 0.41615 time= 0.29571\n",
      "Epoch: 0169 train_loss= 0.41596 time= 0.30244\n",
      "Epoch: 0170 train_loss= 0.41576 time= 0.31100\n",
      "Epoch: 0171 train_loss= 0.41557 time= 0.30942\n",
      "Epoch: 0172 train_loss= 0.41538 time= 0.30782\n",
      "Epoch: 0173 train_loss= 0.41519 time= 0.29708\n",
      "Epoch: 0174 train_loss= 0.41501 time= 0.29800\n",
      "Epoch: 0175 train_loss= 0.41483 time= 0.30818\n",
      "Epoch: 0176 train_loss= 0.41464 time= 0.31678\n",
      "Epoch: 0177 train_loss= 0.41446 time= 0.30795\n",
      "Epoch: 0178 train_loss= 0.41429 time= 0.29369\n",
      "Epoch: 0179 train_loss= 0.41411 time= 0.31134\n",
      "Epoch: 0180 train_loss= 0.41394 time= 0.30297\n",
      "Epoch: 0181 train_loss= 0.41376 time= 0.30956\n",
      "Epoch: 0182 train_loss= 0.41360 time= 0.30908\n",
      "Epoch: 0183 train_loss= 0.41343 time= 0.30893\n",
      "Epoch: 0184 train_loss= 0.41327 time= 0.29295\n",
      "Epoch: 0185 train_loss= 0.41311 time= 0.31998\n",
      "Epoch: 0186 train_loss= 0.41295 time= 0.29713\n",
      "Epoch: 0187 train_loss= 0.41279 time= 0.29485\n",
      "Epoch: 0188 train_loss= 0.41264 time= 0.30214\n",
      "Epoch: 0189 train_loss= 0.41249 time= 0.29584\n",
      "Epoch: 0190 train_loss= 0.41235 time= 0.33404\n",
      "Epoch: 0191 train_loss= 0.41220 time= 0.28858\n",
      "Epoch: 0192 train_loss= 0.41206 time= 0.30071\n",
      "Epoch: 0193 train_loss= 0.41192 time= 0.31919\n",
      "Epoch: 0194 train_loss= 0.41179 time= 0.31053\n",
      "Epoch: 0195 train_loss= 0.41165 time= 0.29444\n",
      "Epoch: 0196 train_loss= 0.41152 time= 0.30996\n",
      "Epoch: 0197 train_loss= 0.41139 time= 0.29673\n",
      "Epoch: 0198 train_loss= 0.41126 time= 0.31997\n",
      "Epoch: 0199 train_loss= 0.41114 time= 0.30457\n",
      "Epoch: 0200 train_loss= 0.41101 time= 0.30543\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84166 time= 0.44429\n",
      "Epoch: 0002 train_loss= 0.84070 time= 0.29042\n",
      "Epoch: 0003 train_loss= 0.83844 time= 0.32469\n",
      "Epoch: 0004 train_loss= 0.83416 time= 0.31283\n",
      "Epoch: 0005 train_loss= 0.82700 time= 0.33043\n",
      "Epoch: 0006 train_loss= 0.81606 time= 0.31716\n",
      "Epoch: 0007 train_loss= 0.80068 time= 0.32017\n",
      "Epoch: 0008 train_loss= 0.78094 time= 0.30315\n",
      "Epoch: 0009 train_loss= 0.75845 time= 0.31011\n",
      "Epoch: 0010 train_loss= 0.73734 time= 0.29222\n",
      "Epoch: 0011 train_loss= 0.72454 time= 0.32478\n",
      "Epoch: 0012 train_loss= 0.72460 time= 0.30796\n",
      "Epoch: 0013 train_loss= 0.72776 time= 0.29371\n",
      "Epoch: 0014 train_loss= 0.72318 time= 0.30188\n",
      "Epoch: 0015 train_loss= 0.71161 time= 0.29150\n",
      "Epoch: 0016 train_loss= 0.69784 time= 0.29739\n",
      "Epoch: 0017 train_loss= 0.68554 time= 0.29967\n",
      "Epoch: 0018 train_loss= 0.67590 time= 0.31207\n",
      "Epoch: 0019 train_loss= 0.66816 time= 0.29705\n",
      "Epoch: 0020 train_loss= 0.66074 time= 0.29179\n",
      "Epoch: 0021 train_loss= 0.65232 time= 0.30826\n",
      "Epoch: 0022 train_loss= 0.64217 time= 0.29415\n",
      "Epoch: 0023 train_loss= 0.63017 time= 0.30219\n",
      "Epoch: 0024 train_loss= 0.61662 time= 0.29279\n",
      "Epoch: 0025 train_loss= 0.60216 time= 0.31767\n",
      "Epoch: 0026 train_loss= 0.58757 time= 0.30840\n",
      "Epoch: 0027 train_loss= 0.57360 time= 0.32558\n",
      "Epoch: 0028 train_loss= 0.56083 time= 0.29869\n",
      "Epoch: 0029 train_loss= 0.54959 time= 0.30220\n",
      "Epoch: 0030 train_loss= 0.53993 time= 0.30542\n",
      "Epoch: 0031 train_loss= 0.53182 time= 0.29558\n",
      "Epoch: 0032 train_loss= 0.52520 time= 0.29490\n",
      "Epoch: 0033 train_loss= 0.52005 time= 0.29931\n",
      "Epoch: 0034 train_loss= 0.51631 time= 0.29919\n",
      "Epoch: 0035 train_loss= 0.51374 time= 0.30477\n",
      "Epoch: 0036 train_loss= 0.51188 time= 0.30520\n",
      "Epoch: 0037 train_loss= 0.51013 time= 0.29276\n",
      "Epoch: 0038 train_loss= 0.50799 time= 0.29803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0039 train_loss= 0.50516 time= 0.31173\n",
      "Epoch: 0040 train_loss= 0.50162 time= 0.29812\n",
      "Epoch: 0041 train_loss= 0.49761 time= 0.29897\n",
      "Epoch: 0042 train_loss= 0.49347 time= 0.29034\n",
      "Epoch: 0043 train_loss= 0.48955 time= 0.31316\n",
      "Epoch: 0044 train_loss= 0.48607 time= 0.29881\n",
      "Epoch: 0045 train_loss= 0.48314 time= 0.29413\n",
      "Epoch: 0046 train_loss= 0.48074 time= 0.31944\n",
      "Epoch: 0047 train_loss= 0.47879 time= 0.29534\n",
      "Epoch: 0048 train_loss= 0.47718 time= 0.28797\n",
      "Epoch: 0049 train_loss= 0.47579 time= 0.30319\n",
      "Epoch: 0050 train_loss= 0.47453 time= 0.29877\n",
      "Epoch: 0051 train_loss= 0.47332 time= 0.30532\n",
      "Epoch: 0052 train_loss= 0.47211 time= 0.31169\n",
      "Epoch: 0053 train_loss= 0.47085 time= 0.30756\n",
      "Epoch: 0054 train_loss= 0.46950 time= 0.30391\n",
      "Epoch: 0055 train_loss= 0.46806 time= 0.28609\n",
      "Epoch: 0056 train_loss= 0.46657 time= 0.30918\n",
      "Epoch: 0057 train_loss= 0.46506 time= 0.30268\n",
      "Epoch: 0058 train_loss= 0.46361 time= 0.29662\n",
      "Epoch: 0059 train_loss= 0.46227 time= 0.30662\n",
      "Epoch: 0060 train_loss= 0.46107 time= 0.30854\n",
      "Epoch: 0061 train_loss= 0.46001 time= 0.29758\n",
      "Epoch: 0062 train_loss= 0.45907 time= 0.29958\n",
      "Epoch: 0063 train_loss= 0.45822 time= 0.30373\n",
      "Epoch: 0064 train_loss= 0.45741 time= 0.29240\n",
      "Epoch: 0065 train_loss= 0.45664 time= 0.30640\n",
      "Epoch: 0066 train_loss= 0.45589 time= 0.31326\n",
      "Epoch: 0067 train_loss= 0.45516 time= 0.29839\n",
      "Epoch: 0068 train_loss= 0.45443 time= 0.29199\n",
      "Epoch: 0069 train_loss= 0.45371 time= 0.30210\n",
      "Epoch: 0070 train_loss= 0.45298 time= 0.30357\n",
      "Epoch: 0071 train_loss= 0.45225 time= 0.31303\n",
      "Epoch: 0072 train_loss= 0.45153 time= 0.31070\n",
      "Epoch: 0073 train_loss= 0.45083 time= 0.30315\n",
      "Epoch: 0074 train_loss= 0.45018 time= 0.31716\n",
      "Epoch: 0075 train_loss= 0.44957 time= 0.30652\n",
      "Epoch: 0076 train_loss= 0.44900 time= 0.29916\n",
      "Epoch: 0077 train_loss= 0.44846 time= 0.31180\n",
      "Epoch: 0078 train_loss= 0.44796 time= 0.29862\n",
      "Epoch: 0079 train_loss= 0.44747 time= 0.30191\n",
      "Epoch: 0080 train_loss= 0.44699 time= 0.30114\n",
      "Epoch: 0081 train_loss= 0.44653 time= 0.29643\n",
      "Epoch: 0082 train_loss= 0.44609 time= 0.29528\n",
      "Epoch: 0083 train_loss= 0.44566 time= 0.29664\n",
      "Epoch: 0084 train_loss= 0.44525 time= 0.31422\n",
      "Epoch: 0085 train_loss= 0.44486 time= 0.31089\n",
      "Epoch: 0086 train_loss= 0.44449 time= 0.29513\n",
      "Epoch: 0087 train_loss= 0.44412 time= 0.28439\n",
      "Epoch: 0088 train_loss= 0.44378 time= 0.32240\n",
      "Epoch: 0089 train_loss= 0.44345 time= 0.31233\n",
      "Epoch: 0090 train_loss= 0.44314 time= 0.29844\n",
      "Epoch: 0091 train_loss= 0.44283 time= 0.31078\n",
      "Epoch: 0092 train_loss= 0.44255 time= 0.30837\n",
      "Epoch: 0093 train_loss= 0.44227 time= 0.30961\n",
      "Epoch: 0094 train_loss= 0.44199 time= 0.30263\n",
      "Epoch: 0095 train_loss= 0.44173 time= 0.29323\n",
      "Epoch: 0096 train_loss= 0.44147 time= 0.30513\n",
      "Epoch: 0097 train_loss= 0.44121 time= 0.30443\n",
      "Epoch: 0098 train_loss= 0.44096 time= 0.30937\n",
      "Epoch: 0099 train_loss= 0.44070 time= 0.31775\n",
      "Epoch: 0100 train_loss= 0.44045 time= 0.28933\n",
      "Epoch: 0101 train_loss= 0.44020 time= 0.29695\n",
      "Epoch: 0102 train_loss= 0.43995 time= 0.30357\n",
      "Epoch: 0103 train_loss= 0.43969 time= 0.30166\n",
      "Epoch: 0104 train_loss= 0.43943 time= 0.30718\n",
      "Epoch: 0105 train_loss= 0.43917 time= 0.30898\n",
      "Epoch: 0106 train_loss= 0.43891 time= 0.29580\n",
      "Epoch: 0107 train_loss= 0.43864 time= 0.29817\n",
      "Epoch: 0108 train_loss= 0.43837 time= 0.30297\n",
      "Epoch: 0109 train_loss= 0.43809 time= 0.31445\n",
      "Epoch: 0110 train_loss= 0.43781 time= 0.29015\n",
      "Epoch: 0111 train_loss= 0.43753 time= 0.30629\n",
      "Epoch: 0112 train_loss= 0.43724 time= 0.30099\n",
      "Epoch: 0113 train_loss= 0.43696 time= 0.30635\n",
      "Epoch: 0114 train_loss= 0.43667 time= 0.28760\n",
      "Epoch: 0115 train_loss= 0.43637 time= 0.31577\n",
      "Epoch: 0116 train_loss= 0.43608 time= 0.30789\n",
      "Epoch: 0117 train_loss= 0.43578 time= 0.32060\n",
      "Epoch: 0118 train_loss= 0.43548 time= 0.31374\n",
      "Epoch: 0119 train_loss= 0.43518 time= 0.31727\n",
      "Epoch: 0120 train_loss= 0.43488 time= 0.31013\n",
      "Epoch: 0121 train_loss= 0.43458 time= 0.29699\n",
      "Epoch: 0122 train_loss= 0.43428 time= 0.30419\n",
      "Epoch: 0123 train_loss= 0.43398 time= 0.30999\n",
      "Epoch: 0124 train_loss= 0.43369 time= 0.30270\n",
      "Epoch: 0125 train_loss= 0.43339 time= 0.29696\n",
      "Epoch: 0126 train_loss= 0.43310 time= 0.30205\n",
      "Epoch: 0127 train_loss= 0.43281 time= 0.29353\n",
      "Epoch: 0128 train_loss= 0.43252 time= 0.30155\n",
      "Epoch: 0129 train_loss= 0.43223 time= 0.31415\n",
      "Epoch: 0130 train_loss= 0.43195 time= 0.29582\n",
      "Epoch: 0131 train_loss= 0.43167 time= 0.29921\n",
      "Epoch: 0132 train_loss= 0.43140 time= 0.30812\n",
      "Epoch: 0133 train_loss= 0.43112 time= 0.30157\n",
      "Epoch: 0134 train_loss= 0.43085 time= 0.30077\n",
      "Epoch: 0135 train_loss= 0.43058 time= 0.30020\n",
      "Epoch: 0136 train_loss= 0.43031 time= 0.30585\n",
      "Epoch: 0137 train_loss= 0.43004 time= 0.30163\n",
      "Epoch: 0138 train_loss= 0.42977 time= 0.30889\n",
      "Epoch: 0139 train_loss= 0.42951 time= 0.30960\n",
      "Epoch: 0140 train_loss= 0.42925 time= 0.29484\n",
      "Epoch: 0141 train_loss= 0.42899 time= 0.30933\n",
      "Epoch: 0142 train_loss= 0.42873 time= 0.30340\n",
      "Epoch: 0143 train_loss= 0.42847 time= 0.29634\n",
      "Epoch: 0144 train_loss= 0.42821 time= 0.30095\n",
      "Epoch: 0145 train_loss= 0.42796 time= 0.29941\n",
      "Epoch: 0146 train_loss= 0.42771 time= 0.33098\n",
      "Epoch: 0147 train_loss= 0.42746 time= 0.30723\n",
      "Epoch: 0148 train_loss= 0.42721 time= 0.30518\n",
      "Epoch: 0149 train_loss= 0.42696 time= 0.30786\n",
      "Epoch: 0150 train_loss= 0.42672 time= 0.31108\n",
      "Epoch: 0151 train_loss= 0.42648 time= 0.30638\n",
      "Epoch: 0152 train_loss= 0.42623 time= 0.30007\n",
      "Epoch: 0153 train_loss= 0.42599 time= 0.29430\n",
      "Epoch: 0154 train_loss= 0.42576 time= 0.31625\n",
      "Epoch: 0155 train_loss= 0.42552 time= 0.30692\n",
      "Epoch: 0156 train_loss= 0.42528 time= 0.30603\n",
      "Epoch: 0157 train_loss= 0.42505 time= 0.30669\n",
      "Epoch: 0158 train_loss= 0.42481 time= 0.32691\n",
      "Epoch: 0159 train_loss= 0.42458 time= 0.31099\n",
      "Epoch: 0160 train_loss= 0.42435 time= 0.29592\n",
      "Epoch: 0161 train_loss= 0.42411 time= 0.30302\n",
      "Epoch: 0162 train_loss= 0.42388 time= 0.30622\n",
      "Epoch: 0163 train_loss= 0.42366 time= 0.29793\n",
      "Epoch: 0164 train_loss= 0.42343 time= 0.30963\n",
      "Epoch: 0165 train_loss= 0.42320 time= 0.30122\n",
      "Epoch: 0166 train_loss= 0.42298 time= 0.30834\n",
      "Epoch: 0167 train_loss= 0.42276 time= 0.30823\n",
      "Epoch: 0168 train_loss= 0.42254 time= 0.28681\n",
      "Epoch: 0169 train_loss= 0.42233 time= 0.31294\n",
      "Epoch: 0170 train_loss= 0.42212 time= 0.32396\n",
      "Epoch: 0171 train_loss= 0.42191 time= 0.30953\n",
      "Epoch: 0172 train_loss= 0.42170 time= 0.29182\n",
      "Epoch: 0173 train_loss= 0.42149 time= 0.29775\n",
      "Epoch: 0174 train_loss= 0.42129 time= 0.30691\n",
      "Epoch: 0175 train_loss= 0.42109 time= 0.30614\n",
      "Epoch: 0176 train_loss= 0.42090 time= 0.30378\n",
      "Epoch: 0177 train_loss= 0.42070 time= 0.31912\n",
      "Epoch: 0178 train_loss= 0.42051 time= 0.33003\n",
      "Epoch: 0179 train_loss= 0.42032 time= 0.29037\n",
      "Epoch: 0180 train_loss= 0.42013 time= 0.29721\n",
      "Epoch: 0181 train_loss= 0.41994 time= 0.31092\n",
      "Epoch: 0182 train_loss= 0.41975 time= 0.30960\n",
      "Epoch: 0183 train_loss= 0.41957 time= 0.30565\n",
      "Epoch: 0184 train_loss= 0.41939 time= 0.29839\n",
      "Epoch: 0185 train_loss= 0.41920 time= 0.30304\n",
      "Epoch: 0186 train_loss= 0.41902 time= 0.30140\n",
      "Epoch: 0187 train_loss= 0.41884 time= 0.31491\n",
      "Epoch: 0188 train_loss= 0.41867 time= 0.31902\n",
      "Epoch: 0189 train_loss= 0.41849 time= 0.30273\n",
      "Epoch: 0190 train_loss= 0.41832 time= 0.32907\n",
      "Epoch: 0191 train_loss= 0.41814 time= 0.30654\n",
      "Epoch: 0192 train_loss= 0.41798 time= 0.30050\n",
      "Epoch: 0193 train_loss= 0.41781 time= 0.33650\n",
      "Epoch: 0194 train_loss= 0.41765 time= 0.32738\n",
      "Epoch: 0195 train_loss= 0.41749 time= 0.30850\n",
      "Epoch: 0196 train_loss= 0.41733 time= 0.32205\n",
      "Epoch: 0197 train_loss= 0.41718 time= 0.30874\n",
      "Epoch: 0198 train_loss= 0.41703 time= 0.31620\n",
      "Epoch: 0199 train_loss= 0.41689 time= 0.30496\n",
      "Epoch: 0200 train_loss= 0.41675 time= 0.30606\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84165 time= 0.50216\n",
      "Epoch: 0002 train_loss= 0.84071 time= 0.29321\n",
      "Epoch: 0003 train_loss= 0.83851 time= 0.29083\n",
      "Epoch: 0004 train_loss= 0.83437 time= 0.30966\n",
      "Epoch: 0005 train_loss= 0.82755 time= 0.30739\n",
      "Epoch: 0006 train_loss= 0.81726 time= 0.29268\n",
      "Epoch: 0007 train_loss= 0.80298 time= 0.29923\n",
      "Epoch: 0008 train_loss= 0.78476 time= 0.29031\n",
      "Epoch: 0009 train_loss= 0.76387 time= 0.30141\n",
      "Epoch: 0010 train_loss= 0.74362 time= 0.30616\n",
      "Epoch: 0011 train_loss= 0.72964 time= 0.29719\n",
      "Epoch: 0012 train_loss= 0.72645 time= 0.30018\n",
      "Epoch: 0013 train_loss= 0.72815 time= 0.30897\n",
      "Epoch: 0014 train_loss= 0.72457 time= 0.32682\n",
      "Epoch: 0015 train_loss= 0.71412 time= 0.31914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0016 train_loss= 0.69983 time= 0.31859\n",
      "Epoch: 0017 train_loss= 0.68486 time= 0.30114\n",
      "Epoch: 0018 train_loss= 0.67089 time= 0.30186\n",
      "Epoch: 0019 train_loss= 0.65811 time= 0.31977\n",
      "Epoch: 0020 train_loss= 0.64584 time= 0.31867\n",
      "Epoch: 0021 train_loss= 0.63331 time= 0.29714\n",
      "Epoch: 0022 train_loss= 0.62007 time= 0.31711\n",
      "Epoch: 0023 train_loss= 0.60610 time= 0.30069\n",
      "Epoch: 0024 train_loss= 0.59187 time= 0.30446\n",
      "Epoch: 0025 train_loss= 0.57807 time= 0.32334\n",
      "Epoch: 0026 train_loss= 0.56544 time= 0.29543\n",
      "Epoch: 0027 train_loss= 0.55453 time= 0.31035\n",
      "Epoch: 0028 train_loss= 0.54552 time= 0.29920\n",
      "Epoch: 0029 train_loss= 0.53821 time= 0.30119\n",
      "Epoch: 0030 train_loss= 0.53214 time= 0.31146\n",
      "Epoch: 0031 train_loss= 0.52684 time= 0.29837\n",
      "Epoch: 0032 train_loss= 0.52190 time= 0.31049\n",
      "Epoch: 0033 train_loss= 0.51714 time= 0.31702\n",
      "Epoch: 0034 train_loss= 0.51256 time= 0.30568\n",
      "Epoch: 0035 train_loss= 0.50827 time= 0.31954\n",
      "Epoch: 0036 train_loss= 0.50443 time= 0.31346\n",
      "Epoch: 0037 train_loss= 0.50117 time= 0.30781\n",
      "Epoch: 0038 train_loss= 0.49854 time= 0.30036\n",
      "Epoch: 0039 train_loss= 0.49650 time= 0.30332\n",
      "Epoch: 0040 train_loss= 0.49491 time= 0.29990\n",
      "Epoch: 0041 train_loss= 0.49359 time= 0.30690\n",
      "Epoch: 0042 train_loss= 0.49235 time= 0.32076\n",
      "Epoch: 0043 train_loss= 0.49106 time= 0.29986\n",
      "Epoch: 0044 train_loss= 0.48961 time= 0.31987\n",
      "Epoch: 0045 train_loss= 0.48796 time= 0.30204\n",
      "Epoch: 0046 train_loss= 0.48614 time= 0.29948\n",
      "Epoch: 0047 train_loss= 0.48417 time= 0.31551\n",
      "Epoch: 0048 train_loss= 0.48215 time= 0.29507\n",
      "Epoch: 0049 train_loss= 0.48015 time= 0.33782\n",
      "Epoch: 0050 train_loss= 0.47827 time= 0.29608\n",
      "Epoch: 0051 train_loss= 0.47656 time= 0.31306\n",
      "Epoch: 0052 train_loss= 0.47507 time= 0.28829\n",
      "Epoch: 0053 train_loss= 0.47379 time= 0.33641\n",
      "Epoch: 0054 train_loss= 0.47270 time= 0.31217\n",
      "Epoch: 0055 train_loss= 0.47177 time= 0.30247\n",
      "Epoch: 0056 train_loss= 0.47095 time= 0.30467\n",
      "Epoch: 0057 train_loss= 0.47017 time= 0.30760\n",
      "Epoch: 0058 train_loss= 0.46940 time= 0.29936\n",
      "Epoch: 0059 train_loss= 0.46858 time= 0.30142\n",
      "Epoch: 0060 train_loss= 0.46769 time= 0.30319\n",
      "Epoch: 0061 train_loss= 0.46674 time= 0.30418\n",
      "Epoch: 0062 train_loss= 0.46573 time= 0.31441\n",
      "Epoch: 0063 train_loss= 0.46468 time= 0.31611\n",
      "Epoch: 0064 train_loss= 0.46364 time= 0.29951\n",
      "Epoch: 0065 train_loss= 0.46263 time= 0.30082\n",
      "Epoch: 0066 train_loss= 0.46165 time= 0.29590\n",
      "Epoch: 0067 train_loss= 0.46072 time= 0.30836\n",
      "Epoch: 0068 train_loss= 0.45982 time= 0.30119\n",
      "Epoch: 0069 train_loss= 0.45895 time= 0.31070\n",
      "Epoch: 0070 train_loss= 0.45810 time= 0.31750\n",
      "Epoch: 0071 train_loss= 0.45727 time= 0.29584\n",
      "Epoch: 0072 train_loss= 0.45645 time= 0.31160\n",
      "Epoch: 0073 train_loss= 0.45566 time= 0.29452\n",
      "Epoch: 0074 train_loss= 0.45489 time= 0.30319\n",
      "Epoch: 0075 train_loss= 0.45415 time= 0.30519\n",
      "Epoch: 0076 train_loss= 0.45346 time= 0.31849\n",
      "Epoch: 0077 train_loss= 0.45281 time= 0.29660\n",
      "Epoch: 0078 train_loss= 0.45221 time= 0.31267\n",
      "Epoch: 0079 train_loss= 0.45166 time= 0.31750\n",
      "Epoch: 0080 train_loss= 0.45115 time= 0.31455\n",
      "Epoch: 0081 train_loss= 0.45067 time= 0.32015\n",
      "Epoch: 0082 train_loss= 0.45023 time= 0.32271\n",
      "Epoch: 0083 train_loss= 0.44981 time= 0.30978\n",
      "Epoch: 0084 train_loss= 0.44940 time= 0.31594\n",
      "Epoch: 0085 train_loss= 0.44900 time= 0.31015\n",
      "Epoch: 0086 train_loss= 0.44860 time= 0.29375\n",
      "Epoch: 0087 train_loss= 0.44822 time= 0.31077\n",
      "Epoch: 0088 train_loss= 0.44783 time= 0.31317\n",
      "Epoch: 0089 train_loss= 0.44746 time= 0.30192\n",
      "Epoch: 0090 train_loss= 0.44709 time= 0.30000\n",
      "Epoch: 0091 train_loss= 0.44673 time= 0.29001\n",
      "Epoch: 0092 train_loss= 0.44638 time= 0.33303\n",
      "Epoch: 0093 train_loss= 0.44604 time= 0.31615\n",
      "Epoch: 0094 train_loss= 0.44570 time= 0.29920\n",
      "Epoch: 0095 train_loss= 0.44536 time= 0.30455\n",
      "Epoch: 0096 train_loss= 0.44502 time= 0.31658\n",
      "Epoch: 0097 train_loss= 0.44468 time= 0.29468\n",
      "Epoch: 0098 train_loss= 0.44433 time= 0.31319\n",
      "Epoch: 0099 train_loss= 0.44398 time= 0.29122\n",
      "Epoch: 0100 train_loss= 0.44362 time= 0.31915\n",
      "Epoch: 0101 train_loss= 0.44324 time= 0.30519\n",
      "Epoch: 0102 train_loss= 0.44286 time= 0.30862\n",
      "Epoch: 0103 train_loss= 0.44247 time= 0.31620\n",
      "Epoch: 0104 train_loss= 0.44208 time= 0.30646\n",
      "Epoch: 0105 train_loss= 0.44167 time= 0.30913\n",
      "Epoch: 0106 train_loss= 0.44126 time= 0.31815\n",
      "Epoch: 0107 train_loss= 0.44084 time= 0.29836\n",
      "Epoch: 0108 train_loss= 0.44041 time= 0.32726\n",
      "Epoch: 0109 train_loss= 0.43998 time= 0.32157\n",
      "Epoch: 0110 train_loss= 0.43955 time= 0.29901\n",
      "Epoch: 0111 train_loss= 0.43911 time= 0.30230\n",
      "Epoch: 0112 train_loss= 0.43868 time= 0.29507\n",
      "Epoch: 0113 train_loss= 0.43824 time= 0.30221\n",
      "Epoch: 0114 train_loss= 0.43781 time= 0.32330\n",
      "Epoch: 0115 train_loss= 0.43738 time= 0.30592\n",
      "Epoch: 0116 train_loss= 0.43696 time= 0.31965\n",
      "Epoch: 0117 train_loss= 0.43655 time= 0.29828\n",
      "Epoch: 0118 train_loss= 0.43614 time= 0.31693\n",
      "Epoch: 0119 train_loss= 0.43574 time= 0.32075\n",
      "Epoch: 0120 train_loss= 0.43535 time= 0.29948\n",
      "Epoch: 0121 train_loss= 0.43497 time= 0.32462\n",
      "Epoch: 0122 train_loss= 0.43460 time= 0.29510\n",
      "Epoch: 0123 train_loss= 0.43424 time= 0.30480\n",
      "Epoch: 0124 train_loss= 0.43389 time= 0.31090\n",
      "Epoch: 0125 train_loss= 0.43354 time= 0.29521\n",
      "Epoch: 0126 train_loss= 0.43321 time= 0.29222\n",
      "Epoch: 0127 train_loss= 0.43288 time= 0.31254\n",
      "Epoch: 0128 train_loss= 0.43256 time= 0.31425\n",
      "Epoch: 0129 train_loss= 0.43224 time= 0.30983\n",
      "Epoch: 0130 train_loss= 0.43193 time= 0.30200\n",
      "Epoch: 0131 train_loss= 0.43163 time= 0.30618\n",
      "Epoch: 0132 train_loss= 0.43133 time= 0.31416\n",
      "Epoch: 0133 train_loss= 0.43104 time= 0.31715\n",
      "Epoch: 0134 train_loss= 0.43075 time= 0.31330\n",
      "Epoch: 0135 train_loss= 0.43046 time= 0.31333\n",
      "Epoch: 0136 train_loss= 0.43017 time= 0.31248\n",
      "Epoch: 0137 train_loss= 0.42989 time= 0.31059\n",
      "Epoch: 0138 train_loss= 0.42960 time= 0.30267\n",
      "Epoch: 0139 train_loss= 0.42932 time= 0.31531\n",
      "Epoch: 0140 train_loss= 0.42904 time= 0.31747\n",
      "Epoch: 0141 train_loss= 0.42876 time= 0.32679\n",
      "Epoch: 0142 train_loss= 0.42848 time= 0.29581\n",
      "Epoch: 0143 train_loss= 0.42820 time= 0.28700\n",
      "Epoch: 0144 train_loss= 0.42792 time= 0.30213\n",
      "Epoch: 0145 train_loss= 0.42765 time= 0.33112\n",
      "Epoch: 0146 train_loss= 0.42737 time= 0.29136\n",
      "Epoch: 0147 train_loss= 0.42710 time= 0.29694\n",
      "Epoch: 0148 train_loss= 0.42683 time= 0.31124\n",
      "Epoch: 0149 train_loss= 0.42656 time= 0.29479\n",
      "Epoch: 0150 train_loss= 0.42629 time= 0.29529\n",
      "Epoch: 0151 train_loss= 0.42602 time= 0.31615\n",
      "Epoch: 0152 train_loss= 0.42575 time= 0.30950\n",
      "Epoch: 0153 train_loss= 0.42549 time= 0.31205\n",
      "Epoch: 0154 train_loss= 0.42523 time= 0.30725\n",
      "Epoch: 0155 train_loss= 0.42497 time= 0.29740\n",
      "Epoch: 0156 train_loss= 0.42471 time= 0.29118\n",
      "Epoch: 0157 train_loss= 0.42445 time= 0.31952\n",
      "Epoch: 0158 train_loss= 0.42420 time= 0.29785\n",
      "Epoch: 0159 train_loss= 0.42395 time= 0.30653\n",
      "Epoch: 0160 train_loss= 0.42370 time= 0.30155\n",
      "Epoch: 0161 train_loss= 0.42346 time= 0.29734\n",
      "Epoch: 0162 train_loss= 0.42322 time= 0.31279\n",
      "Epoch: 0163 train_loss= 0.42299 time= 0.30222\n",
      "Epoch: 0164 train_loss= 0.42276 time= 0.31010\n",
      "Epoch: 0165 train_loss= 0.42254 time= 0.30348\n",
      "Epoch: 0166 train_loss= 0.42232 time= 0.30964\n",
      "Epoch: 0167 train_loss= 0.42211 time= 0.30949\n",
      "Epoch: 0168 train_loss= 0.42190 time= 0.30030\n",
      "Epoch: 0169 train_loss= 0.42170 time= 0.29885\n",
      "Epoch: 0170 train_loss= 0.42151 time= 0.30049\n",
      "Epoch: 0171 train_loss= 0.42132 time= 0.29704\n",
      "Epoch: 0172 train_loss= 0.42113 time= 0.31018\n",
      "Epoch: 0173 train_loss= 0.42095 time= 0.31921\n",
      "Epoch: 0174 train_loss= 0.42077 time= 0.31725\n",
      "Epoch: 0175 train_loss= 0.42060 time= 0.29729\n",
      "Epoch: 0176 train_loss= 0.42043 time= 0.30586\n",
      "Epoch: 0177 train_loss= 0.42027 time= 0.31370\n",
      "Epoch: 0178 train_loss= 0.42011 time= 0.29497\n",
      "Epoch: 0179 train_loss= 0.41995 time= 0.30691\n",
      "Epoch: 0180 train_loss= 0.41980 time= 0.31635\n",
      "Epoch: 0181 train_loss= 0.41964 time= 0.31455\n",
      "Epoch: 0182 train_loss= 0.41950 time= 0.28126\n",
      "Epoch: 0183 train_loss= 0.41935 time= 0.30930\n",
      "Epoch: 0184 train_loss= 0.41921 time= 0.32640\n",
      "Epoch: 0185 train_loss= 0.41907 time= 0.30419\n",
      "Epoch: 0186 train_loss= 0.41894 time= 0.31949\n",
      "Epoch: 0187 train_loss= 0.41880 time= 0.30730\n",
      "Epoch: 0188 train_loss= 0.41867 time= 0.30392\n",
      "Epoch: 0189 train_loss= 0.41854 time= 0.30579\n",
      "Epoch: 0190 train_loss= 0.41841 time= 0.29387\n",
      "Epoch: 0191 train_loss= 0.41828 time= 0.32414\n",
      "Epoch: 0192 train_loss= 0.41816 time= 0.31283\n",
      "Epoch: 0193 train_loss= 0.41804 time= 0.29771\n",
      "Epoch: 0194 train_loss= 0.41792 time= 0.31888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0195 train_loss= 0.41780 time= 0.30149\n",
      "Epoch: 0196 train_loss= 0.41768 time= 0.30603\n",
      "Epoch: 0197 train_loss= 0.41757 time= 0.33411\n",
      "Epoch: 0198 train_loss= 0.41745 time= 0.31521\n",
      "Epoch: 0199 train_loss= 0.41734 time= 0.32135\n",
      "Epoch: 0200 train_loss= 0.41723 time= 0.30937\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84166 time= 0.48472\n",
      "Epoch: 0002 train_loss= 0.84065 time= 0.29672\n",
      "Epoch: 0003 train_loss= 0.83825 time= 0.30576\n",
      "Epoch: 0004 train_loss= 0.83371 time= 0.29373\n",
      "Epoch: 0005 train_loss= 0.82614 time= 0.29186\n",
      "Epoch: 0006 train_loss= 0.81466 time= 0.28943\n",
      "Epoch: 0007 train_loss= 0.79876 time= 0.31238\n",
      "Epoch: 0008 train_loss= 0.77880 time= 0.31163\n",
      "Epoch: 0009 train_loss= 0.75684 time= 0.29604\n",
      "Epoch: 0010 train_loss= 0.73748 time= 0.32414\n",
      "Epoch: 0011 train_loss= 0.72757 time= 0.28871\n",
      "Epoch: 0012 train_loss= 0.72903 time= 0.31799\n",
      "Epoch: 0013 train_loss= 0.73034 time= 0.29780\n",
      "Epoch: 0014 train_loss= 0.72367 time= 0.29839\n",
      "Epoch: 0015 train_loss= 0.71130 time= 0.30067\n",
      "Epoch: 0016 train_loss= 0.69785 time= 0.32340\n",
      "Epoch: 0017 train_loss= 0.68632 time= 0.30142\n",
      "Epoch: 0018 train_loss= 0.67729 time= 0.31520\n",
      "Epoch: 0019 train_loss= 0.66970 time= 0.31740\n",
      "Epoch: 0020 train_loss= 0.66203 time= 0.31416\n",
      "Epoch: 0021 train_loss= 0.65311 time= 0.30746\n",
      "Epoch: 0022 train_loss= 0.64242 time= 0.31636\n",
      "Epoch: 0023 train_loss= 0.63008 time= 0.30034\n",
      "Epoch: 0024 train_loss= 0.61663 time= 0.30348\n",
      "Epoch: 0025 train_loss= 0.60293 time= 0.29885\n",
      "Epoch: 0026 train_loss= 0.58991 time= 0.31800\n",
      "Epoch: 0027 train_loss= 0.57838 time= 0.29920\n",
      "Epoch: 0028 train_loss= 0.56879 time= 0.30780\n",
      "Epoch: 0029 train_loss= 0.56111 time= 0.30940\n",
      "Epoch: 0030 train_loss= 0.55487 time= 0.30038\n",
      "Epoch: 0031 train_loss= 0.54942 time= 0.29972\n",
      "Epoch: 0032 train_loss= 0.54419 time= 0.30274\n",
      "Epoch: 0033 train_loss= 0.53884 time= 0.30317\n",
      "Epoch: 0034 train_loss= 0.53331 time= 0.30219\n",
      "Epoch: 0035 train_loss= 0.52774 time= 0.30847\n",
      "Epoch: 0036 train_loss= 0.52228 time= 0.29664\n",
      "Epoch: 0037 train_loss= 0.51710 time= 0.29070\n",
      "Epoch: 0038 train_loss= 0.51229 time= 0.32034\n",
      "Epoch: 0039 train_loss= 0.50791 time= 0.30818\n",
      "Epoch: 0040 train_loss= 0.50401 time= 0.30319\n",
      "Epoch: 0041 train_loss= 0.50061 time= 0.30957\n",
      "Epoch: 0042 train_loss= 0.49767 time= 0.32215\n",
      "Epoch: 0043 train_loss= 0.49510 time= 0.29635\n",
      "Epoch: 0044 train_loss= 0.49277 time= 0.30142\n",
      "Epoch: 0045 train_loss= 0.49053 time= 0.29321\n",
      "Epoch: 0046 train_loss= 0.48833 time= 0.31709\n",
      "Epoch: 0047 train_loss= 0.48616 time= 0.30020\n",
      "Epoch: 0048 train_loss= 0.48406 time= 0.30510\n",
      "Epoch: 0049 train_loss= 0.48210 time= 0.31122\n",
      "Epoch: 0050 train_loss= 0.48028 time= 0.30478\n",
      "Epoch: 0051 train_loss= 0.47860 time= 0.29376\n",
      "Epoch: 0052 train_loss= 0.47704 time= 0.31216\n",
      "Epoch: 0053 train_loss= 0.47555 time= 0.31017\n",
      "Epoch: 0054 train_loss= 0.47412 time= 0.29689\n",
      "Epoch: 0055 train_loss= 0.47272 time= 0.31104\n",
      "Epoch: 0056 train_loss= 0.47136 time= 0.29721\n",
      "Epoch: 0057 train_loss= 0.47004 time= 0.29549\n",
      "Epoch: 0058 train_loss= 0.46875 time= 0.30450\n",
      "Epoch: 0059 train_loss= 0.46752 time= 0.32511\n",
      "Epoch: 0060 train_loss= 0.46633 time= 0.30874\n",
      "Epoch: 0061 train_loss= 0.46519 time= 0.30498\n",
      "Epoch: 0062 train_loss= 0.46408 time= 0.31510\n",
      "Epoch: 0063 train_loss= 0.46299 time= 0.29844\n",
      "Epoch: 0064 train_loss= 0.46192 time= 0.29733\n",
      "Epoch: 0065 train_loss= 0.46086 time= 0.30618\n",
      "Epoch: 0066 train_loss= 0.45980 time= 0.31915\n",
      "Epoch: 0067 train_loss= 0.45875 time= 0.31378\n",
      "Epoch: 0068 train_loss= 0.45772 time= 0.29605\n",
      "Epoch: 0069 train_loss= 0.45671 time= 0.30807\n",
      "Epoch: 0070 train_loss= 0.45573 time= 0.31001\n",
      "Epoch: 0071 train_loss= 0.45478 time= 0.31495\n",
      "Epoch: 0072 train_loss= 0.45386 time= 0.30127\n",
      "Epoch: 0073 train_loss= 0.45296 time= 0.29653\n",
      "Epoch: 0074 train_loss= 0.45209 time= 0.31056\n",
      "Epoch: 0075 train_loss= 0.45126 time= 0.31734\n",
      "Epoch: 0076 train_loss= 0.45045 time= 0.29326\n",
      "Epoch: 0077 train_loss= 0.44967 time= 0.31534\n",
      "Epoch: 0078 train_loss= 0.44891 time= 0.29721\n",
      "Epoch: 0079 train_loss= 0.44815 time= 0.31390\n",
      "Epoch: 0080 train_loss= 0.44740 time= 0.29662\n",
      "Epoch: 0081 train_loss= 0.44664 time= 0.31512\n",
      "Epoch: 0082 train_loss= 0.44590 time= 0.31067\n",
      "Epoch: 0083 train_loss= 0.44516 time= 0.29023\n",
      "Epoch: 0084 train_loss= 0.44443 time= 0.30830\n",
      "Epoch: 0085 train_loss= 0.44371 time= 0.31849\n",
      "Epoch: 0086 train_loss= 0.44301 time= 0.31675\n",
      "Epoch: 0087 train_loss= 0.44231 time= 0.31132\n",
      "Epoch: 0088 train_loss= 0.44162 time= 0.28750\n",
      "Epoch: 0089 train_loss= 0.44095 time= 0.30989\n",
      "Epoch: 0090 train_loss= 0.44028 time= 0.29800\n",
      "Epoch: 0091 train_loss= 0.43963 time= 0.30519\n",
      "Epoch: 0092 train_loss= 0.43899 time= 0.31865\n",
      "Epoch: 0093 train_loss= 0.43835 time= 0.30878\n",
      "Epoch: 0094 train_loss= 0.43773 time= 0.29552\n",
      "Epoch: 0095 train_loss= 0.43711 time= 0.31373\n",
      "Epoch: 0096 train_loss= 0.43649 time= 0.29610\n",
      "Epoch: 0097 train_loss= 0.43589 time= 0.30473\n",
      "Epoch: 0098 train_loss= 0.43529 time= 0.31774\n",
      "Epoch: 0099 train_loss= 0.43471 time= 0.31533\n",
      "Epoch: 0100 train_loss= 0.43414 time= 0.32909\n",
      "Epoch: 0101 train_loss= 0.43357 time= 0.30208\n",
      "Epoch: 0102 train_loss= 0.43302 time= 0.28507\n",
      "Epoch: 0103 train_loss= 0.43248 time= 0.32590\n",
      "Epoch: 0104 train_loss= 0.43196 time= 0.32114\n",
      "Epoch: 0105 train_loss= 0.43144 time= 0.29787\n",
      "Epoch: 0106 train_loss= 0.43094 time= 0.30696\n",
      "Epoch: 0107 train_loss= 0.43046 time= 0.30675\n",
      "Epoch: 0108 train_loss= 0.42999 time= 0.29989\n",
      "Epoch: 0109 train_loss= 0.42953 time= 0.31119\n",
      "Epoch: 0110 train_loss= 0.42908 time= 0.31345\n",
      "Epoch: 0111 train_loss= 0.42865 time= 0.31409\n",
      "Epoch: 0112 train_loss= 0.42823 time= 0.30484\n",
      "Epoch: 0113 train_loss= 0.42782 time= 0.30905\n",
      "Epoch: 0114 train_loss= 0.42742 time= 0.30355\n",
      "Epoch: 0115 train_loss= 0.42703 time= 0.29987\n",
      "Epoch: 0116 train_loss= 0.42665 time= 0.31664\n",
      "Epoch: 0117 train_loss= 0.42628 time= 0.31017\n",
      "Epoch: 0118 train_loss= 0.42592 time= 0.31615\n",
      "Epoch: 0119 train_loss= 0.42557 time= 0.30751\n",
      "Epoch: 0120 train_loss= 0.42524 time= 0.29856\n",
      "Epoch: 0121 train_loss= 0.42491 time= 0.31742\n",
      "Epoch: 0122 train_loss= 0.42459 time= 0.30574\n",
      "Epoch: 0123 train_loss= 0.42428 time= 0.30576\n",
      "Epoch: 0124 train_loss= 0.42398 time= 0.32014\n",
      "Epoch: 0125 train_loss= 0.42369 time= 0.30918\n",
      "Epoch: 0126 train_loss= 0.42341 time= 0.30630\n",
      "Epoch: 0127 train_loss= 0.42313 time= 0.30937\n",
      "Epoch: 0128 train_loss= 0.42287 time= 0.28026\n",
      "Epoch: 0129 train_loss= 0.42261 time= 0.30803\n",
      "Epoch: 0130 train_loss= 0.42236 time= 0.30917\n",
      "Epoch: 0131 train_loss= 0.42212 time= 0.29222\n",
      "Epoch: 0132 train_loss= 0.42189 time= 0.30875\n",
      "Epoch: 0133 train_loss= 0.42166 time= 0.32919\n",
      "Epoch: 0134 train_loss= 0.42144 time= 0.29964\n",
      "Epoch: 0135 train_loss= 0.42123 time= 0.28802\n",
      "Epoch: 0136 train_loss= 0.42103 time= 0.31041\n",
      "Epoch: 0137 train_loss= 0.42083 time= 0.32283\n",
      "Epoch: 0138 train_loss= 0.42064 time= 0.29780\n",
      "Epoch: 0139 train_loss= 0.42045 time= 0.29971\n",
      "Epoch: 0140 train_loss= 0.42027 time= 0.30822\n",
      "Epoch: 0141 train_loss= 0.42010 time= 0.29086\n",
      "Epoch: 0142 train_loss= 0.41993 time= 0.30431\n",
      "Epoch: 0143 train_loss= 0.41976 time= 0.31058\n",
      "Epoch: 0144 train_loss= 0.41960 time= 0.30119\n",
      "Epoch: 0145 train_loss= 0.41944 time= 0.30898\n",
      "Epoch: 0146 train_loss= 0.41928 time= 0.32836\n",
      "Epoch: 0147 train_loss= 0.41912 time= 0.30907\n",
      "Epoch: 0148 train_loss= 0.41897 time= 0.29992\n",
      "Epoch: 0149 train_loss= 0.41881 time= 0.29693\n",
      "Epoch: 0150 train_loss= 0.41866 time= 0.29371\n",
      "Epoch: 0151 train_loss= 0.41850 time= 0.30431\n",
      "Epoch: 0152 train_loss= 0.41835 time= 0.31086\n",
      "Epoch: 0153 train_loss= 0.41819 time= 0.32001\n",
      "Epoch: 0154 train_loss= 0.41804 time= 0.30112\n",
      "Epoch: 0155 train_loss= 0.41788 time= 0.29490\n",
      "Epoch: 0156 train_loss= 0.41773 time= 0.32314\n",
      "Epoch: 0157 train_loss= 0.41757 time= 0.32347\n",
      "Epoch: 0158 train_loss= 0.41742 time= 0.30869\n",
      "Epoch: 0159 train_loss= 0.41726 time= 0.31624\n",
      "Epoch: 0160 train_loss= 0.41710 time= 0.30592\n",
      "Epoch: 0161 train_loss= 0.41695 time= 0.32015\n",
      "Epoch: 0162 train_loss= 0.41679 time= 0.31922\n",
      "Epoch: 0163 train_loss= 0.41664 time= 0.31288\n",
      "Epoch: 0164 train_loss= 0.41648 time= 0.34231\n",
      "Epoch: 0165 train_loss= 0.41633 time= 0.32528\n",
      "Epoch: 0166 train_loss= 0.41618 time= 0.31691\n",
      "Epoch: 0167 train_loss= 0.41603 time= 0.31090\n",
      "Epoch: 0168 train_loss= 0.41588 time= 0.30727\n",
      "Epoch: 0169 train_loss= 0.41574 time= 0.33066\n",
      "Epoch: 0170 train_loss= 0.41559 time= 0.30632\n",
      "Epoch: 0171 train_loss= 0.41545 time= 0.31376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0172 train_loss= 0.41532 time= 0.31088\n",
      "Epoch: 0173 train_loss= 0.41518 time= 0.30963\n",
      "Epoch: 0174 train_loss= 0.41505 time= 0.31149\n",
      "Epoch: 0175 train_loss= 0.41492 time= 0.30822\n",
      "Epoch: 0176 train_loss= 0.41480 time= 0.31312\n",
      "Epoch: 0177 train_loss= 0.41467 time= 0.33083\n",
      "Epoch: 0178 train_loss= 0.41455 time= 0.31068\n",
      "Epoch: 0179 train_loss= 0.41443 time= 0.32218\n",
      "Epoch: 0180 train_loss= 0.41432 time= 0.31717\n",
      "Epoch: 0181 train_loss= 0.41420 time= 0.29745\n",
      "Epoch: 0182 train_loss= 0.41409 time= 0.29820\n",
      "Epoch: 0183 train_loss= 0.41399 time= 0.29755\n",
      "Epoch: 0184 train_loss= 0.41388 time= 0.30661\n",
      "Epoch: 0185 train_loss= 0.41378 time= 0.31468\n",
      "Epoch: 0186 train_loss= 0.41368 time= 0.29637\n",
      "Epoch: 0187 train_loss= 0.41358 time= 0.29588\n",
      "Epoch: 0188 train_loss= 0.41348 time= 0.31312\n",
      "Epoch: 0189 train_loss= 0.41339 time= 0.30187\n",
      "Epoch: 0190 train_loss= 0.41329 time= 0.33699\n",
      "Epoch: 0191 train_loss= 0.41320 time= 0.30645\n",
      "Epoch: 0192 train_loss= 0.41311 time= 0.30085\n",
      "Epoch: 0193 train_loss= 0.41302 time= 0.30103\n",
      "Epoch: 0194 train_loss= 0.41292 time= 0.30119\n",
      "Epoch: 0195 train_loss= 0.41283 time= 0.31516\n",
      "Epoch: 0196 train_loss= 0.41274 time= 0.30564\n",
      "Epoch: 0197 train_loss= 0.41265 time= 0.31118\n",
      "Epoch: 0198 train_loss= 0.41256 time= 0.31774\n",
      "Epoch: 0199 train_loss= 0.41247 time= 0.29686\n",
      "Epoch: 0200 train_loss= 0.41238 time= 0.32912\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84169 time= 0.48974\n",
      "Epoch: 0002 train_loss= 0.84075 time= 0.30307\n",
      "Epoch: 0003 train_loss= 0.83851 time= 0.28851\n",
      "Epoch: 0004 train_loss= 0.83429 time= 0.29721\n",
      "Epoch: 0005 train_loss= 0.82726 time= 0.29721\n",
      "Epoch: 0006 train_loss= 0.81658 time= 0.31119\n",
      "Epoch: 0007 train_loss= 0.80164 time= 0.30151\n",
      "Epoch: 0008 train_loss= 0.78244 time= 0.29745\n",
      "Epoch: 0009 train_loss= 0.76024 time= 0.32256\n",
      "Epoch: 0010 train_loss= 0.73823 time= 0.30618\n",
      "Epoch: 0011 train_loss= 0.72202 time= 0.29621\n",
      "Epoch: 0012 train_loss= 0.71686 time= 0.30015\n",
      "Epoch: 0013 train_loss= 0.71818 time= 0.31789\n",
      "Epoch: 0014 train_loss= 0.71475 time= 0.31312\n",
      "Epoch: 0015 train_loss= 0.70380 time= 0.29039\n",
      "Epoch: 0016 train_loss= 0.68875 time= 0.31263\n",
      "Epoch: 0017 train_loss= 0.67354 time= 0.29299\n",
      "Epoch: 0018 train_loss= 0.66032 time= 0.29920\n",
      "Epoch: 0019 train_loss= 0.64918 time= 0.30208\n",
      "Epoch: 0020 train_loss= 0.63899 time= 0.31014\n",
      "Epoch: 0021 train_loss= 0.62848 time= 0.30036\n",
      "Epoch: 0022 train_loss= 0.61690 time= 0.32161\n",
      "Epoch: 0023 train_loss= 0.60417 time= 0.29222\n",
      "Epoch: 0024 train_loss= 0.59077 time= 0.31628\n",
      "Epoch: 0025 train_loss= 0.57759 time= 0.29828\n",
      "Epoch: 0026 train_loss= 0.56564 time= 0.29942\n",
      "Epoch: 0027 train_loss= 0.55582 time= 0.30734\n",
      "Epoch: 0028 train_loss= 0.54856 time= 0.30512\n",
      "Epoch: 0029 train_loss= 0.54365 time= 0.30516\n",
      "Epoch: 0030 train_loss= 0.54028 time= 0.29021\n",
      "Epoch: 0031 train_loss= 0.53740 time= 0.30917\n",
      "Epoch: 0032 train_loss= 0.53418 time= 0.31466\n",
      "Epoch: 0033 train_loss= 0.53033 time= 0.29982\n",
      "Epoch: 0034 train_loss= 0.52596 time= 0.29052\n",
      "Epoch: 0035 train_loss= 0.52141 time= 0.31719\n",
      "Epoch: 0036 train_loss= 0.51699 time= 0.30696\n",
      "Epoch: 0037 train_loss= 0.51285 time= 0.29820\n",
      "Epoch: 0038 train_loss= 0.50897 time= 0.29920\n",
      "Epoch: 0039 train_loss= 0.50526 time= 0.30407\n",
      "Epoch: 0040 train_loss= 0.50166 time= 0.29926\n",
      "Epoch: 0041 train_loss= 0.49819 time= 0.30477\n",
      "Epoch: 0042 train_loss= 0.49490 time= 0.31017\n",
      "Epoch: 0043 train_loss= 0.49188 time= 0.30804\n",
      "Epoch: 0044 train_loss= 0.48917 time= 0.30560\n",
      "Epoch: 0045 train_loss= 0.48674 time= 0.29923\n",
      "Epoch: 0046 train_loss= 0.48456 time= 0.30177\n",
      "Epoch: 0047 train_loss= 0.48255 time= 0.29578\n",
      "Epoch: 0048 train_loss= 0.48063 time= 0.29772\n",
      "Epoch: 0049 train_loss= 0.47878 time= 0.30818\n",
      "Epoch: 0050 train_loss= 0.47698 time= 0.30319\n",
      "Epoch: 0051 train_loss= 0.47527 time= 0.32343\n",
      "Epoch: 0052 train_loss= 0.47365 time= 0.30442\n",
      "Epoch: 0053 train_loss= 0.47214 time= 0.30301\n",
      "Epoch: 0054 train_loss= 0.47074 time= 0.29387\n",
      "Epoch: 0055 train_loss= 0.46941 time= 0.29662\n",
      "Epoch: 0056 train_loss= 0.46813 time= 0.30718\n",
      "Epoch: 0057 train_loss= 0.46687 time= 0.32015\n",
      "Epoch: 0058 train_loss= 0.46562 time= 0.29292\n",
      "Epoch: 0059 train_loss= 0.46438 time= 0.29603\n",
      "Epoch: 0060 train_loss= 0.46316 time= 0.30978\n",
      "Epoch: 0061 train_loss= 0.46197 time= 0.31206\n",
      "Epoch: 0062 train_loss= 0.46080 time= 0.31237\n",
      "Epoch: 0063 train_loss= 0.45967 time= 0.29647\n",
      "Epoch: 0064 train_loss= 0.45858 time= 0.29756\n",
      "Epoch: 0065 train_loss= 0.45755 time= 0.31480\n",
      "Epoch: 0066 train_loss= 0.45658 time= 0.29193\n",
      "Epoch: 0067 train_loss= 0.45569 time= 0.31205\n",
      "Epoch: 0068 train_loss= 0.45486 time= 0.30466\n",
      "Epoch: 0069 train_loss= 0.45410 time= 0.30043\n",
      "Epoch: 0070 train_loss= 0.45339 time= 0.30171\n",
      "Epoch: 0071 train_loss= 0.45271 time= 0.31357\n",
      "Epoch: 0072 train_loss= 0.45205 time= 0.32784\n",
      "Epoch: 0073 train_loss= 0.45140 time= 0.30006\n",
      "Epoch: 0074 train_loss= 0.45077 time= 0.29750\n",
      "Epoch: 0075 train_loss= 0.45014 time= 0.30718\n",
      "Epoch: 0076 train_loss= 0.44952 time= 0.31815\n",
      "Epoch: 0077 train_loss= 0.44892 time= 0.29820\n",
      "Epoch: 0078 train_loss= 0.44833 time= 0.31525\n",
      "Epoch: 0079 train_loss= 0.44776 time= 0.31382\n",
      "Epoch: 0080 train_loss= 0.44720 time= 0.29886\n",
      "Epoch: 0081 train_loss= 0.44666 time= 0.29657\n",
      "Epoch: 0082 train_loss= 0.44613 time= 0.29820\n",
      "Epoch: 0083 train_loss= 0.44561 time= 0.29920\n",
      "Epoch: 0084 train_loss= 0.44510 time= 0.30479\n",
      "Epoch: 0085 train_loss= 0.44461 time= 0.31001\n",
      "Epoch: 0086 train_loss= 0.44413 time= 0.30529\n",
      "Epoch: 0087 train_loss= 0.44367 time= 0.30686\n",
      "Epoch: 0088 train_loss= 0.44321 time= 0.30020\n",
      "Epoch: 0089 train_loss= 0.44276 time= 0.30891\n",
      "Epoch: 0090 train_loss= 0.44232 time= 0.31482\n",
      "Epoch: 0091 train_loss= 0.44189 time= 0.30156\n",
      "Epoch: 0092 train_loss= 0.44148 time= 0.29884\n",
      "Epoch: 0093 train_loss= 0.44107 time= 0.30617\n",
      "Epoch: 0094 train_loss= 0.44067 time= 0.29133\n",
      "Epoch: 0095 train_loss= 0.44028 time= 0.29640\n",
      "Epoch: 0096 train_loss= 0.43989 time= 0.31870\n",
      "Epoch: 0097 train_loss= 0.43951 time= 0.30381\n",
      "Epoch: 0098 train_loss= 0.43914 time= 0.32807\n",
      "Epoch: 0099 train_loss= 0.43877 time= 0.28951\n",
      "Epoch: 0100 train_loss= 0.43840 time= 0.31232\n",
      "Epoch: 0101 train_loss= 0.43804 time= 0.29646\n",
      "Epoch: 0102 train_loss= 0.43767 time= 0.31015\n",
      "Epoch: 0103 train_loss= 0.43730 time= 0.29522\n",
      "Epoch: 0104 train_loss= 0.43693 time= 0.31625\n",
      "Epoch: 0105 train_loss= 0.43655 time= 0.30237\n",
      "Epoch: 0106 train_loss= 0.43618 time= 0.30970\n",
      "Epoch: 0107 train_loss= 0.43580 time= 0.30399\n",
      "Epoch: 0108 train_loss= 0.43541 time= 0.29521\n",
      "Epoch: 0109 train_loss= 0.43503 time= 0.29426\n",
      "Epoch: 0110 train_loss= 0.43464 time= 0.31117\n",
      "Epoch: 0111 train_loss= 0.43424 time= 0.30872\n",
      "Epoch: 0112 train_loss= 0.43385 time= 0.30734\n",
      "Epoch: 0113 train_loss= 0.43345 time= 0.30181\n",
      "Epoch: 0114 train_loss= 0.43305 time= 0.30032\n",
      "Epoch: 0115 train_loss= 0.43264 time= 0.29920\n",
      "Epoch: 0116 train_loss= 0.43224 time= 0.30519\n",
      "Epoch: 0117 train_loss= 0.43184 time= 0.30798\n",
      "Epoch: 0118 train_loss= 0.43143 time= 0.30278\n",
      "Epoch: 0119 train_loss= 0.43103 time= 0.29372\n",
      "Epoch: 0120 train_loss= 0.43062 time= 0.30165\n",
      "Epoch: 0121 train_loss= 0.43022 time= 0.31316\n",
      "Epoch: 0122 train_loss= 0.42981 time= 0.31715\n",
      "Epoch: 0123 train_loss= 0.42941 time= 0.31132\n",
      "Epoch: 0124 train_loss= 0.42901 time= 0.29348\n",
      "Epoch: 0125 train_loss= 0.42861 time= 0.30594\n",
      "Epoch: 0126 train_loss= 0.42821 time= 0.30942\n",
      "Epoch: 0127 train_loss= 0.42782 time= 0.29388\n",
      "Epoch: 0128 train_loss= 0.42744 time= 0.33150\n",
      "Epoch: 0129 train_loss= 0.42705 time= 0.30020\n",
      "Epoch: 0130 train_loss= 0.42668 time= 0.31971\n",
      "Epoch: 0131 train_loss= 0.42631 time= 0.30638\n",
      "Epoch: 0132 train_loss= 0.42595 time= 0.29930\n",
      "Epoch: 0133 train_loss= 0.42560 time= 0.29372\n",
      "Epoch: 0134 train_loss= 0.42526 time= 0.30518\n",
      "Epoch: 0135 train_loss= 0.42493 time= 0.30403\n",
      "Epoch: 0136 train_loss= 0.42461 time= 0.29820\n",
      "Epoch: 0137 train_loss= 0.42430 time= 0.30735\n",
      "Epoch: 0138 train_loss= 0.42400 time= 0.30095\n",
      "Epoch: 0139 train_loss= 0.42371 time= 0.30271\n",
      "Epoch: 0140 train_loss= 0.42344 time= 0.30846\n",
      "Epoch: 0141 train_loss= 0.42317 time= 0.31715\n",
      "Epoch: 0142 train_loss= 0.42292 time= 0.30917\n",
      "Epoch: 0143 train_loss= 0.42267 time= 0.31731\n",
      "Epoch: 0144 train_loss= 0.42244 time= 0.31636\n",
      "Epoch: 0145 train_loss= 0.42220 time= 0.30658\n",
      "Epoch: 0146 train_loss= 0.42198 time= 0.30094\n",
      "Epoch: 0147 train_loss= 0.42176 time= 0.29714\n",
      "Epoch: 0148 train_loss= 0.42154 time= 0.30318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0149 train_loss= 0.42133 time= 0.30639\n",
      "Epoch: 0150 train_loss= 0.42112 time= 0.30499\n",
      "Epoch: 0151 train_loss= 0.42091 time= 0.31030\n",
      "Epoch: 0152 train_loss= 0.42071 time= 0.30947\n",
      "Epoch: 0153 train_loss= 0.42050 time= 0.29442\n",
      "Epoch: 0154 train_loss= 0.42030 time= 0.30917\n",
      "Epoch: 0155 train_loss= 0.42010 time= 0.29421\n",
      "Epoch: 0156 train_loss= 0.41990 time= 0.30237\n",
      "Epoch: 0157 train_loss= 0.41970 time= 0.32089\n",
      "Epoch: 0158 train_loss= 0.41950 time= 0.28757\n",
      "Epoch: 0159 train_loss= 0.41930 time= 0.29409\n",
      "Epoch: 0160 train_loss= 0.41910 time= 0.31815\n",
      "Epoch: 0161 train_loss= 0.41890 time= 0.32413\n",
      "Epoch: 0162 train_loss= 0.41870 time= 0.31483\n",
      "Epoch: 0163 train_loss= 0.41850 time= 0.29904\n",
      "Epoch: 0164 train_loss= 0.41830 time= 0.29494\n",
      "Epoch: 0165 train_loss= 0.41811 time= 0.30580\n",
      "Epoch: 0166 train_loss= 0.41791 time= 0.29617\n",
      "Epoch: 0167 train_loss= 0.41772 time= 0.30318\n",
      "Epoch: 0168 train_loss= 0.41753 time= 0.30818\n",
      "Epoch: 0169 train_loss= 0.41734 time= 0.31639\n",
      "Epoch: 0170 train_loss= 0.41716 time= 0.31195\n",
      "Epoch: 0171 train_loss= 0.41698 time= 0.30292\n",
      "Epoch: 0172 train_loss= 0.41680 time= 0.31163\n",
      "Epoch: 0173 train_loss= 0.41663 time= 0.31816\n",
      "Epoch: 0174 train_loss= 0.41647 time= 0.30642\n",
      "Epoch: 0175 train_loss= 0.41630 time= 0.30718\n",
      "Epoch: 0176 train_loss= 0.41614 time= 0.30475\n",
      "Epoch: 0177 train_loss= 0.41599 time= 0.29438\n",
      "Epoch: 0178 train_loss= 0.41583 time= 0.29770\n",
      "Epoch: 0179 train_loss= 0.41568 time= 0.30420\n",
      "Epoch: 0180 train_loss= 0.41554 time= 0.30319\n",
      "Epoch: 0181 train_loss= 0.41539 time= 0.32039\n",
      "Epoch: 0182 train_loss= 0.41525 time= 0.29721\n",
      "Epoch: 0183 train_loss= 0.41511 time= 0.31552\n",
      "Epoch: 0184 train_loss= 0.41498 time= 0.30669\n",
      "Epoch: 0185 train_loss= 0.41485 time= 0.29953\n",
      "Epoch: 0186 train_loss= 0.41472 time= 0.30518\n",
      "Epoch: 0187 train_loss= 0.41459 time= 0.31516\n",
      "Epoch: 0188 train_loss= 0.41447 time= 0.29562\n",
      "Epoch: 0189 train_loss= 0.41435 time= 0.30938\n",
      "Epoch: 0190 train_loss= 0.41423 time= 0.29501\n",
      "Epoch: 0191 train_loss= 0.41411 time= 0.31591\n",
      "Epoch: 0192 train_loss= 0.41400 time= 0.28833\n",
      "Epoch: 0193 train_loss= 0.41389 time= 0.31116\n",
      "Epoch: 0194 train_loss= 0.41378 time= 0.32114\n",
      "Epoch: 0195 train_loss= 0.41368 time= 0.32034\n",
      "Epoch: 0196 train_loss= 0.41357 time= 0.31654\n",
      "Epoch: 0197 train_loss= 0.41347 time= 0.29558\n",
      "Epoch: 0198 train_loss= 0.41337 time= 0.29628\n",
      "Epoch: 0199 train_loss= 0.41327 time= 0.32136\n",
      "Epoch: 0200 train_loss= 0.41318 time= 0.31007\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84172 time= 0.52177\n",
      "Epoch: 0002 train_loss= 0.84098 time= 0.28586\n",
      "Epoch: 0003 train_loss= 0.83934 time= 0.29643\n",
      "Epoch: 0004 train_loss= 0.83638 time= 0.29215\n",
      "Epoch: 0005 train_loss= 0.83157 time= 0.30393\n",
      "Epoch: 0006 train_loss= 0.82430 time= 0.29464\n",
      "Epoch: 0007 train_loss= 0.81395 time= 0.29410\n",
      "Epoch: 0008 train_loss= 0.80002 time= 0.30169\n",
      "Epoch: 0009 train_loss= 0.78233 time= 0.31216\n",
      "Epoch: 0010 train_loss= 0.76142 time= 0.29023\n",
      "Epoch: 0011 train_loss= 0.73897 time= 0.30395\n",
      "Epoch: 0012 train_loss= 0.71824 time= 0.29254\n",
      "Epoch: 0013 train_loss= 0.70358 time= 0.30331\n",
      "Epoch: 0014 train_loss= 0.69689 time= 0.29390\n",
      "Epoch: 0015 train_loss= 0.69277 time= 0.29720\n",
      "Epoch: 0016 train_loss= 0.68408 time= 0.30108\n",
      "Epoch: 0017 train_loss= 0.66927 time= 0.30917\n",
      "Epoch: 0018 train_loss= 0.65070 time= 0.29165\n",
      "Epoch: 0019 train_loss= 0.63141 time= 0.29717\n",
      "Epoch: 0020 train_loss= 0.61339 time= 0.29263\n",
      "Epoch: 0021 train_loss= 0.59736 time= 0.30220\n",
      "Epoch: 0022 train_loss= 0.58316 time= 0.29670\n",
      "Epoch: 0023 train_loss= 0.57036 time= 0.30618\n",
      "Epoch: 0024 train_loss= 0.55866 time= 0.29893\n",
      "Epoch: 0025 train_loss= 0.54797 time= 0.30656\n",
      "Epoch: 0026 train_loss= 0.53826 time= 0.29114\n",
      "Epoch: 0027 train_loss= 0.52952 time= 0.30851\n",
      "Epoch: 0028 train_loss= 0.52183 time= 0.28404\n",
      "Epoch: 0029 train_loss= 0.51536 time= 0.31316\n",
      "Epoch: 0030 train_loss= 0.51017 time= 0.31901\n",
      "Epoch: 0031 train_loss= 0.50605 time= 0.31469\n",
      "Epoch: 0032 train_loss= 0.50253 time= 0.29724\n",
      "Epoch: 0033 train_loss= 0.49916 time= 0.29888\n",
      "Epoch: 0034 train_loss= 0.49573 time= 0.29549\n",
      "Epoch: 0035 train_loss= 0.49222 time= 0.30319\n",
      "Epoch: 0036 train_loss= 0.48879 time= 0.30818\n",
      "Epoch: 0037 train_loss= 0.48558 time= 0.30120\n",
      "Epoch: 0038 train_loss= 0.48274 time= 0.29539\n",
      "Epoch: 0039 train_loss= 0.48031 time= 0.29300\n",
      "Epoch: 0040 train_loss= 0.47823 time= 0.31115\n",
      "Epoch: 0041 train_loss= 0.47643 time= 0.31231\n",
      "Epoch: 0042 train_loss= 0.47479 time= 0.29308\n",
      "Epoch: 0043 train_loss= 0.47323 time= 0.29621\n",
      "Epoch: 0044 train_loss= 0.47168 time= 0.31635\n",
      "Epoch: 0045 train_loss= 0.47013 time= 0.29957\n",
      "Epoch: 0046 train_loss= 0.46855 time= 0.29840\n",
      "Epoch: 0047 train_loss= 0.46696 time= 0.30438\n",
      "Epoch: 0048 train_loss= 0.46537 time= 0.30022\n",
      "Epoch: 0049 train_loss= 0.46378 time= 0.30020\n",
      "Epoch: 0050 train_loss= 0.46219 time= 0.30013\n",
      "Epoch: 0051 train_loss= 0.46061 time= 0.31616\n",
      "Epoch: 0052 train_loss= 0.45906 time= 0.30787\n",
      "Epoch: 0053 train_loss= 0.45757 time= 0.29618\n",
      "Epoch: 0054 train_loss= 0.45620 time= 0.30271\n",
      "Epoch: 0055 train_loss= 0.45495 time= 0.30154\n",
      "Epoch: 0056 train_loss= 0.45385 time= 0.29725\n",
      "Epoch: 0057 train_loss= 0.45288 time= 0.29621\n",
      "Epoch: 0058 train_loss= 0.45200 time= 0.32206\n",
      "Epoch: 0059 train_loss= 0.45117 time= 0.30467\n",
      "Epoch: 0060 train_loss= 0.45037 time= 0.29338\n",
      "Epoch: 0061 train_loss= 0.44955 time= 0.29678\n",
      "Epoch: 0062 train_loss= 0.44873 time= 0.29721\n",
      "Epoch: 0063 train_loss= 0.44791 time= 0.30618\n",
      "Epoch: 0064 train_loss= 0.44709 time= 0.29759\n",
      "Epoch: 0065 train_loss= 0.44627 time= 0.31417\n",
      "Epoch: 0066 train_loss= 0.44546 time= 0.31168\n",
      "Epoch: 0067 train_loss= 0.44465 time= 0.29558\n",
      "Epoch: 0068 train_loss= 0.44385 time= 0.29920\n",
      "Epoch: 0069 train_loss= 0.44307 time= 0.32393\n",
      "Epoch: 0070 train_loss= 0.44232 time= 0.30770\n",
      "Epoch: 0071 train_loss= 0.44160 time= 0.31598\n",
      "Epoch: 0072 train_loss= 0.44092 time= 0.31406\n",
      "Epoch: 0073 train_loss= 0.44027 time= 0.30522\n",
      "Epoch: 0074 train_loss= 0.43965 time= 0.30205\n",
      "Epoch: 0075 train_loss= 0.43905 time= 0.31167\n",
      "Epoch: 0076 train_loss= 0.43847 time= 0.30020\n",
      "Epoch: 0077 train_loss= 0.43791 time= 0.29913\n",
      "Epoch: 0078 train_loss= 0.43737 time= 0.30775\n",
      "Epoch: 0079 train_loss= 0.43686 time= 0.30994\n",
      "Epoch: 0080 train_loss= 0.43636 time= 0.29324\n",
      "Epoch: 0081 train_loss= 0.43587 time= 0.29822\n",
      "Epoch: 0082 train_loss= 0.43540 time= 0.30641\n",
      "Epoch: 0083 train_loss= 0.43493 time= 0.30818\n",
      "Epoch: 0084 train_loss= 0.43448 time= 0.31376\n",
      "Epoch: 0085 train_loss= 0.43404 time= 0.29977\n",
      "Epoch: 0086 train_loss= 0.43362 time= 0.28765\n",
      "Epoch: 0087 train_loss= 0.43321 time= 0.30350\n",
      "Epoch: 0088 train_loss= 0.43281 time= 0.29751\n",
      "Epoch: 0089 train_loss= 0.43241 time= 0.30101\n",
      "Epoch: 0090 train_loss= 0.43203 time= 0.30726\n",
      "Epoch: 0091 train_loss= 0.43166 time= 0.30307\n",
      "Epoch: 0092 train_loss= 0.43129 time= 0.32393\n",
      "Epoch: 0093 train_loss= 0.43094 time= 0.30732\n",
      "Epoch: 0094 train_loss= 0.43059 time= 0.30004\n",
      "Epoch: 0095 train_loss= 0.43025 time= 0.31987\n",
      "Epoch: 0096 train_loss= 0.42992 time= 0.29352\n",
      "Epoch: 0097 train_loss= 0.42958 time= 0.30901\n",
      "Epoch: 0098 train_loss= 0.42925 time= 0.29969\n",
      "Epoch: 0099 train_loss= 0.42893 time= 0.29844\n",
      "Epoch: 0100 train_loss= 0.42861 time= 0.30933\n",
      "Epoch: 0101 train_loss= 0.42830 time= 0.30283\n",
      "Epoch: 0102 train_loss= 0.42799 time= 0.30448\n",
      "Epoch: 0103 train_loss= 0.42769 time= 0.30446\n",
      "Epoch: 0104 train_loss= 0.42739 time= 0.29948\n",
      "Epoch: 0105 train_loss= 0.42710 time= 0.30479\n",
      "Epoch: 0106 train_loss= 0.42681 time= 0.28877\n",
      "Epoch: 0107 train_loss= 0.42653 time= 0.31639\n",
      "Epoch: 0108 train_loss= 0.42625 time= 0.30020\n",
      "Epoch: 0109 train_loss= 0.42599 time= 0.30020\n",
      "Epoch: 0110 train_loss= 0.42573 time= 0.30180\n",
      "Epoch: 0111 train_loss= 0.42548 time= 0.30453\n",
      "Epoch: 0112 train_loss= 0.42523 time= 0.29197\n",
      "Epoch: 0113 train_loss= 0.42500 time= 0.29635\n",
      "Epoch: 0114 train_loss= 0.42477 time= 0.30718\n",
      "Epoch: 0115 train_loss= 0.42455 time= 0.32413\n",
      "Epoch: 0116 train_loss= 0.42434 time= 0.30997\n",
      "Epoch: 0117 train_loss= 0.42413 time= 0.29968\n",
      "Epoch: 0118 train_loss= 0.42393 time= 0.29583\n",
      "Epoch: 0119 train_loss= 0.42373 time= 0.29653\n",
      "Epoch: 0120 train_loss= 0.42353 time= 0.30663\n",
      "Epoch: 0121 train_loss= 0.42334 time= 0.32176\n",
      "Epoch: 0122 train_loss= 0.42315 time= 0.29720\n",
      "Epoch: 0123 train_loss= 0.42296 time= 0.31391\n",
      "Epoch: 0124 train_loss= 0.42277 time= 0.30831\n",
      "Epoch: 0125 train_loss= 0.42259 time= 0.29220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0126 train_loss= 0.42240 time= 0.29128\n",
      "Epoch: 0127 train_loss= 0.42222 time= 0.29920\n",
      "Epoch: 0128 train_loss= 0.42203 time= 0.30041\n",
      "Epoch: 0129 train_loss= 0.42184 time= 0.30421\n",
      "Epoch: 0130 train_loss= 0.42165 time= 0.31937\n",
      "Epoch: 0131 train_loss= 0.42145 time= 0.31006\n",
      "Epoch: 0132 train_loss= 0.42126 time= 0.31326\n",
      "Epoch: 0133 train_loss= 0.42106 time= 0.30083\n",
      "Epoch: 0134 train_loss= 0.42085 time= 0.30225\n",
      "Epoch: 0135 train_loss= 0.42064 time= 0.30718\n",
      "Epoch: 0136 train_loss= 0.42043 time= 0.31579\n",
      "Epoch: 0137 train_loss= 0.42022 time= 0.30782\n",
      "Epoch: 0138 train_loss= 0.42000 time= 0.29747\n",
      "Epoch: 0139 train_loss= 0.41977 time= 0.30163\n",
      "Epoch: 0140 train_loss= 0.41954 time= 0.31098\n",
      "Epoch: 0141 train_loss= 0.41931 time= 0.30419\n",
      "Epoch: 0142 train_loss= 0.41908 time= 0.29345\n",
      "Epoch: 0143 train_loss= 0.41884 time= 0.30219\n",
      "Epoch: 0144 train_loss= 0.41860 time= 0.29916\n",
      "Epoch: 0145 train_loss= 0.41836 time= 0.30118\n",
      "Epoch: 0146 train_loss= 0.41812 time= 0.31109\n",
      "Epoch: 0147 train_loss= 0.41788 time= 0.32086\n",
      "Epoch: 0148 train_loss= 0.41764 time= 0.29920\n",
      "Epoch: 0149 train_loss= 0.41740 time= 0.30285\n",
      "Epoch: 0150 train_loss= 0.41716 time= 0.29770\n",
      "Epoch: 0151 train_loss= 0.41692 time= 0.30623\n",
      "Epoch: 0152 train_loss= 0.41668 time= 0.29233\n",
      "Epoch: 0153 train_loss= 0.41645 time= 0.31216\n",
      "Epoch: 0154 train_loss= 0.41621 time= 0.30818\n",
      "Epoch: 0155 train_loss= 0.41599 time= 0.30341\n",
      "Epoch: 0156 train_loss= 0.41576 time= 0.30278\n",
      "Epoch: 0157 train_loss= 0.41554 time= 0.31163\n",
      "Epoch: 0158 train_loss= 0.41533 time= 0.28884\n",
      "Epoch: 0159 train_loss= 0.41512 time= 0.32266\n",
      "Epoch: 0160 train_loss= 0.41491 time= 0.30618\n",
      "Epoch: 0161 train_loss= 0.41471 time= 0.30355\n",
      "Epoch: 0162 train_loss= 0.41451 time= 0.29660\n",
      "Epoch: 0163 train_loss= 0.41433 time= 0.30075\n",
      "Epoch: 0164 train_loss= 0.41414 time= 0.30953\n",
      "Epoch: 0165 train_loss= 0.41397 time= 0.29357\n",
      "Epoch: 0166 train_loss= 0.41380 time= 0.30419\n",
      "Epoch: 0167 train_loss= 0.41363 time= 0.31080\n",
      "Epoch: 0168 train_loss= 0.41347 time= 0.29720\n",
      "Epoch: 0169 train_loss= 0.41332 time= 0.30607\n",
      "Epoch: 0170 train_loss= 0.41318 time= 0.29260\n",
      "Epoch: 0171 train_loss= 0.41303 time= 0.31080\n",
      "Epoch: 0172 train_loss= 0.41290 time= 0.29429\n",
      "Epoch: 0173 train_loss= 0.41277 time= 0.31217\n",
      "Epoch: 0174 train_loss= 0.41264 time= 0.31134\n",
      "Epoch: 0175 train_loss= 0.41251 time= 0.29521\n",
      "Epoch: 0176 train_loss= 0.41239 time= 0.30613\n",
      "Epoch: 0177 train_loss= 0.41226 time= 0.29802\n",
      "Epoch: 0178 train_loss= 0.41214 time= 0.29142\n",
      "Epoch: 0179 train_loss= 0.41202 time= 0.30476\n",
      "Epoch: 0180 train_loss= 0.41190 time= 0.30708\n",
      "Epoch: 0181 train_loss= 0.41177 time= 0.30219\n",
      "Epoch: 0182 train_loss= 0.41165 time= 0.32093\n",
      "Epoch: 0183 train_loss= 0.41153 time= 0.30965\n",
      "Epoch: 0184 train_loss= 0.41140 time= 0.29713\n",
      "Epoch: 0185 train_loss= 0.41128 time= 0.29705\n",
      "Epoch: 0186 train_loss= 0.41115 time= 0.31017\n",
      "Epoch: 0187 train_loss= 0.41102 time= 0.30688\n",
      "Epoch: 0188 train_loss= 0.41089 time= 0.30107\n",
      "Epoch: 0189 train_loss= 0.41077 time= 0.31495\n",
      "Epoch: 0190 train_loss= 0.41063 time= 0.30573\n",
      "Epoch: 0191 train_loss= 0.41050 time= 0.29050\n",
      "Epoch: 0192 train_loss= 0.41037 time= 0.30319\n",
      "Epoch: 0193 train_loss= 0.41024 time= 0.29222\n",
      "Epoch: 0194 train_loss= 0.41011 time= 0.30618\n",
      "Epoch: 0195 train_loss= 0.40998 time= 0.29886\n",
      "Epoch: 0196 train_loss= 0.40984 time= 0.30746\n",
      "Epoch: 0197 train_loss= 0.40971 time= 0.31067\n",
      "Epoch: 0198 train_loss= 0.40959 time= 0.29912\n",
      "Epoch: 0199 train_loss= 0.40946 time= 0.30356\n",
      "Epoch: 0200 train_loss= 0.40933 time= 0.30120\n",
      "Testing model...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.84172 time= 0.53167\n",
      "Epoch: 0002 train_loss= 0.84098 time= 0.30050\n",
      "Epoch: 0003 train_loss= 0.83940 time= 0.28693\n",
      "Epoch: 0004 train_loss= 0.83658 time= 0.29241\n",
      "Epoch: 0005 train_loss= 0.83203 time= 0.30076\n",
      "Epoch: 0006 train_loss= 0.82516 time= 0.29586\n",
      "Epoch: 0007 train_loss= 0.81538 time= 0.28449\n",
      "Epoch: 0008 train_loss= 0.80214 time= 0.33810\n",
      "Epoch: 0009 train_loss= 0.78516 time= 0.29534\n",
      "Epoch: 0010 train_loss= 0.76484 time= 0.29948\n",
      "Epoch: 0011 train_loss= 0.74272 time= 0.31051\n",
      "Epoch: 0012 train_loss= 0.72206 time= 0.30279\n",
      "Epoch: 0013 train_loss= 0.70737 time= 0.30201\n",
      "Epoch: 0014 train_loss= 0.70075 time= 0.32563\n",
      "Epoch: 0015 train_loss= 0.69684 time= 0.29321\n",
      "Epoch: 0016 train_loss= 0.68853 time= 0.31117\n",
      "Epoch: 0017 train_loss= 0.67419 time= 0.31548\n",
      "Epoch: 0018 train_loss= 0.65588 time= 0.30227\n",
      "Epoch: 0019 train_loss= 0.63635 time= 0.30779\n",
      "Epoch: 0020 train_loss= 0.61751 time= 0.31818\n",
      "Epoch: 0021 train_loss= 0.60015 time= 0.30993\n",
      "Epoch: 0022 train_loss= 0.58431 time= 0.32418\n",
      "Epoch: 0023 train_loss= 0.56976 time= 0.31692\n",
      "Epoch: 0024 train_loss= 0.55644 time= 0.31645\n",
      "Epoch: 0025 train_loss= 0.54455 time= 0.30950\n",
      "Epoch: 0026 train_loss= 0.53441 time= 0.29874\n",
      "Epoch: 0027 train_loss= 0.52619 time= 0.31903\n",
      "Epoch: 0028 train_loss= 0.51971 time= 0.29336\n",
      "Epoch: 0029 train_loss= 0.51440 time= 0.29321\n",
      "Epoch: 0030 train_loss= 0.50959 time= 0.30583\n",
      "Epoch: 0031 train_loss= 0.50488 time= 0.31556\n",
      "Epoch: 0032 train_loss= 0.50022 time= 0.31009\n",
      "Epoch: 0033 train_loss= 0.49578 time= 0.31256\n",
      "Epoch: 0034 train_loss= 0.49176 time= 0.31354\n",
      "Epoch: 0035 train_loss= 0.48830 time= 0.31989\n",
      "Epoch: 0036 train_loss= 0.48535 time= 0.30152\n",
      "Epoch: 0037 train_loss= 0.48276 time= 0.31078\n",
      "Epoch: 0038 train_loss= 0.48039 time= 0.30575\n",
      "Epoch: 0039 train_loss= 0.47810 time= 0.30319\n",
      "Epoch: 0040 train_loss= 0.47584 time= 0.29521\n",
      "Epoch: 0041 train_loss= 0.47362 time= 0.32237\n",
      "Epoch: 0042 train_loss= 0.47145 time= 0.31904\n",
      "Epoch: 0043 train_loss= 0.46935 time= 0.30665\n",
      "Epoch: 0044 train_loss= 0.46734 time= 0.31206\n",
      "Epoch: 0045 train_loss= 0.46540 time= 0.31199\n",
      "Epoch: 0046 train_loss= 0.46351 time= 0.29415\n",
      "Epoch: 0047 train_loss= 0.46167 time= 0.29920\n",
      "Epoch: 0048 train_loss= 0.45988 time= 0.30507\n",
      "Epoch: 0049 train_loss= 0.45819 time= 0.31130\n",
      "Epoch: 0050 train_loss= 0.45662 time= 0.29789\n",
      "Epoch: 0051 train_loss= 0.45520 time= 0.32021\n",
      "Epoch: 0052 train_loss= 0.45395 time= 0.29330\n",
      "Epoch: 0053 train_loss= 0.45284 time= 0.31184\n",
      "Epoch: 0054 train_loss= 0.45186 time= 0.29870\n",
      "Epoch: 0055 train_loss= 0.45095 time= 0.31316\n",
      "Epoch: 0056 train_loss= 0.45009 time= 0.30835\n",
      "Epoch: 0057 train_loss= 0.44925 time= 0.30553\n",
      "Epoch: 0058 train_loss= 0.44843 time= 0.30439\n",
      "Epoch: 0059 train_loss= 0.44765 time= 0.32349\n",
      "Epoch: 0060 train_loss= 0.44690 time= 0.29221\n",
      "Epoch: 0061 train_loss= 0.44619 time= 0.29622\n",
      "Epoch: 0062 train_loss= 0.44551 time= 0.32299\n",
      "Epoch: 0063 train_loss= 0.44485 time= 0.30226\n",
      "Epoch: 0064 train_loss= 0.44418 time= 0.29782\n",
      "Epoch: 0065 train_loss= 0.44353 time= 0.29704\n",
      "Epoch: 0066 train_loss= 0.44289 time= 0.30173\n",
      "Epoch: 0067 train_loss= 0.44229 time= 0.32654\n",
      "Epoch: 0068 train_loss= 0.44172 time= 0.30219\n",
      "Epoch: 0069 train_loss= 0.44119 time= 0.30058\n",
      "Epoch: 0070 train_loss= 0.44070 time= 0.30527\n",
      "Epoch: 0071 train_loss= 0.44025 time= 0.28554\n",
      "Epoch: 0072 train_loss= 0.43981 time= 0.30982\n",
      "Epoch: 0073 train_loss= 0.43940 time= 0.29128\n",
      "Epoch: 0074 train_loss= 0.43900 time= 0.30612\n",
      "Epoch: 0075 train_loss= 0.43861 time= 0.31823\n",
      "Epoch: 0076 train_loss= 0.43824 time= 0.30631\n",
      "Epoch: 0077 train_loss= 0.43788 time= 0.29979\n",
      "Epoch: 0078 train_loss= 0.43753 time= 0.28853\n",
      "Epoch: 0079 train_loss= 0.43718 time= 0.30425\n",
      "Epoch: 0080 train_loss= 0.43684 time= 0.30120\n",
      "Epoch: 0081 train_loss= 0.43651 time= 0.31134\n",
      "Epoch: 0082 train_loss= 0.43618 time= 0.30874\n",
      "Epoch: 0083 train_loss= 0.43587 time= 0.29743\n",
      "Epoch: 0084 train_loss= 0.43556 time= 0.30128\n",
      "Epoch: 0085 train_loss= 0.43527 time= 0.31052\n",
      "Epoch: 0086 train_loss= 0.43499 time= 0.31017\n",
      "Epoch: 0087 train_loss= 0.43471 time= 0.31776\n",
      "Epoch: 0088 train_loss= 0.43444 time= 0.30224\n",
      "Epoch: 0089 train_loss= 0.43416 time= 0.29813\n",
      "Epoch: 0090 train_loss= 0.43390 time= 0.31449\n",
      "Epoch: 0091 train_loss= 0.43363 time= 0.28807\n",
      "Epoch: 0092 train_loss= 0.43338 time= 0.30640\n",
      "Epoch: 0093 train_loss= 0.43313 time= 0.31690\n",
      "Epoch: 0094 train_loss= 0.43288 time= 0.31117\n",
      "Epoch: 0095 train_loss= 0.43264 time= 0.29231\n",
      "Epoch: 0096 train_loss= 0.43240 time= 0.29652\n",
      "Epoch: 0097 train_loss= 0.43217 time= 0.30769\n",
      "Epoch: 0098 train_loss= 0.43194 time= 0.29916\n",
      "Epoch: 0099 train_loss= 0.43172 time= 0.30219\n",
      "Epoch: 0100 train_loss= 0.43149 time= 0.29720\n",
      "Epoch: 0101 train_loss= 0.43127 time= 0.30917\n",
      "Epoch: 0102 train_loss= 0.43104 time= 0.30756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0103 train_loss= 0.43082 time= 0.31140\n",
      "Epoch: 0104 train_loss= 0.43060 time= 0.29582\n",
      "Epoch: 0105 train_loss= 0.43038 time= 0.30467\n",
      "Epoch: 0106 train_loss= 0.43015 time= 0.30353\n",
      "Epoch: 0107 train_loss= 0.42993 time= 0.30901\n",
      "Epoch: 0108 train_loss= 0.42970 time= 0.31617\n",
      "Epoch: 0109 train_loss= 0.42947 time= 0.29641\n",
      "Epoch: 0110 train_loss= 0.42924 time= 0.30174\n",
      "Epoch: 0111 train_loss= 0.42901 time= 0.31407\n",
      "Epoch: 0112 train_loss= 0.42878 time= 0.30319\n",
      "Epoch: 0113 train_loss= 0.42855 time= 0.29920\n",
      "Epoch: 0114 train_loss= 0.42831 time= 0.31479\n",
      "Epoch: 0115 train_loss= 0.42808 time= 0.30512\n",
      "Epoch: 0116 train_loss= 0.42785 time= 0.30241\n",
      "Epoch: 0117 train_loss= 0.42762 time= 0.30291\n",
      "Epoch: 0118 train_loss= 0.42738 time= 0.29424\n",
      "Epoch: 0119 train_loss= 0.42715 time= 0.32994\n",
      "Epoch: 0120 train_loss= 0.42692 time= 0.29521\n",
      "Epoch: 0121 train_loss= 0.42669 time= 0.30732\n",
      "Epoch: 0122 train_loss= 0.42646 time= 0.33413\n",
      "Epoch: 0123 train_loss= 0.42622 time= 0.29880\n",
      "Epoch: 0124 train_loss= 0.42599 time= 0.30901\n",
      "Epoch: 0125 train_loss= 0.42576 time= 0.29421\n",
      "Epoch: 0126 train_loss= 0.42553 time= 0.31376\n",
      "Epoch: 0127 train_loss= 0.42529 time= 0.31516\n",
      "Epoch: 0128 train_loss= 0.42505 time= 0.29890\n",
      "Epoch: 0129 train_loss= 0.42481 time= 0.31074\n",
      "Epoch: 0130 train_loss= 0.42457 time= 0.30990\n",
      "Epoch: 0131 train_loss= 0.42432 time= 0.29969\n",
      "Epoch: 0132 train_loss= 0.42407 time= 0.30418\n",
      "Epoch: 0133 train_loss= 0.42381 time= 0.31018\n",
      "Epoch: 0134 train_loss= 0.42355 time= 0.29446\n",
      "Epoch: 0135 train_loss= 0.42329 time= 0.31472\n",
      "Epoch: 0136 train_loss= 0.42302 time= 0.30360\n",
      "Epoch: 0137 train_loss= 0.42275 time= 0.29182\n",
      "Epoch: 0138 train_loss= 0.42248 time= 0.32313\n",
      "Epoch: 0139 train_loss= 0.42221 time= 0.30005\n",
      "Epoch: 0140 train_loss= 0.42193 time= 0.29472\n",
      "Epoch: 0141 train_loss= 0.42166 time= 0.30134\n",
      "Epoch: 0142 train_loss= 0.42138 time= 0.30869\n",
      "Epoch: 0143 train_loss= 0.42110 time= 0.29284\n",
      "Epoch: 0144 train_loss= 0.42083 time= 0.29841\n",
      "Epoch: 0145 train_loss= 0.42055 time= 0.30618\n",
      "Epoch: 0146 train_loss= 0.42028 time= 0.30668\n",
      "Epoch: 0147 train_loss= 0.42001 time= 0.30754\n",
      "Epoch: 0148 train_loss= 0.41974 time= 0.30735\n",
      "Epoch: 0149 train_loss= 0.41947 time= 0.31870\n",
      "Epoch: 0150 train_loss= 0.41920 time= 0.30359\n",
      "Epoch: 0151 train_loss= 0.41894 time= 0.31038\n",
      "Epoch: 0152 train_loss= 0.41868 time= 0.30979\n",
      "Epoch: 0153 train_loss= 0.41843 time= 0.30330\n",
      "Epoch: 0154 train_loss= 0.41818 time= 0.29652\n",
      "Epoch: 0155 train_loss= 0.41793 time= 0.28387\n",
      "Epoch: 0156 train_loss= 0.41769 time= 0.27956\n",
      "Epoch: 0157 train_loss= 0.41745 time= 0.27589\n",
      "Epoch: 0158 train_loss= 0.41722 time= 0.28663\n",
      "Epoch: 0159 train_loss= 0.41699 time= 0.29421\n",
      "Epoch: 0160 train_loss= 0.41677 time= 0.28846\n",
      "Epoch: 0161 train_loss= 0.41656 time= 0.28479\n",
      "Epoch: 0162 train_loss= 0.41635 time= 0.27869\n",
      "Epoch: 0163 train_loss= 0.41614 time= 0.29059\n",
      "Epoch: 0164 train_loss= 0.41594 time= 0.30167\n",
      "Epoch: 0165 train_loss= 0.41575 time= 0.28770\n",
      "Epoch: 0166 train_loss= 0.41556 time= 0.29477\n",
      "Epoch: 0167 train_loss= 0.41538 time= 0.29864\n",
      "Epoch: 0168 train_loss= 0.41520 time= 0.28002\n",
      "Epoch: 0169 train_loss= 0.41503 time= 0.41075\n",
      "Epoch: 0170 train_loss= 0.41485 time= 0.40098\n",
      "Epoch: 0171 train_loss= 0.41469 time= 0.39328\n",
      "Epoch: 0172 train_loss= 0.41453 time= 0.30610\n",
      "Epoch: 0173 train_loss= 0.41437 time= 0.28905\n",
      "Epoch: 0174 train_loss= 0.41421 time= 0.36956\n",
      "Epoch: 0175 train_loss= 0.41407 time= 0.39594\n",
      "Epoch: 0176 train_loss= 0.41392 time= 0.38234\n",
      "Epoch: 0177 train_loss= 0.41378 time= 0.32855\n",
      "Epoch: 0178 train_loss= 0.41365 time= 0.29107\n",
      "Epoch: 0179 train_loss= 0.41352 time= 0.28388\n",
      "Epoch: 0180 train_loss= 0.41340 time= 0.29366\n",
      "Epoch: 0181 train_loss= 0.41328 time= 0.32244\n",
      "Epoch: 0182 train_loss= 0.41316 time= 0.28361\n",
      "Epoch: 0183 train_loss= 0.41305 time= 0.28415\n",
      "Epoch: 0184 train_loss= 0.41294 time= 0.29719\n",
      "Epoch: 0185 train_loss= 0.41284 time= 0.29779\n",
      "Epoch: 0186 train_loss= 0.41274 time= 0.28437\n",
      "Epoch: 0187 train_loss= 0.41264 time= 0.29124\n",
      "Epoch: 0188 train_loss= 0.41254 time= 0.28881\n",
      "Epoch: 0189 train_loss= 0.41245 time= 0.30704\n",
      "Epoch: 0190 train_loss= 0.41235 time= 0.28130\n",
      "Epoch: 0191 train_loss= 0.41226 time= 0.27925\n",
      "Epoch: 0192 train_loss= 0.41217 time= 0.29621\n",
      "Epoch: 0193 train_loss= 0.41208 time= 0.28424\n",
      "Epoch: 0194 train_loss= 0.41199 time= 0.29584\n",
      "Epoch: 0195 train_loss= 0.41190 time= 0.28738\n",
      "Epoch: 0196 train_loss= 0.41182 time= 0.28389\n",
      "Epoch: 0197 train_loss= 0.41173 time= 0.28723\n",
      "Epoch: 0198 train_loss= 0.41164 time= 0.28775\n",
      "Epoch: 0199 train_loss= 0.41156 time= 0.29124\n",
      "Epoch: 0200 train_loss= 0.41147 time= 0.29000\n",
      "Testing model...\n"
     ]
    }
   ],
   "source": [
    "# The entire training+test process is repeated FLAGS.nb_run times\n",
    "for i in range(FLAGS.nb_run):\n",
    "\n",
    "    if FLAGS.task == 'link_prediction' :\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Masking test edges...\")\n",
    "        # Edge Masking for Link Prediction: compute Train/Validation/Test set\n",
    "        adj, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "        mask_test_edges(adj_init, FLAGS.prop_test, FLAGS.prop_val)\n",
    "\n",
    "    # Start computation of running times\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Degeneracy Framework / K-Core Decomposition\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Starting k-core decomposition of the graph\")\n",
    "        # Save adjacency matrix of un-decomposed graph\n",
    "        # (needed to embed nodes that are not in k-core, after GAE training)\n",
    "        adj_orig = adj\n",
    "        # Get the (smaller) adjacency matrix of the k-core subgraph,\n",
    "        # and the corresponding nodes\n",
    "        adj, nodes_kcore = compute_kcore(adj, FLAGS.k)\n",
    "        # Get the (smaller) feature matrix of the nb_core graph\n",
    "        if FLAGS.features:\n",
    "            features = features_init[nodes_kcore,:]\n",
    "        # Flag to compute k-core decomposition's running time\n",
    "        t_core = time.time()\n",
    "    elif FLAGS.features:\n",
    "        features = features_init\n",
    "\n",
    "    # Preprocessing and initialization\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Preprocessing and Initializing...\")\n",
    "    # Compute number of nodes\n",
    "    num_nodes = adj.shape[0]\n",
    "    # If features are not used, replace feature matrix by identity matrix\n",
    "    if not FLAGS.features:\n",
    "        features = sp.identity(adj.shape[0])\n",
    "    # Preprocessing on node features\n",
    "    features = sparse_to_tuple(features)\n",
    "    num_features = features[2][1]\n",
    "    features_nonzero = features[1].shape[0]\n",
    "\n",
    "    # Define placeholders\n",
    "    placeholders = {\n",
    "        'features': tf.sparse_placeholder(tf.float32),\n",
    "        'adj': tf.sparse_placeholder(tf.float32),\n",
    "        'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "        'dropout': tf.placeholder_with_default(0., shape = ())\n",
    "    }\n",
    "\n",
    "    # Create model\n",
    "    model = None\n",
    "    if FLAGS.model == 'gcn_ae':\n",
    "        # Standard Graph Autoencoder\n",
    "        model = GCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'gcn_vae':\n",
    "        # Standard Graph Variational Autoencoder\n",
    "        model = GCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                            features_nonzero)\n",
    "    elif FLAGS.model == 'linear_ae':\n",
    "        # Linear Graph Autoencoder\n",
    "        model = LinearModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'linear_vae':\n",
    "        # Linear Graph Variational Autoencoder\n",
    "        model = LinearModelVAE(placeholders, num_features, num_nodes,\n",
    "                               features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_ae':\n",
    "        # Deep (3-layer GCN) Graph Autoencoder\n",
    "        model = DeepGCNModelAE(placeholders, num_features, features_nonzero)\n",
    "    elif FLAGS.model == 'deep_gcn_vae':\n",
    "        # Deep (3-layer GCN) Graph Variational Autoencoder\n",
    "        model = DeepGCNModelVAE(placeholders, num_features, num_nodes,\n",
    "                                features_nonzero)\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "\n",
    "    # Optimizer\n",
    "    pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0]\n",
    "                                                - adj.sum()) * 2)\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # Optimizer for Non-Variational Autoencoders\n",
    "        if FLAGS.model in ('gcn_ae', 'linear_ae', 'deep_gcn_ae'):\n",
    "            opt = OptimizerAE(preds = model.reconstructions,\n",
    "                              labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                            validate_indices = False), [-1]),\n",
    "                              pos_weight = pos_weight,\n",
    "                              norm = norm)\n",
    "        # Optimizer for Variational Autoencoders\n",
    "        elif FLAGS.model in ('gcn_vae', 'linear_vae', 'deep_gcn_vae'):\n",
    "            opt = OptimizerVAE(preds = model.reconstructions,\n",
    "                               labels = tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'],\n",
    "                                                                             validate_indices = False), [-1]),\n",
    "                               model = model,\n",
    "                               num_nodes = num_nodes,\n",
    "                               pos_weight = pos_weight,\n",
    "                               norm = norm)\n",
    "\n",
    "    # Normalization and preprocessing on adjacency matrix\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    adj_label = sparse_to_tuple(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    # Initialize TF session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Model training\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Training...\")\n",
    "\n",
    "    for epoch in range(FLAGS.epochs):\n",
    "        # Flag to compute running time for each epoch\n",
    "        t = time.time()\n",
    "        # Construct feed dictionary\n",
    "        feed_dict = construct_feed_dict(adj_norm, adj_label, features,\n",
    "                                        placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        # Weights update\n",
    "        outs = sess.run([opt.opt_op, opt.cost, opt.accuracy],\n",
    "                        feed_dict = feed_dict)\n",
    "        # Compute average loss\n",
    "        avg_cost = outs[1]\n",
    "        if FLAGS.verbose:\n",
    "            # Display epoch information\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "                  \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "            # Validation, for Link Prediction\n",
    "            if not FLAGS.kcore and FLAGS.validation and FLAGS.task == 'link_prediction':\n",
    "                feed_dict.update({placeholders['dropout']: 0})\n",
    "                emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "                feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "                val_roc, val_ap = get_roc_score(val_edges, val_edges_false, emb)\n",
    "                print(\"val_roc=\", \"{:.5f}\".format(val_roc), \"val_ap=\", \"{:.5f}\".format(val_ap))\n",
    "\n",
    "    # Flag to compute Graph AE/VAE training time\n",
    "    t_model = time.time()\n",
    "\n",
    "    # Compute embedding\n",
    "\n",
    "    # Get embedding from model\n",
    "    emb = sess.run(model.z_mean, feed_dict = feed_dict)\n",
    "\n",
    "    # If k-core is used, only part of the nodes from the original\n",
    "    # graph are embedded. The remaining ones are projected in the\n",
    "    # latent space via the expand_embedding heuristic\n",
    "    if FLAGS.kcore:\n",
    "        if FLAGS.verbose:\n",
    "            print(\"Propagation to remaining nodes...\")\n",
    "        # Project remaining nodes in latent space\n",
    "        emb = expand_embedding(adj_orig, emb, nodes_kcore, FLAGS.nb_iterations)\n",
    "        # Compute mean running times for K-Core, GAE Train and Propagation steps\n",
    "        mean_time_expand.append(time.time() - t_model)\n",
    "        mean_time_train.append(t_model - t_core)\n",
    "        mean_time_kcore.append(t_core - t_start)\n",
    "        # Compute mean size of K-Core graph\n",
    "        # Note: size is fixed if task is node clustering, but will vary if\n",
    "        # task is link prediction due to edge masking\n",
    "        mean_core_size.append(len(nodes_kcore))\n",
    "\n",
    "    # Compute mean total running time\n",
    "    mean_time.append(time.time() - t_start)\n",
    "\n",
    "    # Test model\n",
    "    if FLAGS.verbose:\n",
    "        print(\"Testing model...\")\n",
    "    # Link Prediction: classification edges/non-edges\n",
    "    if FLAGS.task == 'link_prediction':\n",
    "        # Get ROC and AP scores\n",
    "        ap_score, roc_score = get_roc_score(test_edges, test_edges_false, emb)\n",
    "        # Report scores\n",
    "        mean_ap.append(ap_score)\n",
    "        mean_roc.append(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80215135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:56:15.160483Z",
     "start_time": "2022-05-08T13:56:15.146894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results for gcn_ae model on citeseer on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AP scores\n",
      " [0.8316411502813426, 0.8575565952550963, 0.8280541205613199, 0.8461098428691949, 0.8323484746913752, 0.846510282866165, 0.8333283590220779, 0.8298961679076655, 0.8121904210136766, 0.8452024107334349]\n",
      "Mean AP score:  0.836283782520135 \n",
      "Std of AP scores:  0.012116111162884972 \n",
      " \n",
      "\n",
      "AUC scores\n",
      " [0.7719381717183915, 0.8073807511170148, 0.769803163869098, 0.7898949402246106, 0.7664774785653907, 0.7935756551141167, 0.7795000603791813, 0.7671150827194783, 0.7397826349474701, 0.7910783721772733]\n",
      "Mean AUC score:  0.7776546310832025 \n",
      "Std of AUC scores:  0.01798658159597936 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Report Final Results ######\n",
    "\n",
    "# Report final results\n",
    "print(\"\\nTest results for\", FLAGS.model,\n",
    "      \"model on\", FLAGS.dataset, \"on\", FLAGS.task, \"\\n\",\n",
    "      \"___________________________________________________\\n\")\n",
    "\n",
    "if FLAGS.task == 'link_prediction':\n",
    "    print(\"AP scores\\n\", mean_ap)\n",
    "    print(\"Mean AP score: \", np.mean(mean_ap),\n",
    "          \"\\nStd of AP scores: \", np.std(mean_ap), \"\\n \\n\")\n",
    "    \n",
    "    print(\"AUC scores\\n\", mean_roc)\n",
    "    print(\"Mean AUC score: \", np.mean(mean_roc),\n",
    "          \"\\nStd of AUC scores: \", np.std(mean_roc), \"\\n \\n\")\n",
    "\n",
    "if FLAGS.kcore:\n",
    "    print(\"Details on degeneracy framework, with k =\", FLAGS.k, \": \\n \\n\")\n",
    "\n",
    "    print(\"Running times for k-core decomposition\\n\", mean_time_kcore)\n",
    "    print(\"Mean: \", np.mean(mean_time_kcore),\n",
    "          \"\\nStd: \", np.std(mean_time_kcore), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for autoencoder training\\n\", mean_time_train)\n",
    "    print(\"Mean: \", np.mean(mean_time_train),\n",
    "          \"\\nStd: \", np.std(mean_time_train), \"\\n \\n\")\n",
    "\n",
    "    print(\"Running times for propagation\\n\", mean_time_expand)\n",
    "    print(\"Mean: \", np.mean(mean_time_expand),\n",
    "          \"\\nStd: \", np.std(mean_time_expand), \"\\n \\n\")\n",
    "\n",
    "    print(\"Sizes of k-core subgraph\\n\", mean_core_size)\n",
    "    print(\"Mean: \", np.mean(mean_core_size),\n",
    "          \"\\nStd: \", np.std(mean_core_size), \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01df94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
