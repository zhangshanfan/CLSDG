{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65775001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:12:20.288403Z",
     "start_time": "2022-05-08T13:12:08.800159Z"
    }
   },
   "outputs": [],
   "source": [
    "import settings\n",
    "from link_prediction import Link_pred_Runner\n",
    "dataname = 'citeseer'     \n",
    "model = 'arga_vae'     # 'arga_ae' or 'arga_vae'\n",
    "task = 'link_prediction'         # 'clustering' or 'link_prediction'\n",
    "settings = settings.get_settings(dataname, model, task)\n",
    "runner = Link_pred_Runner(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254bd93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T13:19:29.395370Z",
     "start_time": "2022-05-08T13:12:22.043576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:143: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:145: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:147: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:27: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:126: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\constructor.py:94: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\.conda\\envs\\tf1.x\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:67: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:68: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:75: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 2.37881 val_roc= 0.55316 val_ap= 0.59231\n",
      "Epoch: 0002 train_loss= 2.37709 val_roc= 0.56463 val_ap= 0.60142\n",
      "Epoch: 0003 train_loss= 2.30156 val_roc= 0.58321 val_ap= 0.61514\n",
      "Epoch: 0004 train_loss= 2.31442 val_roc= 0.60363 val_ap= 0.62957\n",
      "Epoch: 0005 train_loss= 2.32894 val_roc= 0.62533 val_ap= 0.64769\n",
      "Epoch: 0006 train_loss= 2.33406 val_roc= 0.64274 val_ap= 0.66392\n",
      "Epoch: 0007 train_loss= 2.32947 val_roc= 0.65885 val_ap= 0.68324\n",
      "Epoch: 0008 train_loss= 2.29342 val_roc= 0.67551 val_ap= 0.70592\n",
      "Epoch: 0009 train_loss= 2.30711 val_roc= 0.68769 val_ap= 0.72512\n",
      "Epoch: 0010 train_loss= 2.31966 val_roc= 0.69877 val_ap= 0.74299\n",
      "Test AP score: 0.7360353237690012\n",
      "Test ROC score: 0.7128994082840238\n",
      "Epoch: 0011 train_loss= 2.30305 val_roc= 0.70965 val_ap= 0.75640\n",
      "Epoch: 0012 train_loss= 2.28570 val_roc= 0.71534 val_ap= 0.76239\n",
      "Epoch: 0013 train_loss= 2.25882 val_roc= 0.72080 val_ap= 0.76669\n",
      "Epoch: 0014 train_loss= 2.25891 val_roc= 0.72457 val_ap= 0.76928\n",
      "Epoch: 0015 train_loss= 2.22026 val_roc= 0.72567 val_ap= 0.76899\n",
      "Epoch: 0016 train_loss= 2.24252 val_roc= 0.72569 val_ap= 0.76829\n",
      "Epoch: 0017 train_loss= 2.29150 val_roc= 0.72456 val_ap= 0.76619\n",
      "Epoch: 0018 train_loss= 2.14713 val_roc= 0.72373 val_ap= 0.76494\n",
      "Epoch: 0019 train_loss= 2.16536 val_roc= 0.72304 val_ap= 0.76334\n",
      "Epoch: 0020 train_loss= 2.17208 val_roc= 0.72163 val_ap= 0.76037\n",
      "Test AP score: 0.7687399392156402\n",
      "Test ROC score: 0.733730225818138\n",
      "Epoch: 0021 train_loss= 2.21916 val_roc= 0.72109 val_ap= 0.75876\n",
      "Epoch: 0022 train_loss= 2.11046 val_roc= 0.71959 val_ap= 0.75662\n",
      "Epoch: 0023 train_loss= 2.13696 val_roc= 0.71732 val_ap= 0.75415\n",
      "Epoch: 0024 train_loss= 2.11292 val_roc= 0.71641 val_ap= 0.75290\n",
      "Epoch: 0025 train_loss= 2.09112 val_roc= 0.71428 val_ap= 0.75081\n",
      "Epoch: 0026 train_loss= 2.08104 val_roc= 0.71288 val_ap= 0.74892\n",
      "Epoch: 0027 train_loss= 2.03996 val_roc= 0.71201 val_ap= 0.74799\n",
      "Epoch: 0028 train_loss= 2.06343 val_roc= 0.71104 val_ap= 0.74676\n",
      "Epoch: 0029 train_loss= 2.03203 val_roc= 0.70941 val_ap= 0.74520\n",
      "Epoch: 0030 train_loss= 2.01543 val_roc= 0.70793 val_ap= 0.74411\n",
      "Test AP score: 0.7602756553601591\n",
      "Test ROC score: 0.7176017389204202\n",
      "Epoch: 0031 train_loss= 1.98248 val_roc= 0.70601 val_ap= 0.74273\n",
      "Epoch: 0032 train_loss= 1.96174 val_roc= 0.70407 val_ap= 0.74126\n",
      "Epoch: 0033 train_loss= 1.90721 val_roc= 0.70252 val_ap= 0.74038\n",
      "Epoch: 0034 train_loss= 1.92898 val_roc= 0.70135 val_ap= 0.73983\n",
      "Epoch: 0035 train_loss= 1.81612 val_roc= 0.70015 val_ap= 0.73941\n",
      "Epoch: 0036 train_loss= 1.85730 val_roc= 0.69840 val_ap= 0.73842\n",
      "Epoch: 0037 train_loss= 1.83905 val_roc= 0.69724 val_ap= 0.73769\n",
      "Epoch: 0038 train_loss= 1.85026 val_roc= 0.69637 val_ap= 0.73744\n",
      "Epoch: 0039 train_loss= 1.78595 val_roc= 0.69530 val_ap= 0.73679\n",
      "Epoch: 0040 train_loss= 1.79638 val_roc= 0.69413 val_ap= 0.73621\n",
      "Test AP score: 0.7560630298838426\n",
      "Test ROC score: 0.707615022340297\n",
      "Epoch: 0041 train_loss= 1.73447 val_roc= 0.69316 val_ap= 0.73607\n",
      "Epoch: 0042 train_loss= 1.71819 val_roc= 0.69213 val_ap= 0.73560\n",
      "Epoch: 0043 train_loss= 1.70430 val_roc= 0.69128 val_ap= 0.73549\n",
      "Epoch: 0044 train_loss= 1.69827 val_roc= 0.69070 val_ap= 0.73557\n",
      "Epoch: 0045 train_loss= 1.65817 val_roc= 0.68971 val_ap= 0.73498\n",
      "Epoch: 0046 train_loss= 1.67307 val_roc= 0.68858 val_ap= 0.73450\n",
      "Epoch: 0047 train_loss= 1.60210 val_roc= 0.68697 val_ap= 0.73336\n",
      "Epoch: 0048 train_loss= 1.59233 val_roc= 0.68515 val_ap= 0.73271\n",
      "Epoch: 0049 train_loss= 1.61664 val_roc= 0.68420 val_ap= 0.73262\n",
      "Epoch: 0050 train_loss= 1.56435 val_roc= 0.68276 val_ap= 0.73224\n",
      "Test AP score: 0.7532341277323947\n",
      "Test ROC score: 0.7006110373143339\n",
      "Epoch: 0051 train_loss= 1.54749 val_roc= 0.68158 val_ap= 0.73164\n",
      "Epoch: 0052 train_loss= 1.53004 val_roc= 0.68047 val_ap= 0.73155\n",
      "Epoch: 0053 train_loss= 1.51559 val_roc= 0.67865 val_ap= 0.73080\n",
      "Epoch: 0054 train_loss= 1.51485 val_roc= 0.67706 val_ap= 0.73002\n",
      "Epoch: 0055 train_loss= 1.48260 val_roc= 0.67585 val_ap= 0.72989\n",
      "Epoch: 0056 train_loss= 1.49020 val_roc= 0.67577 val_ap= 0.73028\n",
      "Epoch: 0057 train_loss= 1.43333 val_roc= 0.67411 val_ap= 0.72931\n",
      "Epoch: 0058 train_loss= 1.46794 val_roc= 0.67358 val_ap= 0.72892\n",
      "Epoch: 0059 train_loss= 1.42524 val_roc= 0.67321 val_ap= 0.72848\n",
      "Epoch: 0060 train_loss= 1.43472 val_roc= 0.67296 val_ap= 0.72861\n",
      "Test AP score: 0.7504283663554583\n",
      "Test ROC score: 0.6925685303707282\n",
      "Epoch: 0061 train_loss= 1.40455 val_roc= 0.67271 val_ap= 0.72825\n",
      "Epoch: 0062 train_loss= 1.37154 val_roc= 0.67250 val_ap= 0.72816\n",
      "Epoch: 0063 train_loss= 1.35966 val_roc= 0.67265 val_ap= 0.72844\n",
      "Epoch: 0064 train_loss= 1.37878 val_roc= 0.67246 val_ap= 0.72884\n",
      "Epoch: 0065 train_loss= 1.34554 val_roc= 0.67378 val_ap= 0.72871\n",
      "Epoch: 0066 train_loss= 1.32533 val_roc= 0.67461 val_ap= 0.72938\n",
      "Epoch: 0067 train_loss= 1.32291 val_roc= 0.67638 val_ap= 0.73014\n",
      "Epoch: 0068 train_loss= 1.32172 val_roc= 0.67723 val_ap= 0.73047\n",
      "Epoch: 0069 train_loss= 1.30897 val_roc= 0.67888 val_ap= 0.73126\n",
      "Epoch: 0070 train_loss= 1.30055 val_roc= 0.68028 val_ap= 0.73171\n",
      "Test AP score: 0.7540081675971577\n",
      "Test ROC score: 0.6985436541480498\n",
      "Epoch: 0071 train_loss= 1.26835 val_roc= 0.68169 val_ap= 0.73245\n",
      "Epoch: 0072 train_loss= 1.26368 val_roc= 0.68410 val_ap= 0.73395\n",
      "Epoch: 0073 train_loss= 1.23457 val_roc= 0.68738 val_ap= 0.73556\n",
      "Epoch: 0074 train_loss= 1.23844 val_roc= 0.69072 val_ap= 0.73727\n",
      "Epoch: 0075 train_loss= 1.22374 val_roc= 0.69743 val_ap= 0.74120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0076 train_loss= 1.22329 val_roc= 0.70300 val_ap= 0.74440\n",
      "Epoch: 0077 train_loss= 1.20337 val_roc= 0.70844 val_ap= 0.74725\n",
      "Epoch: 0078 train_loss= 1.19903 val_roc= 0.71265 val_ap= 0.74921\n",
      "Epoch: 0079 train_loss= 1.16186 val_roc= 0.71633 val_ap= 0.75054\n",
      "Epoch: 0080 train_loss= 1.16417 val_roc= 0.71899 val_ap= 0.75195\n",
      "Test AP score: 0.767631601840024\n",
      "Test ROC score: 0.7271682163989857\n",
      "Epoch: 0081 train_loss= 1.14088 val_roc= 0.72150 val_ap= 0.75332\n",
      "Epoch: 0082 train_loss= 1.13353 val_roc= 0.72379 val_ap= 0.75407\n",
      "Epoch: 0083 train_loss= 1.11359 val_roc= 0.72450 val_ap= 0.75390\n",
      "Epoch: 0084 train_loss= 1.11476 val_roc= 0.72511 val_ap= 0.75404\n",
      "Epoch: 0085 train_loss= 1.11021 val_roc= 0.72580 val_ap= 0.75379\n",
      "Epoch: 0086 train_loss= 1.10014 val_roc= 0.72646 val_ap= 0.75334\n",
      "Epoch: 0087 train_loss= 1.08494 val_roc= 0.72617 val_ap= 0.75255\n",
      "Epoch: 0088 train_loss= 1.07776 val_roc= 0.72536 val_ap= 0.75158\n",
      "Epoch: 0089 train_loss= 1.04530 val_roc= 0.72410 val_ap= 0.75001\n",
      "Epoch: 0090 train_loss= 1.04104 val_roc= 0.72245 val_ap= 0.74870\n",
      "Test AP score: 0.7731739563308664\n",
      "Test ROC score: 0.7416302378939741\n",
      "Epoch: 0091 train_loss= 1.04039 val_roc= 0.72167 val_ap= 0.74837\n",
      "Epoch: 0092 train_loss= 1.02327 val_roc= 0.72055 val_ap= 0.74780\n",
      "Epoch: 0093 train_loss= 1.01791 val_roc= 0.71987 val_ap= 0.74740\n",
      "Epoch: 0094 train_loss= 0.99582 val_roc= 0.71905 val_ap= 0.74711\n",
      "Epoch: 0095 train_loss= 1.00083 val_roc= 0.71767 val_ap= 0.74635\n",
      "Epoch: 0096 train_loss= 0.97757 val_roc= 0.71692 val_ap= 0.74597\n",
      "Epoch: 0097 train_loss= 0.96840 val_roc= 0.71585 val_ap= 0.74523\n",
      "Epoch: 0098 train_loss= 0.97632 val_roc= 0.71523 val_ap= 0.74461\n",
      "Epoch: 0099 train_loss= 0.97915 val_roc= 0.71496 val_ap= 0.74464\n",
      "Epoch: 0100 train_loss= 0.95440 val_roc= 0.71426 val_ap= 0.74402\n",
      "Test AP score: 0.771053025556345\n",
      "Test ROC score: 0.7435623716942398\n",
      "Epoch: 0101 train_loss= 0.93628 val_roc= 0.71391 val_ap= 0.74430\n",
      "Epoch: 0102 train_loss= 0.94504 val_roc= 0.71426 val_ap= 0.74458\n",
      "Epoch: 0103 train_loss= 0.93058 val_roc= 0.71470 val_ap= 0.74485\n",
      "Epoch: 0104 train_loss= 0.93177 val_roc= 0.71480 val_ap= 0.74467\n",
      "Epoch: 0105 train_loss= 0.92437 val_roc= 0.71564 val_ap= 0.74520\n",
      "Epoch: 0106 train_loss= 0.90917 val_roc= 0.71616 val_ap= 0.74588\n",
      "Epoch: 0107 train_loss= 0.92993 val_roc= 0.71668 val_ap= 0.74624\n",
      "Epoch: 0108 train_loss= 0.90132 val_roc= 0.71721 val_ap= 0.74683\n",
      "Epoch: 0109 train_loss= 0.90482 val_roc= 0.71771 val_ap= 0.74686\n",
      "Epoch: 0110 train_loss= 0.88963 val_roc= 0.71820 val_ap= 0.74726\n",
      "Test AP score: 0.7742363671110593\n",
      "Test ROC score: 0.7490592923559957\n",
      "Epoch: 0111 train_loss= 0.88288 val_roc= 0.71872 val_ap= 0.74815\n",
      "Epoch: 0112 train_loss= 0.89279 val_roc= 0.71938 val_ap= 0.74917\n",
      "Epoch: 0113 train_loss= 0.87038 val_roc= 0.72002 val_ap= 0.75026\n",
      "Epoch: 0114 train_loss= 0.88726 val_roc= 0.72086 val_ap= 0.75135\n",
      "Epoch: 0115 train_loss= 0.87234 val_roc= 0.72142 val_ap= 0.75224\n",
      "Epoch: 0116 train_loss= 0.86730 val_roc= 0.72165 val_ap= 0.75240\n",
      "Epoch: 0117 train_loss= 0.87142 val_roc= 0.72208 val_ap= 0.75327\n",
      "Epoch: 0118 train_loss= 0.87688 val_roc= 0.72284 val_ap= 0.75434\n",
      "Epoch: 0119 train_loss= 0.85782 val_roc= 0.72293 val_ap= 0.75476\n",
      "Epoch: 0120 train_loss= 0.87335 val_roc= 0.72317 val_ap= 0.75531\n",
      "Test AP score: 0.7725594794378327\n",
      "Test ROC score: 0.7493491124260356\n",
      "Epoch: 0121 train_loss= 0.86594 val_roc= 0.72369 val_ap= 0.75647\n",
      "Epoch: 0122 train_loss= 0.85537 val_roc= 0.72392 val_ap= 0.75708\n",
      "Epoch: 0123 train_loss= 0.85410 val_roc= 0.72416 val_ap= 0.75765\n",
      "Epoch: 0124 train_loss= 0.85112 val_roc= 0.72384 val_ap= 0.75850\n",
      "Epoch: 0125 train_loss= 0.85496 val_roc= 0.72384 val_ap= 0.75938\n",
      "Epoch: 0126 train_loss= 0.85636 val_roc= 0.72326 val_ap= 0.75972\n",
      "Epoch: 0127 train_loss= 0.85234 val_roc= 0.72274 val_ap= 0.75966\n",
      "Epoch: 0128 train_loss= 0.84268 val_roc= 0.72270 val_ap= 0.75961\n",
      "Epoch: 0129 train_loss= 0.85066 val_roc= 0.72216 val_ap= 0.75898\n",
      "Epoch: 0130 train_loss= 0.83941 val_roc= 0.72161 val_ap= 0.75859\n",
      "Test AP score: 0.7747540836195017\n",
      "Test ROC score: 0.7500205289216277\n",
      "Epoch: 0131 train_loss= 0.84595 val_roc= 0.72173 val_ap= 0.75909\n",
      "Epoch: 0132 train_loss= 0.84652 val_roc= 0.72150 val_ap= 0.75893\n",
      "Epoch: 0133 train_loss= 0.83936 val_roc= 0.72136 val_ap= 0.75897\n",
      "Epoch: 0134 train_loss= 0.84674 val_roc= 0.72138 val_ap= 0.75912\n",
      "Epoch: 0135 train_loss= 0.83471 val_roc= 0.72163 val_ap= 0.75928\n",
      "Epoch: 0136 train_loss= 0.84848 val_roc= 0.72185 val_ap= 0.75948\n",
      "Epoch: 0137 train_loss= 0.83888 val_roc= 0.72167 val_ap= 0.75982\n",
      "Epoch: 0138 train_loss= 0.84021 val_roc= 0.72181 val_ap= 0.75976\n",
      "Epoch: 0139 train_loss= 0.84333 val_roc= 0.72154 val_ap= 0.75953\n",
      "Epoch: 0140 train_loss= 0.84662 val_roc= 0.72142 val_ap= 0.75969\n",
      "Test AP score: 0.779644136572855\n",
      "Test ROC score: 0.7514406472648232\n",
      "Epoch: 0141 train_loss= 0.84937 val_roc= 0.72105 val_ap= 0.75952\n",
      "Epoch: 0142 train_loss= 0.83985 val_roc= 0.72128 val_ap= 0.75992\n",
      "Epoch: 0143 train_loss= 0.83927 val_roc= 0.72138 val_ap= 0.76001\n",
      "Epoch: 0144 train_loss= 0.83270 val_roc= 0.72134 val_ap= 0.75995\n",
      "Epoch: 0145 train_loss= 0.84161 val_roc= 0.72093 val_ap= 0.75977\n",
      "Epoch: 0146 train_loss= 0.84976 val_roc= 0.72064 val_ap= 0.75948\n",
      "Epoch: 0147 train_loss= 0.84509 val_roc= 0.72045 val_ap= 0.75957\n",
      "Epoch: 0148 train_loss= 0.83505 val_roc= 0.72053 val_ap= 0.75960\n",
      "Epoch: 0149 train_loss= 0.84578 val_roc= 0.72016 val_ap= 0.75939\n",
      "Epoch: 0150 train_loss= 0.83921 val_roc= 0.72000 val_ap= 0.75956\n",
      "Test AP score: 0.7824225525378844\n",
      "Test ROC score: 0.750377973674677\n",
      "Epoch: 0151 train_loss= 0.82723 val_roc= 0.71950 val_ap= 0.75912\n",
      "Epoch: 0152 train_loss= 0.84547 val_roc= 0.71934 val_ap= 0.75910\n",
      "Epoch: 0153 train_loss= 0.84214 val_roc= 0.71934 val_ap= 0.75908\n",
      "Epoch: 0154 train_loss= 0.83167 val_roc= 0.71903 val_ap= 0.75895\n",
      "Epoch: 0155 train_loss= 0.83302 val_roc= 0.71940 val_ap= 0.75931\n",
      "Epoch: 0156 train_loss= 0.84455 val_roc= 0.71954 val_ap= 0.75946\n",
      "Epoch: 0157 train_loss= 0.84815 val_roc= 0.71992 val_ap= 0.75978\n",
      "Epoch: 0158 train_loss= 0.84373 val_roc= 0.71991 val_ap= 0.75981\n",
      "Epoch: 0159 train_loss= 0.83749 val_roc= 0.72010 val_ap= 0.75975\n",
      "Epoch: 0160 train_loss= 0.84899 val_roc= 0.72039 val_ap= 0.76013\n",
      "Test AP score: 0.7822011041486323\n",
      "Test ROC score: 0.748523125226422\n",
      "Epoch: 0161 train_loss= 0.83362 val_roc= 0.72088 val_ap= 0.76042\n",
      "Epoch: 0162 train_loss= 0.84399 val_roc= 0.72066 val_ap= 0.76016\n",
      "Epoch: 0163 train_loss= 0.85008 val_roc= 0.72033 val_ap= 0.75988\n",
      "Epoch: 0164 train_loss= 0.84376 val_roc= 0.71991 val_ap= 0.75964\n",
      "Epoch: 0165 train_loss= 0.83500 val_roc= 0.71921 val_ap= 0.75890\n",
      "Epoch: 0166 train_loss= 0.83788 val_roc= 0.71814 val_ap= 0.75808\n",
      "Epoch: 0167 train_loss= 0.85158 val_roc= 0.71793 val_ap= 0.75799\n",
      "Epoch: 0168 train_loss= 0.82340 val_roc= 0.71734 val_ap= 0.75743\n",
      "Epoch: 0169 train_loss= 0.84744 val_roc= 0.71744 val_ap= 0.75780\n",
      "Epoch: 0170 train_loss= 0.83125 val_roc= 0.71736 val_ap= 0.75765\n",
      "Test AP score: 0.7814995032362598\n",
      "Test ROC score: 0.7463108320251177\n",
      "Epoch: 0171 train_loss= 0.84108 val_roc= 0.71767 val_ap= 0.75810\n",
      "Epoch: 0172 train_loss= 0.83128 val_roc= 0.71783 val_ap= 0.75841\n",
      "Epoch: 0173 train_loss= 0.83357 val_roc= 0.71800 val_ap= 0.75864\n",
      "Epoch: 0174 train_loss= 0.83334 val_roc= 0.71732 val_ap= 0.75821\n",
      "Epoch: 0175 train_loss= 0.83127 val_roc= 0.71699 val_ap= 0.75818\n",
      "Epoch: 0176 train_loss= 0.83439 val_roc= 0.71666 val_ap= 0.75763\n",
      "Epoch: 0177 train_loss= 0.83947 val_roc= 0.71690 val_ap= 0.75728\n",
      "Epoch: 0178 train_loss= 0.83647 val_roc= 0.71637 val_ap= 0.75667\n",
      "Epoch: 0179 train_loss= 0.85716 val_roc= 0.71647 val_ap= 0.75734\n",
      "Epoch: 0180 train_loss= 0.83981 val_roc= 0.71672 val_ap= 0.75857\n",
      "Test AP score: 0.7796532794056813\n",
      "Test ROC score: 0.7423982610795797\n",
      "Epoch: 0181 train_loss= 0.83988 val_roc= 0.71692 val_ap= 0.75928\n",
      "Epoch: 0182 train_loss= 0.82760 val_roc= 0.71750 val_ap= 0.75974\n",
      "Epoch: 0183 train_loss= 0.82942 val_roc= 0.71882 val_ap= 0.76084\n",
      "Epoch: 0184 train_loss= 0.83260 val_roc= 0.71991 val_ap= 0.76143\n",
      "Epoch: 0185 train_loss= 0.82992 val_roc= 0.71971 val_ap= 0.76144\n",
      "Epoch: 0186 train_loss= 0.83694 val_roc= 0.72024 val_ap= 0.76175\n",
      "Epoch: 0187 train_loss= 0.84089 val_roc= 0.72088 val_ap= 0.76220\n",
      "Epoch: 0188 train_loss= 0.84135 val_roc= 0.72105 val_ap= 0.76257\n",
      "Epoch: 0189 train_loss= 0.84713 val_roc= 0.72078 val_ap= 0.76248\n",
      "Epoch: 0190 train_loss= 0.83922 val_roc= 0.72130 val_ap= 0.76275\n",
      "Test AP score: 0.7818045186511309\n",
      "Test ROC score: 0.742712232822123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0191 train_loss= 0.84134 val_roc= 0.72214 val_ap= 0.76342\n",
      "Epoch: 0192 train_loss= 0.84260 val_roc= 0.72216 val_ap= 0.76348\n",
      "Epoch: 0193 train_loss= 0.84041 val_roc= 0.72190 val_ap= 0.76337\n",
      "Epoch: 0194 train_loss= 0.84017 val_roc= 0.72167 val_ap= 0.76299\n",
      "Epoch: 0195 train_loss= 0.83603 val_roc= 0.72194 val_ap= 0.76318\n",
      "Epoch: 0196 train_loss= 0.83308 val_roc= 0.72216 val_ap= 0.76337\n",
      "Epoch: 0197 train_loss= 0.83444 val_roc= 0.72274 val_ap= 0.76349\n",
      "Epoch: 0198 train_loss= 0.83138 val_roc= 0.72334 val_ap= 0.76428\n",
      "Epoch: 0199 train_loss= 0.83408 val_roc= 0.72390 val_ap= 0.76459\n",
      "Epoch: 0200 train_loss= 0.83989 val_roc= 0.72402 val_ap= 0.76428\n",
      "Test AP score: 0.7818260841537061\n",
      "Test ROC score: 0.7427846878396329\n"
     ]
    }
   ],
   "source": [
    "runner.erun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c11174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
