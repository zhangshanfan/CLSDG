{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65775001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T10:06:32.415435Z",
     "start_time": "2022-05-07T10:06:27.286012Z"
    }
   },
   "outputs": [],
   "source": [
    "import settings\n",
    "from link_prediction import Link_pred_Runner\n",
    "dataname = 'cora'       # 'cora' or 'wiki' or 'power' or 'dublin'\n",
    "model = 'arga_ae'     # 'arga_ae' or 'arga_vae'\n",
    "task = 'link_prediction'         # 'clustering' or 'link_prediction'\n",
    "settings = settings.get_settings(dataname, model, task)\n",
    "runner = Link_pred_Runner(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254bd93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T10:10:54.430288Z",
     "start_time": "2022-05-07T10:06:34.183664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:143: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:145: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:147: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:27: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\.conda\\envs\\tf1.x\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:28: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:37: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:38: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.78176 val_roc= 0.56664 val_ap= 0.56560\n",
      "Epoch: 0002 train_loss= 0.78082 val_roc= 0.60604 val_ap= 0.63632\n",
      "Epoch: 0003 train_loss= 0.78050 val_roc= 0.55519 val_ap= 0.57714\n",
      "Epoch: 0004 train_loss= 0.77986 val_roc= 0.56576 val_ap= 0.56256\n",
      "Epoch: 0005 train_loss= 0.77935 val_roc= 0.56803 val_ap= 0.56982\n",
      "Epoch: 0006 train_loss= 0.77809 val_roc= 0.57941 val_ap= 0.59349\n",
      "Epoch: 0007 train_loss= 0.77798 val_roc= 0.58409 val_ap= 0.59789\n",
      "Epoch: 0008 train_loss= 0.77718 val_roc= 0.59545 val_ap= 0.60121\n",
      "Epoch: 0009 train_loss= 0.77552 val_roc= 0.57639 val_ap= 0.58180\n",
      "Epoch: 0010 train_loss= 0.77415 val_roc= 0.64149 val_ap= 0.63317\n",
      "Test AP score: 0.5867189670119513\n",
      "Test ROC score: 0.5876537920058762\n",
      "Epoch: 0011 train_loss= 0.77187 val_roc= 0.63803 val_ap= 0.65651\n",
      "Epoch: 0012 train_loss= 0.76990 val_roc= 0.64425 val_ap= 0.65209\n",
      "Epoch: 0013 train_loss= 0.76681 val_roc= 0.60534 val_ap= 0.63270\n",
      "Epoch: 0014 train_loss= 0.76467 val_roc= 0.66978 val_ap= 0.68283\n",
      "Epoch: 0015 train_loss= 0.75808 val_roc= 0.62853 val_ap= 0.67747\n",
      "Epoch: 0016 train_loss= 0.75475 val_roc= 0.66414 val_ap= 0.69455\n",
      "Epoch: 0017 train_loss= 0.74542 val_roc= 0.66677 val_ap= 0.67571\n",
      "Epoch: 0018 train_loss= 0.73814 val_roc= 0.65343 val_ap= 0.67801\n",
      "Epoch: 0019 train_loss= 0.72896 val_roc= 0.68649 val_ap= 0.69576\n",
      "Epoch: 0020 train_loss= 0.72012 val_roc= 0.68928 val_ap= 0.71011\n",
      "Test AP score: 0.7286451055701573\n",
      "Test ROC score: 0.6926338264999334\n",
      "Epoch: 0021 train_loss= 0.70948 val_roc= 0.67938 val_ap= 0.70752\n",
      "Epoch: 0022 train_loss= 0.69952 val_roc= 0.68868 val_ap= 0.71433\n",
      "Epoch: 0023 train_loss= 0.69021 val_roc= 0.68995 val_ap= 0.71433\n",
      "Epoch: 0024 train_loss= 0.68458 val_roc= 0.67263 val_ap= 0.70533\n",
      "Epoch: 0025 train_loss= 0.67933 val_roc= 0.67764 val_ap= 0.69765\n",
      "Epoch: 0026 train_loss= 0.67879 val_roc= 0.68833 val_ap= 0.71007\n",
      "Epoch: 0027 train_loss= 0.67288 val_roc= 0.69703 val_ap= 0.72099\n",
      "Epoch: 0028 train_loss= 0.66746 val_roc= 0.68645 val_ap= 0.70894\n",
      "Epoch: 0029 train_loss= 0.66348 val_roc= 0.71322 val_ap= 0.73855\n",
      "Epoch: 0030 train_loss= 0.66089 val_roc= 0.70439 val_ap= 0.72821\n",
      "Test AP score: 0.7455310182472107\n",
      "Test ROC score: 0.7017902343651545\n",
      "Epoch: 0031 train_loss= 0.65284 val_roc= 0.70221 val_ap= 0.72944\n",
      "Epoch: 0032 train_loss= 0.64957 val_roc= 0.73001 val_ap= 0.74110\n",
      "Epoch: 0033 train_loss= 0.64134 val_roc= 0.71652 val_ap= 0.74535\n",
      "Epoch: 0034 train_loss= 0.63484 val_roc= 0.70352 val_ap= 0.73503\n",
      "Epoch: 0035 train_loss= 0.62924 val_roc= 0.73537 val_ap= 0.76305\n",
      "Epoch: 0036 train_loss= 0.62070 val_roc= 0.74288 val_ap= 0.76776\n",
      "Epoch: 0037 train_loss= 0.61144 val_roc= 0.75483 val_ap= 0.77163\n",
      "Epoch: 0038 train_loss= 0.60610 val_roc= 0.73825 val_ap= 0.76275\n",
      "Epoch: 0039 train_loss= 0.59707 val_roc= 0.77117 val_ap= 0.77174\n",
      "Epoch: 0040 train_loss= 0.58729 val_roc= 0.77337 val_ap= 0.79768\n",
      "Test AP score: 0.8173223681148287\n",
      "Test ROC score: 0.7931166712874781\n",
      "Epoch: 0041 train_loss= 0.57698 val_roc= 0.77856 val_ap= 0.80339\n",
      "Epoch: 0042 train_loss= 0.57262 val_roc= 0.75680 val_ap= 0.78959\n",
      "Epoch: 0043 train_loss= 0.56039 val_roc= 0.78272 val_ap= 0.80873\n",
      "Epoch: 0044 train_loss= 0.55235 val_roc= 0.78100 val_ap= 0.80250\n",
      "Epoch: 0045 train_loss= 0.54494 val_roc= 0.79732 val_ap= 0.81405\n",
      "Epoch: 0046 train_loss= 0.53647 val_roc= 0.80199 val_ap= 0.81864\n",
      "Epoch: 0047 train_loss= 0.52764 val_roc= 0.80718 val_ap= 0.81899\n",
      "Epoch: 0048 train_loss= 0.52243 val_roc= 0.80350 val_ap= 0.82047\n",
      "Epoch: 0049 train_loss= 0.51653 val_roc= 0.80457 val_ap= 0.82218\n",
      "Epoch: 0050 train_loss= 0.51148 val_roc= 0.80053 val_ap= 0.81635\n",
      "Test AP score: 0.8412641493748748\n",
      "Test ROC score: 0.8262496894454666\n",
      "Epoch: 0051 train_loss= 0.50611 val_roc= 0.81295 val_ap= 0.82832\n",
      "Epoch: 0052 train_loss= 0.50204 val_roc= 0.81077 val_ap= 0.82452\n",
      "Epoch: 0053 train_loss= 0.49730 val_roc= 0.81031 val_ap= 0.81769\n",
      "Epoch: 0054 train_loss= 0.49595 val_roc= 0.82131 val_ap= 0.83126\n",
      "Epoch: 0055 train_loss= 0.49076 val_roc= 0.81444 val_ap= 0.82871\n",
      "Epoch: 0056 train_loss= 0.48915 val_roc= 0.81288 val_ap= 0.82680\n",
      "Epoch: 0057 train_loss= 0.48621 val_roc= 0.82251 val_ap= 0.84213\n",
      "Epoch: 0058 train_loss= 0.48378 val_roc= 0.80863 val_ap= 0.82987\n",
      "Epoch: 0059 train_loss= 0.48152 val_roc= 0.81432 val_ap= 0.83237\n",
      "Epoch: 0060 train_loss= 0.47873 val_roc= 0.81308 val_ap= 0.83579\n",
      "Test AP score: 0.849760951104716\n",
      "Test ROC score: 0.824802235272514\n",
      "Epoch: 0061 train_loss= 0.47773 val_roc= 0.82470 val_ap= 0.84190\n",
      "Epoch: 0062 train_loss= 0.47509 val_roc= 0.82736 val_ap= 0.84908\n",
      "Epoch: 0063 train_loss= 0.47361 val_roc= 0.82430 val_ap= 0.85017\n",
      "Epoch: 0064 train_loss= 0.47274 val_roc= 0.82483 val_ap= 0.84682\n",
      "Epoch: 0065 train_loss= 0.47151 val_roc= 0.81049 val_ap= 0.83605\n",
      "Epoch: 0066 train_loss= 0.47004 val_roc= 0.83153 val_ap= 0.85218\n",
      "Epoch: 0067 train_loss= 0.46815 val_roc= 0.84145 val_ap= 0.86586\n",
      "Epoch: 0068 train_loss= 0.46637 val_roc= 0.81716 val_ap= 0.84983\n",
      "Epoch: 0069 train_loss= 0.46529 val_roc= 0.82932 val_ap= 0.85849\n",
      "Epoch: 0070 train_loss= 0.46451 val_roc= 0.82669 val_ap= 0.85555\n",
      "Test AP score: 0.8612032885514656\n",
      "Test ROC score: 0.8406018096777794\n",
      "Epoch: 0071 train_loss= 0.46363 val_roc= 0.82605 val_ap= 0.85296\n",
      "Epoch: 0072 train_loss= 0.46240 val_roc= 0.83338 val_ap= 0.85979\n",
      "Epoch: 0073 train_loss= 0.46174 val_roc= 0.83133 val_ap= 0.85895\n",
      "Epoch: 0074 train_loss= 0.46035 val_roc= 0.82449 val_ap= 0.85247\n",
      "Epoch: 0075 train_loss= 0.46013 val_roc= 0.83947 val_ap= 0.86502\n",
      "Epoch: 0076 train_loss= 0.45932 val_roc= 0.82336 val_ap= 0.85460\n",
      "Epoch: 0077 train_loss= 0.45795 val_roc= 0.82076 val_ap= 0.85100\n",
      "Epoch: 0078 train_loss= 0.45667 val_roc= 0.83173 val_ap= 0.85570\n",
      "Epoch: 0079 train_loss= 0.45659 val_roc= 0.83870 val_ap= 0.86508\n",
      "Epoch: 0080 train_loss= 0.45483 val_roc= 0.83588 val_ap= 0.86196\n",
      "Test AP score: 0.8697545086154965\n",
      "Test ROC score: 0.8511534625480235\n",
      "Epoch: 0081 train_loss= 0.45489 val_roc= 0.83819 val_ap= 0.86521\n",
      "Epoch: 0082 train_loss= 0.45406 val_roc= 0.83442 val_ap= 0.86201\n",
      "Epoch: 0083 train_loss= 0.45291 val_roc= 0.83516 val_ap= 0.86372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0084 train_loss= 0.45248 val_roc= 0.84263 val_ap= 0.86869\n",
      "Epoch: 0085 train_loss= 0.45153 val_roc= 0.84600 val_ap= 0.86801\n",
      "Epoch: 0086 train_loss= 0.45073 val_roc= 0.83222 val_ap= 0.86504\n",
      "Epoch: 0087 train_loss= 0.44991 val_roc= 0.83328 val_ap= 0.86541\n",
      "Epoch: 0088 train_loss= 0.44922 val_roc= 0.82703 val_ap= 0.86502\n",
      "Epoch: 0089 train_loss= 0.44861 val_roc= 0.82447 val_ap= 0.85960\n",
      "Epoch: 0090 train_loss= 0.44785 val_roc= 0.83681 val_ap= 0.86404\n",
      "Test AP score: 0.8781129507424082\n",
      "Test ROC score: 0.8549323261164661\n",
      "Epoch: 0091 train_loss= 0.44651 val_roc= 0.83053 val_ap= 0.86056\n",
      "Epoch: 0092 train_loss= 0.44634 val_roc= 0.84176 val_ap= 0.86566\n",
      "Epoch: 0093 train_loss= 0.44526 val_roc= 0.83410 val_ap= 0.86309\n",
      "Epoch: 0094 train_loss= 0.44469 val_roc= 0.83589 val_ap= 0.86754\n",
      "Epoch: 0095 train_loss= 0.44392 val_roc= 0.83384 val_ap= 0.86460\n",
      "Epoch: 0096 train_loss= 0.44315 val_roc= 0.83287 val_ap= 0.86512\n",
      "Epoch: 0097 train_loss= 0.44304 val_roc= 0.84120 val_ap= 0.86412\n",
      "Epoch: 0098 train_loss= 0.44321 val_roc= 0.83062 val_ap= 0.85766\n",
      "Epoch: 0099 train_loss= 0.44101 val_roc= 0.83691 val_ap= 0.87064\n",
      "Epoch: 0100 train_loss= 0.44120 val_roc= 0.84438 val_ap= 0.87260\n",
      "Test AP score: 0.8776127240817635\n",
      "Test ROC score: 0.8536901079829617\n",
      "Epoch: 0101 train_loss= 0.44037 val_roc= 0.83138 val_ap= 0.86230\n",
      "Epoch: 0102 train_loss= 0.43951 val_roc= 0.82950 val_ap= 0.86053\n",
      "Epoch: 0103 train_loss= 0.43975 val_roc= 0.84497 val_ap= 0.87151\n",
      "Epoch: 0104 train_loss= 0.43922 val_roc= 0.83874 val_ap= 0.86751\n",
      "Epoch: 0105 train_loss= 0.43863 val_roc= 0.85440 val_ap= 0.87222\n",
      "Epoch: 0106 train_loss= 0.43746 val_roc= 0.84412 val_ap= 0.86994\n",
      "Epoch: 0107 train_loss= 0.43699 val_roc= 0.85667 val_ap= 0.87817\n",
      "Epoch: 0108 train_loss= 0.43652 val_roc= 0.84273 val_ap= 0.87069\n",
      "Epoch: 0109 train_loss= 0.43648 val_roc= 0.82812 val_ap= 0.85425\n",
      "Epoch: 0110 train_loss= 0.43618 val_roc= 0.83610 val_ap= 0.86231\n",
      "Test AP score: 0.8922699488947498\n",
      "Test ROC score: 0.8718192914675817\n",
      "Epoch: 0111 train_loss= 0.43553 val_roc= 0.84447 val_ap= 0.86939\n",
      "Epoch: 0112 train_loss= 0.43512 val_roc= 0.83730 val_ap= 0.86631\n",
      "Epoch: 0113 train_loss= 0.43455 val_roc= 0.85155 val_ap= 0.87228\n",
      "Epoch: 0114 train_loss= 0.43423 val_roc= 0.83926 val_ap= 0.87191\n",
      "Epoch: 0115 train_loss= 0.43375 val_roc= 0.85083 val_ap= 0.87416\n",
      "Epoch: 0116 train_loss= 0.43344 val_roc= 0.85161 val_ap= 0.87676\n",
      "Epoch: 0117 train_loss= 0.43323 val_roc= 0.83618 val_ap= 0.86154\n",
      "Epoch: 0118 train_loss= 0.43301 val_roc= 0.85385 val_ap= 0.88035\n",
      "Epoch: 0119 train_loss= 0.43238 val_roc= 0.84656 val_ap= 0.87472\n",
      "Epoch: 0120 train_loss= 0.43215 val_roc= 0.84737 val_ap= 0.87841\n",
      "Test AP score: 0.8941791596437116\n",
      "Test ROC score: 0.8692124337033582\n",
      "Epoch: 0121 train_loss= 0.43108 val_roc= 0.83402 val_ap= 0.86699\n",
      "Epoch: 0122 train_loss= 0.43107 val_roc= 0.84137 val_ap= 0.86515\n",
      "Epoch: 0123 train_loss= 0.43072 val_roc= 0.84821 val_ap= 0.87137\n",
      "Epoch: 0124 train_loss= 0.43070 val_roc= 0.83792 val_ap= 0.87477\n",
      "Epoch: 0125 train_loss= 0.43025 val_roc= 0.82832 val_ap= 0.86279\n",
      "Epoch: 0126 train_loss= 0.42985 val_roc= 0.84704 val_ap= 0.87313\n",
      "Epoch: 0127 train_loss= 0.42921 val_roc= 0.85113 val_ap= 0.87639\n",
      "Epoch: 0128 train_loss= 0.42879 val_roc= 0.83052 val_ap= 0.86481\n",
      "Epoch: 0129 train_loss= 0.42849 val_roc= 0.84392 val_ap= 0.87160\n",
      "Epoch: 0130 train_loss= 0.42842 val_roc= 0.85100 val_ap= 0.87594\n",
      "Test AP score: 0.8886719386037294\n",
      "Test ROC score: 0.8548819172646718\n",
      "Epoch: 0131 train_loss= 0.42818 val_roc= 0.83229 val_ap= 0.86404\n",
      "Epoch: 0132 train_loss= 0.42802 val_roc= 0.83235 val_ap= 0.86637\n",
      "Epoch: 0133 train_loss= 0.42768 val_roc= 0.83201 val_ap= 0.86143\n",
      "Epoch: 0134 train_loss= 0.42689 val_roc= 0.84308 val_ap= 0.86506\n",
      "Epoch: 0135 train_loss= 0.42678 val_roc= 0.83675 val_ap= 0.86546\n",
      "Epoch: 0136 train_loss= 0.42626 val_roc= 0.84419 val_ap= 0.86793\n",
      "Epoch: 0137 train_loss= 0.42623 val_roc= 0.84944 val_ap= 0.87565\n",
      "Epoch: 0138 train_loss= 0.42569 val_roc= 0.85822 val_ap= 0.88156\n",
      "Epoch: 0139 train_loss= 0.42503 val_roc= 0.84464 val_ap= 0.87423\n",
      "Epoch: 0140 train_loss= 0.42483 val_roc= 0.85975 val_ap= 0.87971\n",
      "Test AP score: 0.8990000394014732\n",
      "Test ROC score: 0.867012447385761\n",
      "Epoch: 0141 train_loss= 0.42424 val_roc= 0.84318 val_ap= 0.86764\n",
      "Epoch: 0142 train_loss= 0.42384 val_roc= 0.83769 val_ap= 0.86559\n",
      "Epoch: 0143 train_loss= 0.42397 val_roc= 0.83504 val_ap= 0.86838\n",
      "Epoch: 0144 train_loss= 0.42328 val_roc= 0.85252 val_ap= 0.87334\n",
      "Epoch: 0145 train_loss= 0.42345 val_roc= 0.84068 val_ap= 0.87258\n",
      "Epoch: 0146 train_loss= 0.42279 val_roc= 0.84015 val_ap= 0.87423\n",
      "Epoch: 0147 train_loss= 0.42240 val_roc= 0.83634 val_ap= 0.86595\n",
      "Epoch: 0148 train_loss= 0.42181 val_roc= 0.84630 val_ap= 0.87355\n",
      "Epoch: 0149 train_loss= 0.42179 val_roc= 0.84174 val_ap= 0.87139\n",
      "Epoch: 0150 train_loss= 0.42130 val_roc= 0.83650 val_ap= 0.86515\n",
      "Test AP score: 0.8958417056525073\n",
      "Test ROC score: 0.8632569879270799\n",
      "Epoch: 0151 train_loss= 0.42128 val_roc= 0.82017 val_ap= 0.85557\n",
      "Epoch: 0152 train_loss= 0.42073 val_roc= 0.82585 val_ap= 0.86124\n",
      "Epoch: 0153 train_loss= 0.42071 val_roc= 0.82819 val_ap= 0.86382\n",
      "Epoch: 0154 train_loss= 0.42020 val_roc= 0.83960 val_ap= 0.87167\n",
      "Epoch: 0155 train_loss= 0.41978 val_roc= 0.84123 val_ap= 0.87491\n",
      "Epoch: 0156 train_loss= 0.41965 val_roc= 0.83413 val_ap= 0.86500\n",
      "Epoch: 0157 train_loss= 0.41903 val_roc= 0.82952 val_ap= 0.86982\n",
      "Epoch: 0158 train_loss= 0.41904 val_roc= 0.84538 val_ap= 0.87158\n",
      "Epoch: 0159 train_loss= 0.41863 val_roc= 0.84382 val_ap= 0.87242\n",
      "Epoch: 0160 train_loss= 0.41840 val_roc= 0.82893 val_ap= 0.86498\n",
      "Test AP score: 0.9012115965653738\n",
      "Test ROC score: 0.8743649384831977\n",
      "Epoch: 0161 train_loss= 0.41792 val_roc= 0.84030 val_ap= 0.87370\n",
      "Epoch: 0162 train_loss= 0.41777 val_roc= 0.84257 val_ap= 0.87370\n",
      "Epoch: 0163 train_loss= 0.41757 val_roc= 0.83790 val_ap= 0.87559\n",
      "Epoch: 0164 train_loss= 0.41759 val_roc= 0.82096 val_ap= 0.86285\n",
      "Epoch: 0165 train_loss= 0.41708 val_roc= 0.84283 val_ap= 0.87214\n",
      "Epoch: 0166 train_loss= 0.41628 val_roc= 0.84194 val_ap= 0.87176\n",
      "Epoch: 0167 train_loss= 0.41667 val_roc= 0.83905 val_ap= 0.87391\n",
      "Epoch: 0168 train_loss= 0.41626 val_roc= 0.83974 val_ap= 0.87204\n",
      "Epoch: 0169 train_loss= 0.41610 val_roc= 0.82828 val_ap= 0.86004\n",
      "Epoch: 0170 train_loss= 0.41618 val_roc= 0.82549 val_ap= 0.86553\n",
      "Test AP score: 0.889553414438611\n",
      "Test ROC score: 0.8589254273050347\n",
      "Epoch: 0171 train_loss= 0.41617 val_roc= 0.83790 val_ap= 0.86596\n",
      "Epoch: 0172 train_loss= 0.41557 val_roc= 0.84171 val_ap= 0.87001\n",
      "Epoch: 0173 train_loss= 0.41556 val_roc= 0.84843 val_ap= 0.86770\n",
      "Epoch: 0174 train_loss= 0.41516 val_roc= 0.83050 val_ap= 0.86368\n",
      "Epoch: 0175 train_loss= 0.41491 val_roc= 0.83763 val_ap= 0.86880\n",
      "Epoch: 0176 train_loss= 0.41508 val_roc= 0.83783 val_ap= 0.87004\n",
      "Epoch: 0177 train_loss= 0.41493 val_roc= 0.83535 val_ap= 0.86794\n",
      "Epoch: 0178 train_loss= 0.41448 val_roc= 0.82845 val_ap= 0.86844\n",
      "Epoch: 0179 train_loss= 0.41443 val_roc= 0.82725 val_ap= 0.86670\n",
      "Epoch: 0180 train_loss= 0.41461 val_roc= 0.83698 val_ap= 0.86909\n",
      "Test AP score: 0.895685724496692\n",
      "Test ROC score: 0.8657594273554436\n",
      "Epoch: 0181 train_loss= 0.41364 val_roc= 0.82551 val_ap= 0.86187\n",
      "Epoch: 0182 train_loss= 0.41429 val_roc= 0.82391 val_ap= 0.86021\n",
      "Epoch: 0183 train_loss= 0.41386 val_roc= 0.82546 val_ap= 0.86657\n",
      "Epoch: 0184 train_loss= 0.41340 val_roc= 0.83461 val_ap= 0.87008\n",
      "Epoch: 0185 train_loss= 0.41305 val_roc= 0.83426 val_ap= 0.86903\n",
      "Epoch: 0186 train_loss= 0.41365 val_roc= 0.82982 val_ap= 0.86935\n",
      "Epoch: 0187 train_loss= 0.41308 val_roc= 0.82845 val_ap= 0.86355\n",
      "Epoch: 0188 train_loss= 0.41282 val_roc= 0.83796 val_ap= 0.87182\n",
      "Epoch: 0189 train_loss= 0.41315 val_roc= 0.83347 val_ap= 0.87010\n",
      "Epoch: 0190 train_loss= 0.41303 val_roc= 0.82809 val_ap= 0.86450\n",
      "Test AP score: 0.8953497294598378\n",
      "Test ROC score: 0.8643119731824908\n",
      "Epoch: 0191 train_loss= 0.41245 val_roc= 0.82832 val_ap= 0.86420\n",
      "Epoch: 0192 train_loss= 0.41238 val_roc= 0.83258 val_ap= 0.86800\n",
      "Epoch: 0193 train_loss= 0.41219 val_roc= 0.82680 val_ap= 0.86531\n",
      "Epoch: 0194 train_loss= 0.41193 val_roc= 0.82183 val_ap= 0.85753\n",
      "Epoch: 0195 train_loss= 0.41193 val_roc= 0.82316 val_ap= 0.86336\n",
      "Epoch: 0196 train_loss= 0.41136 val_roc= 0.81620 val_ap= 0.85639\n",
      "Epoch: 0197 train_loss= 0.41171 val_roc= 0.82885 val_ap= 0.86215\n",
      "Epoch: 0198 train_loss= 0.41113 val_roc= 0.82225 val_ap= 0.86400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0199 train_loss= 0.41133 val_roc= 0.84240 val_ap= 0.87163\n",
      "Epoch: 0200 train_loss= 0.41131 val_roc= 0.82816 val_ap= 0.86496\n",
      "Test AP score: 0.8892826013273818\n",
      "Test ROC score: 0.8517637697179625\n"
     ]
    }
   ],
   "source": [
    "runner.erun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c11174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
