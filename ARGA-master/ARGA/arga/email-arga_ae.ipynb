{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65775001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T11:48:29.379451Z",
     "start_time": "2022-05-08T11:48:22.739835Z"
    }
   },
   "outputs": [],
   "source": [
    "import settings\n",
    "from link_prediction import Link_pred_Runner\n",
    "dataname = 'email'       # 'wiki' or 'email' or 'cora' or 'citeseer'\n",
    "model = 'arga_ae'     # 'arga_ae' or 'arga_vae'\n",
    "task = 'link_prediction'         # 'clustering' or 'link_prediction'\n",
    "settings = settings.get_settings(dataname, model, task)\n",
    "runner = Link_pred_Runner(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254bd93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T11:49:27.775157Z",
     "start_time": "2022-05-08T11:48:30.917060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:143: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:145: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:147: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:27: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\.conda\\envs\\tf1.x\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:28: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:37: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:38: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.73072 val_roc= 0.60613 val_ap= 0.60211\n",
      "Epoch: 0002 train_loss= 0.73081 val_roc= 0.55786 val_ap= 0.54390\n",
      "Epoch: 0003 train_loss= 0.73027 val_roc= 0.54989 val_ap= 0.52519\n",
      "Epoch: 0004 train_loss= 0.73006 val_roc= 0.62846 val_ap= 0.57723\n",
      "Epoch: 0005 train_loss= 0.72933 val_roc= 0.60694 val_ap= 0.58269\n",
      "Epoch: 0006 train_loss= 0.72919 val_roc= 0.66677 val_ap= 0.64401\n",
      "Epoch: 0007 train_loss= 0.72782 val_roc= 0.64452 val_ap= 0.60814\n",
      "Epoch: 0008 train_loss= 0.72701 val_roc= 0.59818 val_ap= 0.57097\n",
      "Epoch: 0009 train_loss= 0.72467 val_roc= 0.67635 val_ap= 0.64507\n",
      "Epoch: 0010 train_loss= 0.72169 val_roc= 0.67302 val_ap= 0.63299\n",
      "Test AP score: 0.7100546316877683\n",
      "Test ROC score: 0.7157764497937884\n",
      "Epoch: 0011 train_loss= 0.71902 val_roc= 0.70575 val_ap= 0.69232\n",
      "Epoch: 0012 train_loss= 0.71293 val_roc= 0.74561 val_ap= 0.73625\n",
      "Epoch: 0013 train_loss= 0.70478 val_roc= 0.73955 val_ap= 0.71747\n",
      "Epoch: 0014 train_loss= 0.69882 val_roc= 0.75764 val_ap= 0.73849\n",
      "Epoch: 0015 train_loss= 0.68890 val_roc= 0.74776 val_ap= 0.73599\n",
      "Epoch: 0016 train_loss= 0.67971 val_roc= 0.74088 val_ap= 0.74407\n",
      "Epoch: 0017 train_loss= 0.67239 val_roc= 0.75961 val_ap= 0.74571\n",
      "Epoch: 0018 train_loss= 0.66872 val_roc= 0.74193 val_ap= 0.72328\n",
      "Epoch: 0019 train_loss= 0.66485 val_roc= 0.75787 val_ap= 0.73978\n",
      "Epoch: 0020 train_loss= 0.66287 val_roc= 0.75610 val_ap= 0.74552\n",
      "Test AP score: 0.7921127341263245\n",
      "Test ROC score: 0.7873798501809611\n",
      "Epoch: 0021 train_loss= 0.66065 val_roc= 0.75054 val_ap= 0.73905\n",
      "Epoch: 0022 train_loss= 0.65756 val_roc= 0.75400 val_ap= 0.75058\n",
      "Epoch: 0023 train_loss= 0.65642 val_roc= 0.74266 val_ap= 0.73431\n",
      "Epoch: 0024 train_loss= 0.65436 val_roc= 0.74294 val_ap= 0.73691\n",
      "Epoch: 0025 train_loss= 0.65394 val_roc= 0.74897 val_ap= 0.74135\n",
      "Epoch: 0026 train_loss= 0.65072 val_roc= 0.75634 val_ap= 0.74518\n",
      "Epoch: 0027 train_loss= 0.64917 val_roc= 0.74908 val_ap= 0.73758\n",
      "Epoch: 0028 train_loss= 0.64512 val_roc= 0.75288 val_ap= 0.74201\n",
      "Epoch: 0029 train_loss= 0.64388 val_roc= 0.74904 val_ap= 0.73240\n",
      "Epoch: 0030 train_loss= 0.64143 val_roc= 0.74728 val_ap= 0.74712\n",
      "Test AP score: 0.7846242889038864\n",
      "Test ROC score: 0.7810975507112197\n",
      "Epoch: 0031 train_loss= 0.64126 val_roc= 0.75925 val_ap= 0.74344\n",
      "Epoch: 0032 train_loss= 0.63734 val_roc= 0.75656 val_ap= 0.75047\n",
      "Epoch: 0033 train_loss= 0.63554 val_roc= 0.75960 val_ap= 0.74783\n",
      "Epoch: 0034 train_loss= 0.63328 val_roc= 0.76030 val_ap= 0.74206\n",
      "Epoch: 0035 train_loss= 0.62887 val_roc= 0.74674 val_ap= 0.73999\n",
      "Epoch: 0036 train_loss= 0.62720 val_roc= 0.74382 val_ap= 0.73794\n",
      "Epoch: 0037 train_loss= 0.62634 val_roc= 0.75311 val_ap= 0.74752\n",
      "Epoch: 0038 train_loss= 0.62299 val_roc= 0.75787 val_ap= 0.73757\n",
      "Epoch: 0039 train_loss= 0.62099 val_roc= 0.75191 val_ap= 0.74469\n",
      "Epoch: 0040 train_loss= 0.61447 val_roc= 0.75633 val_ap= 0.73729\n",
      "Test AP score: 0.7766880478293795\n",
      "Test ROC score: 0.7710007575119939\n",
      "Epoch: 0041 train_loss= 0.61486 val_roc= 0.75514 val_ap= 0.74102\n",
      "Epoch: 0042 train_loss= 0.60918 val_roc= 0.74988 val_ap= 0.74202\n",
      "Epoch: 0043 train_loss= 0.60579 val_roc= 0.76491 val_ap= 0.75783\n",
      "Epoch: 0044 train_loss= 0.60236 val_roc= 0.76042 val_ap= 0.75247\n",
      "Epoch: 0045 train_loss= 0.60003 val_roc= 0.75074 val_ap= 0.74000\n",
      "Epoch: 0046 train_loss= 0.59651 val_roc= 0.76631 val_ap= 0.75885\n",
      "Epoch: 0047 train_loss= 0.59294 val_roc= 0.77745 val_ap= 0.77252\n",
      "Epoch: 0048 train_loss= 0.59055 val_roc= 0.77115 val_ap= 0.75600\n",
      "Epoch: 0049 train_loss= 0.58561 val_roc= 0.77501 val_ap= 0.76965\n",
      "Epoch: 0050 train_loss= 0.58556 val_roc= 0.78225 val_ap= 0.76972\n",
      "Test AP score: 0.8019842577135299\n",
      "Test ROC score: 0.7888814072889487\n",
      "Epoch: 0051 train_loss= 0.57972 val_roc= 0.77395 val_ap= 0.76556\n",
      "Epoch: 0052 train_loss= 0.57677 val_roc= 0.77464 val_ap= 0.77343\n",
      "Epoch: 0053 train_loss= 0.57383 val_roc= 0.77641 val_ap= 0.77826\n",
      "Epoch: 0054 train_loss= 0.56862 val_roc= 0.78433 val_ap= 0.78569\n",
      "Epoch: 0055 train_loss= 0.56697 val_roc= 0.78013 val_ap= 0.77518\n",
      "Epoch: 0056 train_loss= 0.56392 val_roc= 0.78613 val_ap= 0.79107\n",
      "Epoch: 0057 train_loss= 0.55966 val_roc= 0.78779 val_ap= 0.78611\n",
      "Epoch: 0058 train_loss= 0.55571 val_roc= 0.79414 val_ap= 0.79835\n",
      "Epoch: 0059 train_loss= 0.55188 val_roc= 0.78741 val_ap= 0.78434\n",
      "Epoch: 0060 train_loss= 0.55050 val_roc= 0.79809 val_ap= 0.79895\n",
      "Test AP score: 0.8268817315635747\n",
      "Test ROC score: 0.8120141402238868\n",
      "Epoch: 0061 train_loss= 0.54873 val_roc= 0.79344 val_ap= 0.79849\n",
      "Epoch: 0062 train_loss= 0.54449 val_roc= 0.79364 val_ap= 0.79040\n",
      "Epoch: 0063 train_loss= 0.54036 val_roc= 0.79706 val_ap= 0.79424\n",
      "Epoch: 0064 train_loss= 0.53954 val_roc= 0.80976 val_ap= 0.82062\n",
      "Epoch: 0065 train_loss= 0.53712 val_roc= 0.80386 val_ap= 0.80927\n",
      "Epoch: 0066 train_loss= 0.53545 val_roc= 0.80129 val_ap= 0.80840\n",
      "Epoch: 0067 train_loss= 0.53272 val_roc= 0.80684 val_ap= 0.81372\n",
      "Epoch: 0068 train_loss= 0.53042 val_roc= 0.80465 val_ap= 0.81469\n",
      "Epoch: 0069 train_loss= 0.52883 val_roc= 0.80584 val_ap= 0.81117\n",
      "Epoch: 0070 train_loss= 0.52650 val_roc= 0.81420 val_ap= 0.81300\n",
      "Test AP score: 0.830381890190449\n",
      "Test ROC score: 0.8199057318407542\n",
      "Epoch: 0071 train_loss= 0.52747 val_roc= 0.81442 val_ap= 0.83191\n",
      "Epoch: 0072 train_loss= 0.52462 val_roc= 0.81549 val_ap= 0.82333\n",
      "Epoch: 0073 train_loss= 0.52149 val_roc= 0.81650 val_ap= 0.82744\n",
      "Epoch: 0074 train_loss= 0.52067 val_roc= 0.80974 val_ap= 0.81439\n",
      "Epoch: 0075 train_loss= 0.51755 val_roc= 0.82096 val_ap= 0.82519\n",
      "Epoch: 0076 train_loss= 0.51605 val_roc= 0.81124 val_ap= 0.81345\n",
      "Epoch: 0077 train_loss= 0.51524 val_roc= 0.80950 val_ap= 0.81938\n",
      "Epoch: 0078 train_loss= 0.51192 val_roc= 0.82097 val_ap= 0.82336\n",
      "Epoch: 0079 train_loss= 0.51014 val_roc= 0.82749 val_ap= 0.83697\n",
      "Epoch: 0080 train_loss= 0.50863 val_roc= 0.81793 val_ap= 0.82668\n",
      "Test AP score: 0.8434643345801469\n",
      "Test ROC score: 0.8319383890244929\n",
      "Epoch: 0081 train_loss= 0.50715 val_roc= 0.82756 val_ap= 0.84316\n",
      "Epoch: 0082 train_loss= 0.50426 val_roc= 0.83601 val_ap= 0.85044\n",
      "Epoch: 0083 train_loss= 0.50268 val_roc= 0.83859 val_ap= 0.84586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0084 train_loss= 0.50188 val_roc= 0.83898 val_ap= 0.84103\n",
      "Epoch: 0085 train_loss= 0.49960 val_roc= 0.83084 val_ap= 0.83098\n",
      "Epoch: 0086 train_loss= 0.49945 val_roc= 0.83432 val_ap= 0.84951\n",
      "Epoch: 0087 train_loss= 0.49752 val_roc= 0.84574 val_ap= 0.85075\n",
      "Epoch: 0088 train_loss= 0.49626 val_roc= 0.85005 val_ap= 0.85186\n",
      "Epoch: 0089 train_loss= 0.49593 val_roc= 0.84551 val_ap= 0.85433\n",
      "Epoch: 0090 train_loss= 0.49496 val_roc= 0.84134 val_ap= 0.84743\n",
      "Test AP score: 0.8494862041732875\n",
      "Test ROC score: 0.8395202424038379\n",
      "Epoch: 0091 train_loss= 0.49367 val_roc= 0.84383 val_ap= 0.84692\n",
      "Epoch: 0092 train_loss= 0.49316 val_roc= 0.83990 val_ap= 0.84181\n",
      "Epoch: 0093 train_loss= 0.49166 val_roc= 0.84570 val_ap= 0.84584\n",
      "Epoch: 0094 train_loss= 0.49102 val_roc= 0.84501 val_ap= 0.84463\n",
      "Epoch: 0095 train_loss= 0.49035 val_roc= 0.84851 val_ap= 0.84634\n",
      "Epoch: 0096 train_loss= 0.49042 val_roc= 0.84935 val_ap= 0.84954\n",
      "Epoch: 0097 train_loss= 0.48990 val_roc= 0.84962 val_ap= 0.85021\n",
      "Epoch: 0098 train_loss= 0.48922 val_roc= 0.84997 val_ap= 0.84701\n",
      "Epoch: 0099 train_loss= 0.48852 val_roc= 0.84974 val_ap= 0.85239\n",
      "Epoch: 0100 train_loss= 0.48792 val_roc= 0.84320 val_ap= 0.84368\n",
      "Test AP score: 0.8550933684815295\n",
      "Test ROC score: 0.8441023482871812\n",
      "Epoch: 0101 train_loss= 0.48712 val_roc= 0.84211 val_ap= 0.83864\n",
      "Epoch: 0102 train_loss= 0.48728 val_roc= 0.85390 val_ap= 0.85287\n",
      "Epoch: 0103 train_loss= 0.48648 val_roc= 0.84505 val_ap= 0.84370\n",
      "Epoch: 0104 train_loss= 0.48632 val_roc= 0.85364 val_ap= 0.85261\n",
      "Epoch: 0105 train_loss= 0.48594 val_roc= 0.85014 val_ap= 0.85241\n",
      "Epoch: 0106 train_loss= 0.48496 val_roc= 0.85221 val_ap= 0.85335\n",
      "Epoch: 0107 train_loss= 0.48393 val_roc= 0.84437 val_ap= 0.84339\n",
      "Epoch: 0108 train_loss= 0.48302 val_roc= 0.85308 val_ap= 0.84065\n",
      "Epoch: 0109 train_loss= 0.48279 val_roc= 0.84647 val_ap= 0.85187\n",
      "Epoch: 0110 train_loss= 0.48197 val_roc= 0.84697 val_ap= 0.84676\n",
      "Test AP score: 0.8615136147511895\n",
      "Test ROC score: 0.8508189546334484\n",
      "Epoch: 0111 train_loss= 0.48146 val_roc= 0.84729 val_ap= 0.84887\n",
      "Epoch: 0112 train_loss= 0.48032 val_roc= 0.85197 val_ap= 0.85466\n",
      "Epoch: 0113 train_loss= 0.48055 val_roc= 0.84894 val_ap= 0.85378\n",
      "Epoch: 0114 train_loss= 0.47956 val_roc= 0.84895 val_ap= 0.84297\n",
      "Epoch: 0115 train_loss= 0.47911 val_roc= 0.85356 val_ap= 0.85176\n",
      "Epoch: 0116 train_loss= 0.47762 val_roc= 0.85896 val_ap= 0.84696\n",
      "Epoch: 0117 train_loss= 0.47745 val_roc= 0.85529 val_ap= 0.85280\n",
      "Epoch: 0118 train_loss= 0.47709 val_roc= 0.84436 val_ap= 0.84647\n",
      "Epoch: 0119 train_loss= 0.47499 val_roc= 0.84943 val_ap= 0.85082\n",
      "Epoch: 0120 train_loss= 0.47554 val_roc= 0.85606 val_ap= 0.85992\n",
      "Test AP score: 0.8606556490192584\n",
      "Test ROC score: 0.8496776365625789\n",
      "Epoch: 0121 train_loss= 0.47423 val_roc= 0.84610 val_ap= 0.83869\n",
      "Epoch: 0122 train_loss= 0.47370 val_roc= 0.84825 val_ap= 0.85237\n",
      "Epoch: 0123 train_loss= 0.47332 val_roc= 0.86096 val_ap= 0.85903\n",
      "Epoch: 0124 train_loss= 0.47269 val_roc= 0.85993 val_ap= 0.85713\n",
      "Epoch: 0125 train_loss= 0.47237 val_roc= 0.85444 val_ap= 0.85595\n",
      "Epoch: 0126 train_loss= 0.47200 val_roc= 0.85368 val_ap= 0.85034\n",
      "Epoch: 0127 train_loss= 0.47138 val_roc= 0.85010 val_ap= 0.85563\n",
      "Epoch: 0128 train_loss= 0.47088 val_roc= 0.85698 val_ap= 0.85877\n",
      "Epoch: 0129 train_loss= 0.47023 val_roc= 0.86736 val_ap= 0.86396\n",
      "Epoch: 0130 train_loss= 0.47059 val_roc= 0.85940 val_ap= 0.85965\n",
      "Test AP score: 0.861848509679634\n",
      "Test ROC score: 0.8542328086861376\n",
      "Epoch: 0131 train_loss= 0.47048 val_roc= 0.85448 val_ap= 0.85137\n",
      "Epoch: 0132 train_loss= 0.46904 val_roc= 0.86077 val_ap= 0.86301\n",
      "Epoch: 0133 train_loss= 0.46892 val_roc= 0.85229 val_ap= 0.85246\n",
      "Epoch: 0134 train_loss= 0.46830 val_roc= 0.85662 val_ap= 0.85558\n",
      "Epoch: 0135 train_loss= 0.46832 val_roc= 0.86275 val_ap= 0.86185\n",
      "Epoch: 0136 train_loss= 0.46805 val_roc= 0.85368 val_ap= 0.85358\n",
      "Epoch: 0137 train_loss= 0.46678 val_roc= 0.86097 val_ap= 0.86295\n",
      "Epoch: 0138 train_loss= 0.46682 val_roc= 0.84925 val_ap= 0.85060\n",
      "Epoch: 0139 train_loss= 0.46645 val_roc= 0.85886 val_ap= 0.86033\n",
      "Epoch: 0140 train_loss= 0.46587 val_roc= 0.85724 val_ap= 0.85448\n",
      "Test AP score: 0.8724735938785697\n",
      "Test ROC score: 0.8672586482619308\n",
      "Epoch: 0141 train_loss= 0.46507 val_roc= 0.86171 val_ap= 0.86281\n",
      "Epoch: 0142 train_loss= 0.46553 val_roc= 0.84768 val_ap= 0.84777\n",
      "Epoch: 0143 train_loss= 0.46436 val_roc= 0.86363 val_ap= 0.85943\n",
      "Epoch: 0144 train_loss= 0.46420 val_roc= 0.85117 val_ap= 0.85369\n",
      "Epoch: 0145 train_loss= 0.46323 val_roc= 0.85191 val_ap= 0.84940\n",
      "Epoch: 0146 train_loss= 0.46332 val_roc= 0.86421 val_ap= 0.86259\n",
      "Epoch: 0147 train_loss= 0.46277 val_roc= 0.84829 val_ap= 0.85088\n",
      "Epoch: 0148 train_loss= 0.46267 val_roc= 0.85477 val_ap= 0.85510\n",
      "Epoch: 0149 train_loss= 0.46267 val_roc= 0.85421 val_ap= 0.85475\n",
      "Epoch: 0150 train_loss= 0.46214 val_roc= 0.87457 val_ap= 0.87311\n",
      "Test AP score: 0.8706496660124781\n",
      "Test ROC score: 0.86167999326656\n",
      "Epoch: 0151 train_loss= 0.46192 val_roc= 0.86669 val_ap= 0.86444\n",
      "Epoch: 0152 train_loss= 0.46160 val_roc= 0.86432 val_ap= 0.85627\n",
      "Epoch: 0153 train_loss= 0.46206 val_roc= 0.86751 val_ap= 0.86294\n",
      "Epoch: 0154 train_loss= 0.46126 val_roc= 0.84339 val_ap= 0.84573\n",
      "Epoch: 0155 train_loss= 0.46121 val_roc= 0.86976 val_ap= 0.86436\n",
      "Epoch: 0156 train_loss= 0.46076 val_roc= 0.86052 val_ap= 0.85991\n",
      "Epoch: 0157 train_loss= 0.46025 val_roc= 0.86634 val_ap= 0.86251\n",
      "Epoch: 0158 train_loss= 0.46055 val_roc= 0.85878 val_ap= 0.85786\n",
      "Epoch: 0159 train_loss= 0.45955 val_roc= 0.85516 val_ap= 0.85594\n",
      "Epoch: 0160 train_loss= 0.45963 val_roc= 0.86578 val_ap= 0.85973\n",
      "Test AP score: 0.8661941735217004\n",
      "Test ROC score: 0.8594242908846057\n",
      "Epoch: 0161 train_loss= 0.45993 val_roc= 0.85802 val_ap= 0.84764\n",
      "Epoch: 0162 train_loss= 0.45960 val_roc= 0.86344 val_ap= 0.86392\n",
      "Epoch: 0163 train_loss= 0.45915 val_roc= 0.86343 val_ap= 0.86218\n",
      "Epoch: 0164 train_loss= 0.45873 val_roc= 0.85728 val_ap= 0.85045\n",
      "Epoch: 0165 train_loss= 0.45825 val_roc= 0.86036 val_ap= 0.85239\n",
      "Epoch: 0166 train_loss= 0.45852 val_roc= 0.85518 val_ap= 0.85847\n",
      "Epoch: 0167 train_loss= 0.45871 val_roc= 0.85240 val_ap= 0.84851\n",
      "Epoch: 0168 train_loss= 0.45807 val_roc= 0.85758 val_ap= 0.86398\n",
      "Epoch: 0169 train_loss= 0.45836 val_roc= 0.85658 val_ap= 0.85552\n",
      "Epoch: 0170 train_loss= 0.45706 val_roc= 0.85833 val_ap= 0.85989\n",
      "Test AP score: 0.8729746541449541\n",
      "Test ROC score: 0.8644238700446091\n",
      "Epoch: 0171 train_loss= 0.45715 val_roc= 0.85663 val_ap= 0.85192\n",
      "Epoch: 0172 train_loss= 0.45820 val_roc= 0.85331 val_ap= 0.84924\n",
      "Epoch: 0173 train_loss= 0.45718 val_roc= 0.85367 val_ap= 0.85032\n",
      "Epoch: 0174 train_loss= 0.45649 val_roc= 0.85896 val_ap= 0.86211\n",
      "Epoch: 0175 train_loss= 0.45676 val_roc= 0.85837 val_ap= 0.85400\n",
      "Epoch: 0176 train_loss= 0.45564 val_roc= 0.86471 val_ap= 0.85630\n",
      "Epoch: 0177 train_loss= 0.45596 val_roc= 0.85393 val_ap= 0.85606\n",
      "Epoch: 0178 train_loss= 0.45507 val_roc= 0.85900 val_ap= 0.84732\n",
      "Epoch: 0179 train_loss= 0.45517 val_roc= 0.85318 val_ap= 0.85448\n",
      "Epoch: 0180 train_loss= 0.45530 val_roc= 0.85163 val_ap= 0.85546\n",
      "Test AP score: 0.8673303571653211\n",
      "Test ROC score: 0.8625385068596918\n",
      "Epoch: 0181 train_loss= 0.45497 val_roc= 0.85737 val_ap= 0.84539\n",
      "Epoch: 0182 train_loss= 0.45470 val_roc= 0.85974 val_ap= 0.85255\n",
      "Epoch: 0183 train_loss= 0.45422 val_roc= 0.85252 val_ap= 0.84980\n",
      "Epoch: 0184 train_loss= 0.45394 val_roc= 0.85208 val_ap= 0.85010\n",
      "Epoch: 0185 train_loss= 0.45271 val_roc= 0.85821 val_ap= 0.85998\n",
      "Epoch: 0186 train_loss= 0.45389 val_roc= 0.85181 val_ap= 0.85123\n",
      "Epoch: 0187 train_loss= 0.45330 val_roc= 0.85609 val_ap= 0.85490\n",
      "Epoch: 0188 train_loss= 0.45367 val_roc= 0.86156 val_ap= 0.85450\n",
      "Epoch: 0189 train_loss= 0.45278 val_roc= 0.85721 val_ap= 0.85715\n",
      "Epoch: 0190 train_loss= 0.45263 val_roc= 0.84926 val_ap= 0.85289\n",
      "Test AP score: 0.8783527929402339\n",
      "Test ROC score: 0.8687534719299723\n",
      "Epoch: 0191 train_loss= 0.45222 val_roc= 0.85225 val_ap= 0.85516\n",
      "Epoch: 0192 train_loss= 0.45136 val_roc= 0.86001 val_ap= 0.85837\n",
      "Epoch: 0193 train_loss= 0.45179 val_roc= 0.85258 val_ap= 0.85753\n",
      "Epoch: 0194 train_loss= 0.45163 val_roc= 0.85318 val_ap= 0.85399\n",
      "Epoch: 0195 train_loss= 0.45042 val_roc= 0.86154 val_ap= 0.86061\n",
      "Epoch: 0196 train_loss= 0.45107 val_roc= 0.85167 val_ap= 0.85743\n",
      "Epoch: 0197 train_loss= 0.44974 val_roc= 0.85620 val_ap= 0.85709\n",
      "Epoch: 0198 train_loss= 0.45060 val_roc= 0.86421 val_ap= 0.86210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0199 train_loss= 0.45021 val_roc= 0.85252 val_ap= 0.84993\n",
      "Epoch: 0200 train_loss= 0.44951 val_roc= 0.86001 val_ap= 0.85161\n",
      "Test AP score: 0.8769289200446688\n",
      "Test ROC score: 0.8652453497180371\n"
     ]
    }
   ],
   "source": [
    "runner.erun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c11174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
