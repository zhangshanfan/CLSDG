{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65775001",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T10:11:38.325533Z",
     "start_time": "2022-05-07T10:11:33.051723Z"
    }
   },
   "outputs": [],
   "source": [
    "import settings\n",
    "from link_prediction import Link_pred_Runner\n",
    "dataname = 'cora'       \n",
    "model = 'arga_vae'     # 'arga_ae' or 'arga_vae'\n",
    "task = 'link_prediction'         # 'clustering' or 'link_prediction'\n",
    "settings = settings.get_settings(dataname, model, task)\n",
    "runner = Link_pred_Runner(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254bd93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-07T10:15:40.199353Z",
     "start_time": "2022-05-07T10:11:38.362547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:143: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:145: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:147: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:27: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\model.py:126: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\constructor.py:94: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\.conda\\envs\\tf1.x\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:67: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:68: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\zhangshanfan\\Desktop\\对比方法\\ARGA-master\\ARGA\\arga\\optimizer.py:75: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 2.41562 val_roc= 0.65465 val_ap= 0.66482\n",
      "Epoch: 0002 train_loss= 2.34593 val_roc= 0.65550 val_ap= 0.66541\n",
      "Epoch: 0003 train_loss= 2.34069 val_roc= 0.65790 val_ap= 0.66737\n",
      "Epoch: 0004 train_loss= 2.31077 val_roc= 0.66036 val_ap= 0.67167\n",
      "Epoch: 0005 train_loss= 2.28625 val_roc= 0.66763 val_ap= 0.67776\n",
      "Epoch: 0006 train_loss= 2.29468 val_roc= 0.67297 val_ap= 0.68462\n",
      "Epoch: 0007 train_loss= 2.31910 val_roc= 0.67794 val_ap= 0.68984\n",
      "Epoch: 0008 train_loss= 2.28700 val_roc= 0.68184 val_ap= 0.69367\n",
      "Epoch: 0009 train_loss= 2.29818 val_roc= 0.68544 val_ap= 0.69610\n",
      "Epoch: 0010 train_loss= 2.27581 val_roc= 0.68589 val_ap= 0.69669\n",
      "Test AP score: 0.6987683593140068\n",
      "Test ROC score: 0.6930695030047276\n",
      "Epoch: 0011 train_loss= 2.26405 val_roc= 0.68669 val_ap= 0.69860\n",
      "Epoch: 0012 train_loss= 2.24461 val_roc= 0.68539 val_ap= 0.69895\n",
      "Epoch: 0013 train_loss= 2.29949 val_roc= 0.68395 val_ap= 0.69908\n",
      "Epoch: 0014 train_loss= 2.23993 val_roc= 0.68346 val_ap= 0.70038\n",
      "Epoch: 0015 train_loss= 2.20594 val_roc= 0.68260 val_ap= 0.70164\n",
      "Epoch: 0016 train_loss= 2.20284 val_roc= 0.68002 val_ap= 0.69989\n",
      "Epoch: 0017 train_loss= 2.19529 val_roc= 0.67792 val_ap= 0.69867\n",
      "Epoch: 0018 train_loss= 2.15658 val_roc= 0.67668 val_ap= 0.69844\n",
      "Epoch: 0019 train_loss= 2.22620 val_roc= 0.67524 val_ap= 0.69881\n",
      "Epoch: 0020 train_loss= 2.07985 val_roc= 0.67371 val_ap= 0.69852\n",
      "Test AP score: 0.7109164364960862\n",
      "Test ROC score: 0.6979519603642399\n",
      "Epoch: 0021 train_loss= 2.10550 val_roc= 0.67257 val_ap= 0.69846\n",
      "Epoch: 0022 train_loss= 2.09681 val_roc= 0.67091 val_ap= 0.69799\n",
      "Epoch: 0023 train_loss= 2.07356 val_roc= 0.66890 val_ap= 0.69721\n",
      "Epoch: 0024 train_loss= 2.02677 val_roc= 0.66754 val_ap= 0.69639\n",
      "Epoch: 0025 train_loss= 2.02461 val_roc= 0.66570 val_ap= 0.69554\n",
      "Epoch: 0026 train_loss= 2.04361 val_roc= 0.66447 val_ap= 0.69572\n",
      "Epoch: 0027 train_loss= 1.94255 val_roc= 0.66319 val_ap= 0.69522\n",
      "Epoch: 0028 train_loss= 1.91496 val_roc= 0.66254 val_ap= 0.69501\n",
      "Epoch: 0029 train_loss= 1.90804 val_roc= 0.66180 val_ap= 0.69463\n",
      "Epoch: 0030 train_loss= 1.92025 val_roc= 0.66131 val_ap= 0.69418\n",
      "Test AP score: 0.7093791405081735\n",
      "Test ROC score: 0.686264308012487\n",
      "Epoch: 0031 train_loss= 1.86525 val_roc= 0.66135 val_ap= 0.69433\n",
      "Epoch: 0032 train_loss= 1.85746 val_roc= 0.66058 val_ap= 0.69419\n",
      "Epoch: 0033 train_loss= 1.80773 val_roc= 0.66005 val_ap= 0.69418\n",
      "Epoch: 0034 train_loss= 1.76554 val_roc= 0.65999 val_ap= 0.69476\n",
      "Epoch: 0035 train_loss= 1.74567 val_roc= 0.65979 val_ap= 0.69486\n",
      "Epoch: 0036 train_loss= 1.74101 val_roc= 0.65965 val_ap= 0.69508\n",
      "Epoch: 0037 train_loss= 1.71876 val_roc= 0.65930 val_ap= 0.69464\n",
      "Epoch: 0038 train_loss= 1.70071 val_roc= 0.65953 val_ap= 0.69466\n",
      "Epoch: 0039 train_loss= 1.64670 val_roc= 0.65959 val_ap= 0.69453\n",
      "Epoch: 0040 train_loss= 1.61828 val_roc= 0.65924 val_ap= 0.69443\n",
      "Test AP score: 0.7087202947380353\n",
      "Test ROC score: 0.6814574639306662\n",
      "Epoch: 0041 train_loss= 1.60196 val_roc= 0.65895 val_ap= 0.69437\n",
      "Epoch: 0042 train_loss= 1.58735 val_roc= 0.65871 val_ap= 0.69391\n",
      "Epoch: 0043 train_loss= 1.55954 val_roc= 0.65856 val_ap= 0.69373\n",
      "Epoch: 0044 train_loss= 1.53890 val_roc= 0.65847 val_ap= 0.69379\n",
      "Epoch: 0045 train_loss= 1.52888 val_roc= 0.65860 val_ap= 0.69396\n",
      "Epoch: 0046 train_loss= 1.49954 val_roc= 0.65866 val_ap= 0.69413\n",
      "Epoch: 0047 train_loss= 1.47596 val_roc= 0.65817 val_ap= 0.69376\n",
      "Epoch: 0048 train_loss= 1.47158 val_roc= 0.65829 val_ap= 0.69387\n",
      "Epoch: 0049 train_loss= 1.45219 val_roc= 0.65847 val_ap= 0.69392\n",
      "Epoch: 0050 train_loss= 1.42833 val_roc= 0.65813 val_ap= 0.69379\n",
      "Test AP score: 0.7112678039379843\n",
      "Test ROC score: 0.6805933121856198\n",
      "Epoch: 0051 train_loss= 1.42101 val_roc= 0.65787 val_ap= 0.69371\n",
      "Epoch: 0052 train_loss= 1.41452 val_roc= 0.65752 val_ap= 0.69341\n",
      "Epoch: 0053 train_loss= 1.38280 val_roc= 0.65710 val_ap= 0.69285\n",
      "Epoch: 0054 train_loss= 1.35452 val_roc= 0.65693 val_ap= 0.69263\n",
      "Epoch: 0055 train_loss= 1.32926 val_roc= 0.65651 val_ap= 0.69244\n",
      "Epoch: 0056 train_loss= 1.31123 val_roc= 0.65655 val_ap= 0.69268\n",
      "Epoch: 0057 train_loss= 1.32505 val_roc= 0.65670 val_ap= 0.69261\n",
      "Epoch: 0058 train_loss= 1.31576 val_roc= 0.65685 val_ap= 0.69272\n",
      "Epoch: 0059 train_loss= 1.28944 val_roc= 0.65690 val_ap= 0.69276\n",
      "Epoch: 0060 train_loss= 1.28015 val_roc= 0.65704 val_ap= 0.69211\n",
      "Test AP score: 0.7129223553731403\n",
      "Test ROC score: 0.6795419275624799\n",
      "Epoch: 0061 train_loss= 1.26853 val_roc= 0.65764 val_ap= 0.69258\n",
      "Epoch: 0062 train_loss= 1.24937 val_roc= 0.65779 val_ap= 0.69246\n",
      "Epoch: 0063 train_loss= 1.24703 val_roc= 0.65817 val_ap= 0.69233\n",
      "Epoch: 0064 train_loss= 1.23667 val_roc= 0.65839 val_ap= 0.69253\n",
      "Epoch: 0065 train_loss= 1.21641 val_roc= 0.65852 val_ap= 0.69262\n",
      "Epoch: 0066 train_loss= 1.19959 val_roc= 0.65885 val_ap= 0.69211\n",
      "Epoch: 0067 train_loss= 1.19617 val_roc= 0.65940 val_ap= 0.69240\n",
      "Epoch: 0068 train_loss= 1.20371 val_roc= 0.65928 val_ap= 0.69164\n",
      "Epoch: 0069 train_loss= 1.17535 val_roc= 0.65983 val_ap= 0.69184\n",
      "Epoch: 0070 train_loss= 1.17360 val_roc= 0.66077 val_ap= 0.69251\n",
      "Test AP score: 0.7164707982720263\n",
      "Test ROC score: 0.6822135967075817\n",
      "Epoch: 0071 train_loss= 1.16179 val_roc= 0.66152 val_ap= 0.69248\n",
      "Epoch: 0072 train_loss= 1.14807 val_roc= 0.66271 val_ap= 0.69295\n",
      "Epoch: 0073 train_loss= 1.15170 val_roc= 0.66365 val_ap= 0.69313\n",
      "Epoch: 0074 train_loss= 1.11956 val_roc= 0.66491 val_ap= 0.69388\n",
      "Epoch: 0075 train_loss= 1.11615 val_roc= 0.66660 val_ap= 0.69417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0076 train_loss= 1.10779 val_roc= 0.66806 val_ap= 0.69468\n",
      "Epoch: 0077 train_loss= 1.09138 val_roc= 0.66893 val_ap= 0.69488\n",
      "Epoch: 0078 train_loss= 1.08956 val_roc= 0.67089 val_ap= 0.69625\n",
      "Epoch: 0079 train_loss= 1.07550 val_roc= 0.67247 val_ap= 0.69745\n",
      "Epoch: 0080 train_loss= 1.07189 val_roc= 0.67367 val_ap= 0.69777\n",
      "Test AP score: 0.7221171834656639\n",
      "Test ROC score: 0.6967385472889038\n",
      "Epoch: 0081 train_loss= 1.05711 val_roc= 0.67529 val_ap= 0.69940\n",
      "Epoch: 0082 train_loss= 1.03757 val_roc= 0.67681 val_ap= 0.70015\n",
      "Epoch: 0083 train_loss= 1.03774 val_roc= 0.67866 val_ap= 0.70149\n",
      "Epoch: 0084 train_loss= 1.02774 val_roc= 0.68038 val_ap= 0.70233\n",
      "Epoch: 0085 train_loss= 1.02266 val_roc= 0.68252 val_ap= 0.70362\n",
      "Epoch: 0086 train_loss= 1.00823 val_roc= 0.68441 val_ap= 0.70439\n",
      "Epoch: 0087 train_loss= 1.00348 val_roc= 0.68609 val_ap= 0.70606\n",
      "Epoch: 0088 train_loss= 0.99951 val_roc= 0.68756 val_ap= 0.70714\n",
      "Epoch: 0089 train_loss= 0.98984 val_roc= 0.68967 val_ap= 0.70812\n",
      "Epoch: 0090 train_loss= 0.98897 val_roc= 0.69078 val_ap= 0.70812\n",
      "Test AP score: 0.730141789498266\n",
      "Test ROC score: 0.7135354968332439\n",
      "Epoch: 0091 train_loss= 0.96983 val_roc= 0.69099 val_ap= 0.70796\n",
      "Epoch: 0092 train_loss= 0.96246 val_roc= 0.69136 val_ap= 0.70821\n",
      "Epoch: 0093 train_loss= 0.94326 val_roc= 0.69187 val_ap= 0.70855\n",
      "Epoch: 0094 train_loss= 0.94774 val_roc= 0.69161 val_ap= 0.70778\n",
      "Epoch: 0095 train_loss= 0.94746 val_roc= 0.69177 val_ap= 0.70767\n",
      "Epoch: 0096 train_loss= 0.92391 val_roc= 0.69209 val_ap= 0.70779\n",
      "Epoch: 0097 train_loss= 0.92481 val_roc= 0.69183 val_ap= 0.70696\n",
      "Epoch: 0098 train_loss= 0.91649 val_roc= 0.69242 val_ap= 0.70702\n",
      "Epoch: 0099 train_loss= 0.90475 val_roc= 0.69190 val_ap= 0.70602\n",
      "Epoch: 0100 train_loss= 0.89807 val_roc= 0.69170 val_ap= 0.70536\n",
      "Test AP score: 0.7245879025715005\n",
      "Test ROC score: 0.7132150405611224\n",
      "Epoch: 0101 train_loss= 0.89278 val_roc= 0.69139 val_ap= 0.70450\n",
      "Epoch: 0102 train_loss= 0.87725 val_roc= 0.69063 val_ap= 0.70318\n",
      "Epoch: 0103 train_loss= 0.87752 val_roc= 0.68917 val_ap= 0.70172\n",
      "Epoch: 0104 train_loss= 0.86079 val_roc= 0.68816 val_ap= 0.70053\n",
      "Epoch: 0105 train_loss= 0.86315 val_roc= 0.68765 val_ap= 0.69957\n",
      "Epoch: 0106 train_loss= 0.85831 val_roc= 0.68740 val_ap= 0.69953\n",
      "Epoch: 0107 train_loss= 0.84739 val_roc= 0.68711 val_ap= 0.69840\n",
      "Epoch: 0108 train_loss= 0.84587 val_roc= 0.68698 val_ap= 0.69699\n",
      "Epoch: 0109 train_loss= 0.83090 val_roc= 0.68735 val_ap= 0.69683\n",
      "Epoch: 0110 train_loss= 0.82824 val_roc= 0.68727 val_ap= 0.69587\n",
      "Test AP score: 0.7127903296978242\n",
      "Test ROC score: 0.706568273388807\n",
      "Epoch: 0111 train_loss= 0.82853 val_roc= 0.68743 val_ap= 0.69492\n",
      "Epoch: 0112 train_loss= 0.81618 val_roc= 0.68784 val_ap= 0.69422\n",
      "Epoch: 0113 train_loss= 0.82762 val_roc= 0.68776 val_ap= 0.69319\n",
      "Epoch: 0114 train_loss= 0.81419 val_roc= 0.68774 val_ap= 0.69225\n",
      "Epoch: 0115 train_loss= 0.81833 val_roc= 0.68826 val_ap= 0.69190\n",
      "Epoch: 0116 train_loss= 0.81000 val_roc= 0.68922 val_ap= 0.69254\n",
      "Epoch: 0117 train_loss= 0.81512 val_roc= 0.68917 val_ap= 0.69160\n",
      "Epoch: 0118 train_loss= 0.81159 val_roc= 0.68951 val_ap= 0.69118\n",
      "Epoch: 0119 train_loss= 0.81278 val_roc= 0.68986 val_ap= 0.69082\n",
      "Epoch: 0120 train_loss= 0.80012 val_roc= 0.69021 val_ap= 0.69024\n",
      "Test AP score: 0.71144985141791\n",
      "Test ROC score: 0.7092651469598061\n",
      "Epoch: 0121 train_loss= 0.80078 val_roc= 0.69089 val_ap= 0.69046\n",
      "Epoch: 0122 train_loss= 0.79704 val_roc= 0.69142 val_ap= 0.69064\n",
      "Epoch: 0123 train_loss= 0.78397 val_roc= 0.69215 val_ap= 0.69088\n",
      "Epoch: 0124 train_loss= 0.79032 val_roc= 0.69269 val_ap= 0.69194\n",
      "Epoch: 0125 train_loss= 0.78746 val_roc= 0.69361 val_ap= 0.69136\n",
      "Epoch: 0126 train_loss= 0.78303 val_roc= 0.69410 val_ap= 0.69149\n",
      "Epoch: 0127 train_loss= 0.78157 val_roc= 0.69476 val_ap= 0.69141\n",
      "Epoch: 0128 train_loss= 0.78080 val_roc= 0.69557 val_ap= 0.69179\n",
      "Epoch: 0129 train_loss= 0.78791 val_roc= 0.69629 val_ap= 0.69208\n",
      "Epoch: 0130 train_loss= 0.77914 val_roc= 0.69683 val_ap= 0.69204\n",
      "Test AP score: 0.7148469994374673\n",
      "Test ROC score: 0.7141476043193185\n",
      "Epoch: 0131 train_loss= 0.78441 val_roc= 0.69736 val_ap= 0.69169\n",
      "Epoch: 0132 train_loss= 0.77995 val_roc= 0.69820 val_ap= 0.69192\n",
      "Epoch: 0133 train_loss= 0.78104 val_roc= 0.69862 val_ap= 0.69224\n",
      "Epoch: 0134 train_loss= 0.78068 val_roc= 0.69880 val_ap= 0.69183\n",
      "Epoch: 0135 train_loss= 0.78092 val_roc= 0.69897 val_ap= 0.69163\n",
      "Epoch: 0136 train_loss= 0.78950 val_roc= 0.69930 val_ap= 0.69177\n",
      "Epoch: 0137 train_loss= 0.78729 val_roc= 0.69959 val_ap= 0.69248\n",
      "Epoch: 0138 train_loss= 0.78010 val_roc= 0.69935 val_ap= 0.69214\n",
      "Epoch: 0139 train_loss= 0.77761 val_roc= 0.69985 val_ap= 0.69345\n",
      "Epoch: 0140 train_loss= 0.77928 val_roc= 0.70044 val_ap= 0.69451\n",
      "Test AP score: 0.7137749108416638\n",
      "Test ROC score: 0.7153466148655704\n",
      "Epoch: 0141 train_loss= 0.78349 val_roc= 0.70069 val_ap= 0.69557\n",
      "Epoch: 0142 train_loss= 0.79095 val_roc= 0.70063 val_ap= 0.69625\n",
      "Epoch: 0143 train_loss= 0.78561 val_roc= 0.70000 val_ap= 0.69580\n",
      "Epoch: 0144 train_loss= 0.78103 val_roc= 0.69979 val_ap= 0.69595\n",
      "Epoch: 0145 train_loss= 0.78634 val_roc= 0.70047 val_ap= 0.69773\n",
      "Epoch: 0146 train_loss= 0.78556 val_roc= 0.70143 val_ap= 0.69911\n",
      "Epoch: 0147 train_loss= 0.79102 val_roc= 0.70234 val_ap= 0.70007\n",
      "Epoch: 0148 train_loss= 0.79238 val_roc= 0.70241 val_ap= 0.70055\n",
      "Epoch: 0149 train_loss= 0.78581 val_roc= 0.70257 val_ap= 0.70129\n",
      "Epoch: 0150 train_loss= 0.79086 val_roc= 0.70229 val_ap= 0.70134\n",
      "Test AP score: 0.7181278368060636\n",
      "Test ROC score: 0.7165456254118223\n",
      "Epoch: 0151 train_loss= 0.79043 val_roc= 0.70180 val_ap= 0.70112\n",
      "Epoch: 0152 train_loss= 0.79388 val_roc= 0.70227 val_ap= 0.70198\n",
      "Epoch: 0153 train_loss= 0.79305 val_roc= 0.70268 val_ap= 0.70257\n",
      "Epoch: 0154 train_loss= 0.79181 val_roc= 0.70266 val_ap= 0.70314\n",
      "Epoch: 0155 train_loss= 0.79341 val_roc= 0.70276 val_ap= 0.70334\n",
      "Epoch: 0156 train_loss= 0.80527 val_roc= 0.70251 val_ap= 0.70336\n",
      "Epoch: 0157 train_loss= 0.80365 val_roc= 0.70242 val_ap= 0.70325\n",
      "Epoch: 0158 train_loss= 0.79837 val_roc= 0.70325 val_ap= 0.70410\n",
      "Epoch: 0159 train_loss= 0.79979 val_roc= 0.70396 val_ap= 0.70520\n",
      "Epoch: 0160 train_loss= 0.80398 val_roc= 0.70368 val_ap= 0.70508\n",
      "Test AP score: 0.7176593402414946\n",
      "Test ROC score: 0.7169813019166166\n",
      "Epoch: 0161 train_loss= 0.81064 val_roc= 0.70331 val_ap= 0.70441\n",
      "Epoch: 0162 train_loss= 0.80439 val_roc= 0.70287 val_ap= 0.70359\n",
      "Epoch: 0163 train_loss= 0.81063 val_roc= 0.70235 val_ap= 0.70345\n",
      "Epoch: 0164 train_loss= 0.81274 val_roc= 0.70240 val_ap= 0.70351\n",
      "Epoch: 0165 train_loss= 0.80893 val_roc= 0.70283 val_ap= 0.70330\n",
      "Epoch: 0166 train_loss= 0.81013 val_roc= 0.70386 val_ap= 0.70432\n",
      "Epoch: 0167 train_loss= 0.80838 val_roc= 0.70429 val_ap= 0.70495\n",
      "Epoch: 0168 train_loss= 0.80554 val_roc= 0.70441 val_ap= 0.70559\n",
      "Epoch: 0169 train_loss= 0.80736 val_roc= 0.70475 val_ap= 0.70584\n",
      "Epoch: 0170 train_loss= 0.81297 val_roc= 0.70485 val_ap= 0.70625\n",
      "Test AP score: 0.720027922858787\n",
      "Test ROC score: 0.7186952028776253\n",
      "Epoch: 0171 train_loss= 0.82251 val_roc= 0.70487 val_ap= 0.70663\n",
      "Epoch: 0172 train_loss= 0.80695 val_roc= 0.70501 val_ap= 0.70674\n",
      "Epoch: 0173 train_loss= 0.82229 val_roc= 0.70536 val_ap= 0.70719\n",
      "Epoch: 0174 train_loss= 0.81885 val_roc= 0.70513 val_ap= 0.70726\n",
      "Epoch: 0175 train_loss= 0.81307 val_roc= 0.70490 val_ap= 0.70706\n",
      "Epoch: 0176 train_loss= 0.81637 val_roc= 0.70461 val_ap= 0.70684\n",
      "Epoch: 0177 train_loss= 0.81193 val_roc= 0.70342 val_ap= 0.70567\n",
      "Epoch: 0178 train_loss= 0.82383 val_roc= 0.70257 val_ap= 0.70466\n",
      "Epoch: 0179 train_loss= 0.81951 val_roc= 0.70231 val_ap= 0.70389\n",
      "Epoch: 0180 train_loss= 0.80868 val_roc= 0.70260 val_ap= 0.70469\n",
      "Test AP score: 0.7196927636566092\n",
      "Test ROC score: 0.718108299817448\n",
      "Epoch: 0181 train_loss= 0.81572 val_roc= 0.70287 val_ap= 0.70510\n",
      "Epoch: 0182 train_loss= 0.81280 val_roc= 0.70303 val_ap= 0.70562\n",
      "Epoch: 0183 train_loss= 0.82233 val_roc= 0.70367 val_ap= 0.70664\n",
      "Epoch: 0184 train_loss= 0.82606 val_roc= 0.70458 val_ap= 0.70756\n",
      "Epoch: 0185 train_loss= 0.81325 val_roc= 0.70498 val_ap= 0.70789\n",
      "Epoch: 0186 train_loss= 0.81996 val_roc= 0.70510 val_ap= 0.70770\n",
      "Epoch: 0187 train_loss= 0.82522 val_roc= 0.70485 val_ap= 0.70736\n",
      "Epoch: 0188 train_loss= 0.81937 val_roc= 0.70488 val_ap= 0.70673\n",
      "Epoch: 0189 train_loss= 0.82463 val_roc= 0.70468 val_ap= 0.70630\n",
      "Epoch: 0190 train_loss= 0.82022 val_roc= 0.70465 val_ap= 0.70620\n",
      "Test AP score: 0.7233630555924713\n",
      "Test ROC score: 0.7204919183808677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0191 train_loss= 0.81426 val_roc= 0.70465 val_ap= 0.70596\n",
      "Epoch: 0192 train_loss= 0.82188 val_roc= 0.70533 val_ap= 0.70703\n",
      "Epoch: 0193 train_loss= 0.82438 val_roc= 0.70508 val_ap= 0.70677\n",
      "Epoch: 0194 train_loss= 0.81628 val_roc= 0.70423 val_ap= 0.70549\n",
      "Epoch: 0195 train_loss= 0.82389 val_roc= 0.70300 val_ap= 0.70427\n",
      "Epoch: 0196 train_loss= 0.82388 val_roc= 0.70309 val_ap= 0.70453\n",
      "Epoch: 0197 train_loss= 0.80802 val_roc= 0.70413 val_ap= 0.70554\n",
      "Epoch: 0198 train_loss= 0.81478 val_roc= 0.70527 val_ap= 0.70732\n",
      "Epoch: 0199 train_loss= 0.81933 val_roc= 0.70556 val_ap= 0.70786\n",
      "Epoch: 0200 train_loss= 0.82549 val_roc= 0.70595 val_ap= 0.70859\n",
      "Test AP score: 0.7245021440386787\n",
      "Test ROC score: 0.7212012429382599\n"
     ]
    }
   ],
   "source": [
    "runner.erun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c11174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.15",
   "language": "python",
   "name": "tf1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
